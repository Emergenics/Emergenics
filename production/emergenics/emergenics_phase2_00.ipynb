{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31b3315",
   "metadata": {},
   "source": [
    "# Emergenics: Phase 1 Notebook: Universality & Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b96e03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dea3784",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8999a8af",
   "metadata": {},
   "source": [
    "Copyright 2025 Michael Gerald Young II, Emergenics Foundation\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d69a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ UMAP imported successfully.\n",
      "--- Cell 0: Initial Setup (Emergenics - Phase 2 Ready) (2025-04-15 15:48:33) ---\n",
      "‚úÖ CUDA available with 1 GPU(s).\n",
      "‚úÖ Using GPU 0: NVIDIA GeForce RTX 2060\n",
      "PyTorch Device set to: cuda:0\n",
      "Checked/created base directories (Phase 1: emergenics_phase1_results, Phase 2: emergenics_phase2_results).\n",
      "‚úÖ Set multiprocessing start method to 'spawn'.\n",
      "Cell 0 execution complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Initial Setup & Imports (Emergenics Phase 1 - GPU)\n",
    "# Description: Basic imports, setup, device check (prioritizing GPU).\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import torch # Import PyTorch\n",
    "import requests\n",
    "import io\n",
    "import gzip\n",
    "import shutil\n",
    "import copy\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import itertools\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import entropy as calculate_scipy_entropy\n",
    "from scipy.optimize import curve_fit, minimize # Keep scipy optimize for fitting\n",
    "import multiprocessing as mp # Explicitly import for setting start method\n",
    "import ast # For literal_eval parsing if needed\n",
    "\n",
    "# --- Try importing optional Phase 2 dependencies ---\n",
    "try:\n",
    "    import umap\n",
    "    print(\"‚úÖ UMAP imported successfully.\")\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è UMAP not found. Install with 'pip install umap-learn'. UMAP analysis will be skipped.\")\n",
    "    UMAP_AVAILABLE = False\n",
    "# Add other optional imports here (e.g., for specific information metrics)\n",
    "\n",
    "\n",
    "# Import display tools if needed (less relevant for non-interactive phase 1 runs)\n",
    "# from IPython.display import display, Image\n",
    "\n",
    "# Ignore common warnings for cleaner output (optional)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "print(f\"--- Cell 0: Initial Setup (Emergenics - Phase 2 Ready) ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "# --- Device Check ---\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"‚úÖ CUDA available with {num_gpus} GPU(s).\")\n",
    "    # Simple strategy: use GPU 0 if available, otherwise CPU\n",
    "    # More complex strategies could involve load balancing or specific GPU selection\n",
    "    if num_gpus > 0:\n",
    "        device = torch.device('cuda:0') # Use the first available CUDA device\n",
    "        try:\n",
    "            dev_name = torch.cuda.get_device_name(0)\n",
    "            print(f\"‚úÖ Using GPU 0: {dev_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úÖ Using GPU 0, but couldn't get device name: {e}\")\n",
    "    else:\n",
    "        # This case should ideally not happen if torch.cuda.is_available() is true\n",
    "        print(\"‚ö†Ô∏è CUDA reported available, but no devices found. Using CPU.\")\n",
    "        device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è CUDA not available, using CPU.\")\n",
    "\n",
    "# Make device globally accessible\n",
    "global_device = device\n",
    "print(f\"PyTorch Device set to: {global_device}\")\n",
    "\n",
    "# --- Base Directories (Ensure they exist) ---\n",
    "DATA_ROOT_DIR = \"/tmp/cakg_data\" # Or a more persistent location\n",
    "# Define separate base directories for Phase 1 and Phase 2 results\n",
    "OUTPUT_DIR_BASE_PHASE1 = \"emergenics_phase1_results\"\n",
    "OUTPUT_DIR_BASE_PHASE2 = \"emergenics_phase2_results\"\n",
    "os.makedirs(DATA_ROOT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_BASE_PHASE1, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_BASE_PHASE2, exist_ok=True)\n",
    "print(f\"Checked/created base directories (Phase 1: {OUTPUT_DIR_BASE_PHASE1}, Phase 2: {OUTPUT_DIR_BASE_PHASE2}).\")\n",
    "\n",
    "# --- Multiprocessing Start Method ---\n",
    "# Set start method to 'spawn' early, essential for CUDA with multiprocessing\n",
    "try:\n",
    "    current_start_method = mp.get_start_method(allow_none=True)\n",
    "    if current_start_method != 'spawn':\n",
    "         mp.set_start_method('spawn', force=True)\n",
    "         print(\"‚úÖ Set multiprocessing start method to 'spawn'.\")\n",
    "    else:\n",
    "         print(f\"‚úÖ Multiprocessing start method already '{current_start_method}'.\")\n",
    "except Exception as e_spawn:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not set multiprocessing start method to 'spawn'. Error: {e_spawn}\")\n",
    "    print(\"   CUDA GPU usage in parallel workers might fail.\")\n",
    "\n",
    "\n",
    "print(\"Cell 0 execution complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9906761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 1: Configuration (Phase 2 - Updates - Corrected Metrics Loading) ---\n",
      "  Loaded Phase 1 config from latest run: emergenics_phase1_results/Emergenics_Phase1_5D_HDC_RSV_N357_20250415_133241\n",
      "  Successfully loaded Phase 1 key metrics from: emergenics_phase1_results/Emergenics_Phase1_5D_HDC_RSV_N357_20250415_133241/Emergenics_Phase1_5D_HDC_RSV_N357_20250415_133241_key_metrics.json\n",
      "üß™ Phase 2 Experiment Name: Emergenics_Phase1_5D_HDC_RSV_N357_Phase2_20250415_154833\n",
      "üß¨ Core Params: State Dim=5, Max Steps=200\n",
      "üìê Baseline Rule Params Loaded/Defined.\n",
      "üî¢ System Sizes (N) for Analysis: [300, 500, 700]\n",
      "üï∏Ô∏è Graph Model Params Defined: ['WS', 'SBM', 'RGG']\n",
      "üìä Info Processing: Store History=True (Interval: 10), Metrics=['mean_final_state_entropy']\n",
      "üó∫Ô∏è Attractor Landscape: Run PCA=True, Run UMAP=True (Samples per param: 500)\n",
      "   UMAP Params: {'n_neighbors': 15, 'min_dist': 0.1, 'n_components': 2, 'metric': 'euclidean'}\n",
      "‚ö° Perturbation: Run Analysis=True\n",
      "   Perturbation Config: {'apply_at_step': 50, 'duration_steps': 5, 'target_node_fraction': 0.01, 'target_dimension': 0, 'perturbation_value': 1.0}\n",
      "   Perturbation Metrics: ['relaxation_time', 'perturbation_spread']\n",
      "  Using Phase 1 critical points for targeting: pc(WS)‚âà4.584e-05, pc(SBM)‚âà0.1, rc(RGG)‚âà0.28\n",
      "üéØ Phase 2 Targeted Graph Params:\n",
      "   WS p_values range: 4.6e-06 to 4.6e-04 (15 points around 4.6e-05)\n",
      "   SBM p_intra_values range: 0.020 to 0.500 (15 points around 0.100)\n",
      "   RGG radius_values range: 0.056 to 1.400 (15 points around 0.280)\n",
      "‚û°Ô∏è Phase 2 Results will be saved in: emergenics_phase2_results/Emergenics_Phase1_5D_HDC_RSV_N357_Phase2_20250415_154833\n",
      "   ‚úÖ Saved Phase 2 configuration to emergenics_phase2_results/Emergenics_Phase1_5D_HDC_RSV_N357_Phase2_20250415_154833/run_config_phase2.json\n",
      "   Phase 2 configuration assigned to global 'config'.\n",
      "\n",
      "Cell 1 execution complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Configuration (Phase 2 - Updates - Corrected Metrics Loading)\n",
    "# Description: Adds Phase 2 configuration parameters. Correctly loads Phase 1\n",
    "#              key metrics (pc values) to define targeted sweep ranges.\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import copy\n",
    "import warnings # Import warnings\n",
    "\n",
    "print(f\"\\n--- Cell 1: Configuration (Phase 2 - Updates - Corrected Metrics Loading) ---\")\n",
    "\n",
    "# --- Experiment Setup ---\n",
    "# Load existing Phase 1 config if possible, otherwise define defaults\n",
    "config_save_path_phase1 = None\n",
    "phase1_output_dir = None # Store Phase 1 output directory\n",
    "output_dir_base_phase1 = \"emergenics_phase1_results\"\n",
    "exp_pattern_phase1 = \"Emergenics_Phase1_5D_HDC_RSV_N357\" # Match the pattern used in Phase 1\n",
    "phase1_config = {} # Initialize empty config dict\n",
    "\n",
    "try:\n",
    "    # Check if a config dict already exists globally (e.g., from previous run)\n",
    "    config_loaded_from_global = False\n",
    "    if 'config' in globals() and isinstance(globals()['config'], dict) and 'OUTPUT_DIR' in globals()['config']:\n",
    "        # Use already loaded config if it seems valid (has OUTPUT_DIR)\n",
    "        loaded_config_check = globals()['config']\n",
    "        # Heuristic check: if it contains Phase 2 keys, it might be the wrong one.\n",
    "        # If it lacks Phase 2 keys, assume it's the intended Phase 1 config.\n",
    "        if 'GRAPH_MODEL_PARAMS_PHASE2' not in loaded_config_check:\n",
    "             phase1_config = loaded_config_check\n",
    "             phase1_output_dir = phase1_config.get('OUTPUT_DIR')\n",
    "             if phase1_output_dir:\n",
    "                  config_save_path_phase1 = os.path.join(phase1_output_dir, \"run_config_phase1.json\")\n",
    "                  print(f\"  Using Phase 1 config loaded previously from: {phase1_output_dir}\")\n",
    "                  config_loaded_from_global = True\n",
    "             else:\n",
    "                  print(\"  Previously loaded config lacks OUTPUT_DIR. Searching for latest Phase 1 run.\")\n",
    "        else:\n",
    "            print(\"  Global 'config' seems to be from Phase 2 already. Searching for latest Phase 1 run.\")\n",
    "\n",
    "\n",
    "    # If not loaded from global, search filesystem\n",
    "    if not config_loaded_from_global:\n",
    "        if os.path.isdir(output_dir_base_phase1):\n",
    "            all_subdirs = [d for d in os.listdir(output_dir_base_phase1) if os.path.isdir(os.path.join(output_dir_base_phase1, d)) and d.startswith(exp_pattern_phase1)]\n",
    "            if all_subdirs:\n",
    "                latest_run_dir = max([os.path.join(output_dir_base_phase1, d) for d in all_subdirs], key=os.path.getmtime)\n",
    "                config_save_path_phase1 = os.path.join(latest_run_dir, \"run_config_phase1.json\")\n",
    "                if os.path.exists(config_save_path_phase1):\n",
    "                    with open(config_save_path_phase1, 'r') as f:\n",
    "                        phase1_config = json.load(f)\n",
    "                    phase1_output_dir = phase1_config.get('OUTPUT_DIR', latest_run_dir) # Get output dir from config or use found dir\n",
    "                    print(f\"  Loaded Phase 1 config from latest run: {latest_run_dir}\")\n",
    "                else:\n",
    "                    print(f\"  Config file not found in latest Phase 1 dir: {latest_run_dir}. Using defaults.\")\n",
    "                    phase1_config = {} # Fallback\n",
    "            else:\n",
    "                 print(f\"  No Phase 1 experiment directories found matching pattern '{exp_pattern_phase1}'. Using defaults.\")\n",
    "                 phase1_config = {} # Fallback\n",
    "        else:\n",
    "            print(f\"  Phase 1 base directory '{output_dir_base_phase1}' not found. Using defaults.\")\n",
    "            phase1_config = {} # Fallback\n",
    "\n",
    "except Exception as e_load_cfg:\n",
    "    print(f\"  Error loading Phase 1 config: {e_load_cfg}. Using defaults.\")\n",
    "    traceback.print_exc(limit=1)\n",
    "    phase1_config = {} # Fallback\n",
    "\n",
    "# --- Load Phase 1 Key Metrics (pc values) ---\n",
    "phase1_key_metrics = {} # Initialize empty metrics dict\n",
    "phase1_key_metrics_path = None\n",
    "if phase1_output_dir: # Check if we identified the Phase 1 output directory\n",
    "    phase1_exp_name = phase1_config.get('EXPERIMENT_NAME', os.path.basename(phase1_output_dir)) # Get exp name from config or dir name\n",
    "    phase1_key_metrics_path = os.path.join(phase1_output_dir, f\"{phase1_exp_name}_key_metrics.json\")\n",
    "    if os.path.exists(phase1_key_metrics_path):\n",
    "        try:\n",
    "            with open(phase1_key_metrics_path, 'r') as f_metrics:\n",
    "                phase1_key_metrics = json.load(f_metrics)\n",
    "            print(f\"  Successfully loaded Phase 1 key metrics from: {phase1_key_metrics_path}\")\n",
    "        except Exception as e_load_metrics:\n",
    "            print(f\"  Warning: Failed to load or parse Phase 1 key metrics file '{phase1_key_metrics_path}': {e_load_metrics}\")\n",
    "            phase1_key_metrics = {} # Reset on failure\n",
    "    else:\n",
    "        print(f\"  Warning: Phase 1 key metrics file not found at: {phase1_key_metrics_path}\")\n",
    "        phase1_key_metrics = {} # Set empty if file not found\n",
    "else:\n",
    "    print(\"  Warning: Could not determine Phase 1 output directory. Unable to load key metrics.\")\n",
    "    phase1_key_metrics = {} # Set empty if dir unknown\n",
    "\n",
    "\n",
    "# --- Experiment Naming for Phase 2 ---\n",
    "EXPERIMENT_BASE_NAME_PH2 = phase1_config.get(\"EXPERIMENT_BASE_NAME\", \"Emergenics\") + \"_Phase2\" # Append Phase2 marker\n",
    "EXPERIMENT_NAME = f\"{EXPERIMENT_BASE_NAME_PH2}_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"üß™ Phase 2 Experiment Name: {EXPERIMENT_NAME}\")\n",
    "\n",
    "# --- Inherit or Define Core Model & Simulation Parameters ---\n",
    "# Use .get() with defaults for robustness\n",
    "STATE_DIM = phase1_config.get('STATE_DIM', 5)\n",
    "MAX_SIMULATION_STEPS = phase1_config.get('MAX_SIMULATION_STEPS', 200)\n",
    "CONVERGENCE_THRESHOLD = phase1_config.get('CONVERGENCE_THRESHOLD', 1e-4)\n",
    "RULE_PARAMS = phase1_config.get('RULE_PARAMS', {\n",
    "    'activation_threshold': 0.5, 'activation_increase_rate': 0.15, 'activation_decay_rate': 0.05,\n",
    "    'inhibition_threshold': 0.5, 'inhibition_increase_rate': 0.1, 'inhibition_decay_rate': 0.1,\n",
    "    'inhibition_feedback_threshold': 0.6, 'inhibition_feedback_strength': 0.3,\n",
    "    'diffusion_factor': 0.05, 'noise_level': 0.001, 'harmonic_factor': 0.05,\n",
    "    'w_decay_rate': 0.05, 'x_decay_rate': 0.05, 'y_decay_rate': 0.05\n",
    "}) # Simplified default if needed\n",
    "SYSTEM_SIZES = phase1_config.get('SYSTEM_SIZES', [300, 500, 700])\n",
    "PRIMARY_ORDER_PARAMETER = phase1_config.get('PRIMARY_ORDER_PARAMETER', 'variance_norm')\n",
    "GRAPH_MODEL_PARAMS = phase1_config.get('GRAPH_MODEL_PARAMS', {\n",
    "    'WS': { 'k_neighbors': 4, 'p_values': np.logspace(-5, 0, 20) },\n",
    "    'SBM': { 'n_communities': 2, 'p_inter': 0.01, 'p_intra_values': np.linspace(0.01, 0.5, 20) },\n",
    "    'RGG': { 'radius_values': np.linspace(0.05, 0.5, 20) }\n",
    "})\n",
    "# Ensure parameter values are lists/arrays, not strings from JSON load\n",
    "for model, params in GRAPH_MODEL_PARAMS.items():\n",
    "     for key, value in params.items():\n",
    "          if key.endswith('_values') and isinstance(value, list):\n",
    "               params[key] = np.array(value)\n",
    "\n",
    "\n",
    "NUM_INSTANCES_PER_PARAM = phase1_config.get('NUM_INSTANCES_PER_PARAM', 10) # Can reduce for Phase 2 focused runs if needed\n",
    "NUM_TRIALS_PER_INSTANCE = phase1_config.get('NUM_TRIALS_PER_INSTANCE', 3) # Can reduce\n",
    "PARALLEL_WORKERS = phase1_config.get('PARALLEL_WORKERS', os.cpu_count()) # Use CPU count as fallback\n",
    "\n",
    "print(f\"üß¨ Core Params: State Dim={STATE_DIM}, Max Steps={MAX_SIMULATION_STEPS}\")\n",
    "print(f\"üìê Baseline Rule Params Loaded/Defined.\")\n",
    "print(f\"üî¢ System Sizes (N) for Analysis: {SYSTEM_SIZES}\")\n",
    "print(f\"üï∏Ô∏è Graph Model Params Defined: {list(GRAPH_MODEL_PARAMS.keys())}\")\n",
    "\n",
    "# --- Phase 2 Specific Parameters ---\n",
    "\n",
    "# 2.1 Information Processing\n",
    "STORE_STATE_HISTORY = True # Set to True to enable history-dependent metrics\n",
    "STATE_HISTORY_INTERVAL = 10 # Store state every N steps (reduces memory vs storing every step)\n",
    "INFO_METRICS_TO_CALC = ['mean_final_state_entropy'] # Add metrics like 'mean_step_entropy' later if needed\n",
    "print(f\"üìä Info Processing: Store History={STORE_STATE_HISTORY} (Interval: {STATE_HISTORY_INTERVAL}), Metrics={INFO_METRICS_TO_CALC}\")\n",
    "\n",
    "# 2.2 Attractor Landscape\n",
    "RUN_PCA_ANALYSIS = True # Flag to control running the potentially slow PCA cell\n",
    "RUN_UMAP_ANALYSIS = True # Flag to control running UMAP analysis\n",
    "UMAP_PARAMS = {'n_neighbors': 15, 'min_dist': 0.1, 'n_components': 2, 'metric': 'euclidean'}\n",
    "NUM_SAMPLES_FOR_LANDSCAPE = 500 # Number of final state vectors to collect per parameter setting for UMAP/PCA\n",
    "print(f\"üó∫Ô∏è Attractor Landscape: Run PCA={RUN_PCA_ANALYSIS}, Run UMAP={RUN_UMAP_ANALYSIS} (Samples per param: {NUM_SAMPLES_FOR_LANDSCAPE})\")\n",
    "print(f\"   UMAP Params: {UMAP_PARAMS}\")\n",
    "\n",
    "# 2.3 Perturbation Response\n",
    "RUN_PERTURBATION_ANALYSIS = True # Flag to control perturbation sweeps\n",
    "PERTURBATION_CONFIG = {\n",
    "    'apply_at_step': 50,        # When to apply the perturbation\n",
    "    'duration_steps': 5,        # How long the perturbation lasts\n",
    "    'target_node_fraction': 0.01, # Fraction of nodes to perturb\n",
    "    'target_dimension': 0,      # Which state dimension (e.g., 'u')\n",
    "    'perturbation_value': 1.0   # Value to clamp the state dimension to\n",
    "}\n",
    "PERTURBATION_METRICS_TO_CALC = ['relaxation_time', 'perturbation_spread'] # Metrics calculated by modified worker\n",
    "print(f\"‚ö° Perturbation: Run Analysis={RUN_PERTURBATION_ANALYSIS}\")\n",
    "print(f\"   Perturbation Config: {PERTURBATION_CONFIG}\")\n",
    "print(f\"   Perturbation Metrics: {PERTURBATION_METRICS_TO_CALC}\")\n",
    "\n",
    "# --- Phase 2 Targeted Sweep Parameters ---\n",
    "# Define parameter ranges focused around the critical points found in Phase 1\n",
    "# Use .get() on the loaded phase1_key_metrics dictionary with defaults\n",
    "# ** CORRECTED AREA **\n",
    "pc_ws = phase1_key_metrics.get('final_pc_ws_chi', 0.001) # Use loaded metric or default guess\n",
    "pc_sbm = phase1_key_metrics.get('final_pc_sbm_chi', 0.1)\n",
    "pc_rgg = phase1_key_metrics.get('final_pc_rgg_chi', 0.28)\n",
    "print(f\"  Using Phase 1 critical points for targeting: pc(WS)‚âà{pc_ws:.4g}, pc(SBM)‚âà{pc_sbm:.4g}, rc(RGG)‚âà{pc_rgg:.4g}\")\n",
    "\n",
    "# Example: Define focused ranges (adjust density/width as needed)\n",
    "FOCUS_FACTOR = 5 # How many times wider than pc to scan (linear scale relative to pc)\n",
    "NUM_POINTS_FOCUS = 15 # Number of points in the focused scan\n",
    "\n",
    "# Define GRAPH_MODEL_PARAMS_PHASE2 dictionary\n",
    "GRAPH_MODEL_PARAMS_PHASE2 = {}\n",
    "\n",
    "# WS Targeted Range\n",
    "ws_base_params = GRAPH_MODEL_PARAMS.get('WS', {})\n",
    "p_start_ws = max(1e-6, pc_ws / (FOCUS_FACTOR * 2)) # Ensure positive start, maybe geometric mean?\n",
    "p_end_ws = pc_ws * (FOCUS_FACTOR * 2) # Extend further above\n",
    "# Use logspace for WS 'p' parameter as it spans orders of magnitude\n",
    "ws_p_values_focus = np.logspace(np.log10(p_start_ws), np.log10(p_end_ws), NUM_POINTS_FOCUS)\n",
    "GRAPH_MODEL_PARAMS_PHASE2['WS'] = {\n",
    "    'k_neighbors': ws_base_params.get('k_neighbors', 4),\n",
    "    'p_values': np.unique(ws_p_values_focus) # Ensure unique values\n",
    "}\n",
    "\n",
    "# SBM Targeted Range\n",
    "sbm_base_params = GRAPH_MODEL_PARAMS.get('SBM', {})\n",
    "p_intra_start_sbm = max(sbm_base_params.get('p_inter', 0.01) * 1.01, pc_sbm / FOCUS_FACTOR) # Ensure > p_inter and positive\n",
    "p_intra_end_sbm = pc_sbm * FOCUS_FACTOR\n",
    "sbm_p_intra_values_focus = np.linspace(p_intra_start_sbm, p_intra_end_sbm, NUM_POINTS_FOCUS)\n",
    "GRAPH_MODEL_PARAMS_PHASE2['SBM'] = {\n",
    "    'n_communities': sbm_base_params.get('n_communities', 2),\n",
    "    'p_inter': sbm_base_params.get('p_inter', 0.01),\n",
    "    'p_intra_values': np.unique(sbm_p_intra_values_focus) # Ensure unique\n",
    "}\n",
    "\n",
    "# RGG Targeted Range\n",
    "rgg_base_params = GRAPH_MODEL_PARAMS.get('RGG', {})\n",
    "radius_start_rgg = max(0.01, pc_rgg / FOCUS_FACTOR) # Ensure positive start\n",
    "radius_end_rgg = pc_rgg * FOCUS_FACTOR\n",
    "rgg_radius_values_focus = np.linspace(radius_start_rgg, radius_end_rgg, NUM_POINTS_FOCUS)\n",
    "GRAPH_MODEL_PARAMS_PHASE2['RGG'] = {\n",
    "    'radius_values': np.unique(rgg_radius_values_focus) # Ensure unique\n",
    "}\n",
    "\n",
    "# Ensure no negative values resulted from calculations near zero (redundant but safe)\n",
    "if 'p_values' in GRAPH_MODEL_PARAMS_PHASE2.get('WS',{}):\n",
    "     GRAPH_MODEL_PARAMS_PHASE2['WS']['p_values'] = GRAPH_MODEL_PARAMS_PHASE2['WS']['p_values'][GRAPH_MODEL_PARAMS_PHASE2['WS']['p_values'] > 0]\n",
    "if 'p_intra_values' in GRAPH_MODEL_PARAMS_PHASE2.get('SBM',{}):\n",
    "     p_inter_sbm = GRAPH_MODEL_PARAMS_PHASE2['SBM'].get('p_inter', 0.01)\n",
    "     GRAPH_MODEL_PARAMS_PHASE2['SBM']['p_intra_values'] = np.maximum(GRAPH_MODEL_PARAMS_PHASE2['SBM']['p_intra_values'], p_inter_sbm * 1.01)\n",
    "\n",
    "\n",
    "print(f\"üéØ Phase 2 Targeted Graph Params:\")\n",
    "if 'WS' in GRAPH_MODEL_PARAMS_PHASE2: print(f\"   WS p_values range: {GRAPH_MODEL_PARAMS_PHASE2['WS'].get('p_values', np.array([])).min():.1e} to {GRAPH_MODEL_PARAMS_PHASE2['WS'].get('p_values', np.array([])).max():.1e} ({len(GRAPH_MODEL_PARAMS_PHASE2['WS'].get('p_values', []))} points around {pc_ws:.1e})\")\n",
    "if 'SBM' in GRAPH_MODEL_PARAMS_PHASE2: print(f\"   SBM p_intra_values range: {GRAPH_MODEL_PARAMS_PHASE2['SBM'].get('p_intra_values', np.array([])).min():.3f} to {GRAPH_MODEL_PARAMS_PHASE2['SBM'].get('p_intra_values', np.array([])).max():.3f} ({len(GRAPH_MODEL_PARAMS_PHASE2['SBM'].get('p_intra_values', []))} points around {pc_sbm:.3f})\")\n",
    "if 'RGG' in GRAPH_MODEL_PARAMS_PHASE2: print(f\"   RGG radius_values range: {GRAPH_MODEL_PARAMS_PHASE2['RGG'].get('radius_values', np.array([])).min():.3f} to {GRAPH_MODEL_PARAMS_PHASE2['RGG'].get('radius_values', np.array([])).max():.3f} ({len(GRAPH_MODEL_PARAMS_PHASE2['RGG'].get('radius_values', []))} points around {pc_rgg:.3f})\")\n",
    "\n",
    "\n",
    "# --- Output Directory ---\n",
    "OUTPUT_DIR_BASE = \"emergenics_phase2_results\" # New base directory for Phase 2\n",
    "OUTPUT_DIR = os.path.join(OUTPUT_DIR_BASE, EXPERIMENT_NAME)\n",
    "try:\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"‚û°Ô∏è Phase 2 Results will be saved in: {OUTPUT_DIR}\")\n",
    "except OSError as e_mkdir:\n",
    "    print(f\"‚ùå Error creating Phase 2 output directory '{OUTPUT_DIR}': {e_mkdir}\")\n",
    "    OUTPUT_DIR = \".\" # Fallback to current directory\n",
    "\n",
    "\n",
    "# --- Save Configuration ---\n",
    "config_save_path = os.path.join(OUTPUT_DIR, \"run_config_phase2.json\")\n",
    "try:\n",
    "    # Select uppercase variables and specific lowercase ones relevant to Phase 2\n",
    "    config_to_save = {k: v for k, v in locals().items() if k.isupper()}\n",
    "    # Add phase 1 inherited variables explicitly if needed for reproducibility\n",
    "    config_to_save['STATE_DIM'] = STATE_DIM\n",
    "    config_to_save['MAX_SIMULATION_STEPS'] = MAX_SIMULATION_STEPS\n",
    "    config_to_save['CONVERGENCE_THRESHOLD'] = CONVERGENCE_THRESHOLD\n",
    "    config_to_save['RULE_PARAMS'] = RULE_PARAMS\n",
    "    config_to_save['SYSTEM_SIZES'] = SYSTEM_SIZES\n",
    "    config_to_save['PRIMARY_ORDER_PARAMETER'] = PRIMARY_ORDER_PARAMETER\n",
    "    config_to_save['GRAPH_MODEL_PARAMS'] = GRAPH_MODEL_PARAMS # Keep original Phase 1 ranges\n",
    "    config_to_save['NUM_INSTANCES_PER_PARAM'] = NUM_INSTANCES_PER_PARAM\n",
    "    config_to_save['NUM_TRIALS_PER_INSTANCE'] = NUM_TRIALS_PER_INSTANCE\n",
    "    config_to_save['PARALLEL_WORKERS'] = PARALLEL_WORKERS\n",
    "    # Add Phase 1 metrics file path for reference\n",
    "    config_to_save['phase1_key_metrics_path'] = phase1_key_metrics_path # Store path used\n",
    "    # Add Phase 2 specific configs\n",
    "    config_to_save['STORE_STATE_HISTORY'] = STORE_STATE_HISTORY\n",
    "    config_to_save['STATE_HISTORY_INTERVAL'] = STATE_HISTORY_INTERVAL\n",
    "    config_to_save['INFO_METRICS_TO_CALC'] = INFO_METRICS_TO_CALC\n",
    "    config_to_save['RUN_PCA_ANALYSIS'] = RUN_PCA_ANALYSIS\n",
    "    config_to_save['RUN_UMAP_ANALYSIS'] = RUN_UMAP_ANALYSIS\n",
    "    config_to_save['UMAP_PARAMS'] = UMAP_PARAMS\n",
    "    config_to_save['NUM_SAMPLES_FOR_LANDSCAPE'] = NUM_SAMPLES_FOR_LANDSCAPE\n",
    "    config_to_save['RUN_PERTURBATION_ANALYSIS'] = RUN_PERTURBATION_ANALYSIS\n",
    "    config_to_save['PERTURBATION_CONFIG'] = PERTURBATION_CONFIG\n",
    "    config_to_save['PERTURBATION_METRICS_TO_CALC'] = PERTURBATION_METRICS_TO_CALC\n",
    "    config_to_save['GRAPH_MODEL_PARAMS_PHASE2'] = GRAPH_MODEL_PARAMS_PHASE2 # Save targeted ranges\n",
    "    config_to_save['OUTPUT_DIR'] = OUTPUT_DIR # Save the *Phase 2* output dir\n",
    "\n",
    "    # Make serializable\n",
    "    def default_serializer(obj):\n",
    "        if isinstance(obj, np.ndarray): return obj.tolist()\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)): return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)): return float(obj)\n",
    "        elif isinstance(obj, (np.complex_, np.complex64, np.complex128)): return {'real': obj.real, 'imag': obj.imag}\n",
    "        elif isinstance(obj, (np.bool_)): return bool(obj)\n",
    "        elif isinstance(obj, (np.void)): return None # Or some other representation\n",
    "        try: return str(obj) # Fallback\n",
    "        except: return '<not serializable>'\n",
    "\n",
    "    with open(config_save_path, 'w') as f:\n",
    "        json.dump(config_to_save, f, indent=4, default=default_serializer)\n",
    "    print(f\"   ‚úÖ Saved Phase 2 configuration to {config_save_path}\")\n",
    "except Exception as e_save_cfg:\n",
    "    print(f\"   ‚ö†Ô∏è Warning: Could not save Phase 2 configuration. Error: {e_save_cfg}\")\n",
    "    traceback.print_exc(limit=1)\n",
    "\n",
    "# Make config dictionary globally accessible for Phase 2\n",
    "# Check if 'config_to_save' exists before assignment\n",
    "if 'config_to_save' in locals():\n",
    "    config = config_to_save\n",
    "    print(\"   Phase 2 configuration assigned to global 'config'.\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Could not assign config_to_save to global 'config'.\")\n",
    "    # Fallback: assign the potentially incomplete phase1_config\n",
    "    config = phase1_config\n",
    "    warnings.warn(\"Global 'config' assigned potentially incomplete Phase 1 config due to saving errors.\", RuntimeWarning)\n",
    "\n",
    "\n",
    "print(\"\\nCell 1 execution complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952d5a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ UMAP imported successfully.\n",
      "\n",
      "--- Cell 2: Helper Function Definitions (Phase 2 Implementation - Updated Worker - JIT Fix) ---\n",
      "‚úÖ Phase 2 Helper functions defined (incl. modified worker `run_single_instance_phase2`, UMAP helper, JIT fix).\n",
      "\n",
      "Cell 2 execution complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Helper Function Definitions (Phase 2 Implementation - Updated Worker - JIT Fix)\n",
    "# Description: Defines helper functions. Includes get_sweep_parameters, generate_graph,\n",
    "#              metric calculations, the JIT-compiled GPU step function (with JIT dim fix).\n",
    "#              Defines `run_single_instance_phase2` incorporating state history,\n",
    "#              perturbation logic, and Phase 2 metrics. Adds UMAP/info metric helpers.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import warnings\n",
    "import time\n",
    "from scipy.stats import entropy as calculate_scipy_entropy\n",
    "from scipy.sparse import coo_matrix\n",
    "import traceback\n",
    "import torch\n",
    "import copy\n",
    "import os # For UMAP saving check\n",
    "import pickle # For UMAP saving check\n",
    "\n",
    "# --- Try importing optional Phase 2 dependencies ---\n",
    "try:\n",
    "    import umap\n",
    "    print(\"‚úÖ UMAP imported successfully.\")\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è UMAP not found. Install with 'pip install umap-learn'. UMAP analysis will be skipped.\")\n",
    "    UMAP_AVAILABLE = False\n",
    "# Add other optional imports here (e.g., for specific information metrics)\n",
    "\n",
    "print(\"\\n--- Cell 2: Helper Function Definitions (Phase 2 Implementation - Updated Worker - JIT Fix) ---\")\n",
    "\n",
    "# --- 1. Parameter Generation (Copied from Phase 1 Cell 2) ---\n",
    "def get_sweep_parameters(graph_model_name, model_params, system_sizes, instances, trials, sensitivity_param=None, sensitivity_values=None):\n",
    "    \"\"\"Generates parameter dictionaries for simulation tasks, ensuring primary sweep param is always included.\"\"\"\n",
    "    all_task_params = []\n",
    "    base_seed = int(time.time()) % 10000\n",
    "    param_counter = 0\n",
    "    primary_param_key = None\n",
    "    primary_param_name = None\n",
    "    primary_param_values = None\n",
    "    fixed_params = {}\n",
    "\n",
    "    # Identify primary sweep parameter (e.g., p_values) and fixed params\n",
    "    if not isinstance(model_params, dict): model_params = {} # Handle None case\n",
    "    key_iterator = iter(model_params.keys())\n",
    "    current_key = next(key_iterator, None)\n",
    "    while current_key is not None:\n",
    "        values = model_params[current_key]\n",
    "        # Check if it's a list-like structure intended for sweeping\n",
    "        is_sweep_param = isinstance(values, (list, np.ndarray))\n",
    "        if is_sweep_param:\n",
    "            primary_param_key = current_key\n",
    "            primary_param_name = current_key.replace('_values', '')\n",
    "            primary_param_values = values\n",
    "            # Stop after finding the first sweep parameter if desired, or continue to find last? Assume first.\n",
    "            # break\n",
    "        else:\n",
    "            fixed_params[current_key] = values\n",
    "        current_key = next(key_iterator, None)\n",
    "\n",
    "\n",
    "    # Handle cases where primary sweep param might not be explicitly a list/array\n",
    "    if primary_param_key is None:\n",
    "        if graph_model_name == 'RGG' and 'radius_values' in model_params:\n",
    "            primary_param_key = 'radius_values'\n",
    "            primary_param_name = 'radius'\n",
    "            primary_param_values = model_params['radius_values']\n",
    "        elif graph_model_name == 'SBM' and 'p_intra_values' in model_params:\n",
    "            primary_param_key = 'p_intra_values'\n",
    "            primary_param_name = 'p_intra'\n",
    "            primary_param_values = model_params['p_intra_values']\n",
    "        elif graph_model_name == 'WS' and 'p_values' in model_params:\n",
    "            primary_param_key = 'p_values'\n",
    "            primary_param_name = 'p'\n",
    "            primary_param_values = model_params['p_values']\n",
    "        else:\n",
    "            # Fallback if no sweep parameter identified\n",
    "            primary_param_name = 'param'\n",
    "            primary_param_values = [0] # Dummy sweep value\n",
    "            warnings.warn(f\"Sweep param not found for {graph_model_name}. Using dummy 'param'.\")\n",
    "\n",
    "    # Ensure primary_param_values is iterable\n",
    "    if not hasattr(primary_param_values, '__iter__'):\n",
    "        primary_param_values = [primary_param_values] # Make it a list if it's a single value\n",
    "\n",
    "    # Determine the actual column name for the primary sweep parameter\n",
    "    primary_param_col_name = primary_param_name + '_value'\n",
    "\n",
    "    # Determine sensitivity loop values ([None] if not a sensitivity sweep)\n",
    "    use_sensitivity = isinstance(sensitivity_param, str) and sensitivity_param != \"\" and isinstance(sensitivity_values, (list, np.ndarray)) and len(sensitivity_values) > 0\n",
    "    if use_sensitivity:\n",
    "        sens_loop_values = sensitivity_values\n",
    "    else:\n",
    "        sens_loop_values = [None]\n",
    "\n",
    "    # Main parameter generation loops using while loops\n",
    "    n_idx = 0\n",
    "    while n_idx < len(system_sizes):\n",
    "        N = system_sizes[n_idx]\n",
    "        p_idx = 0\n",
    "        while p_idx < len(primary_param_values):\n",
    "            p_val = primary_param_values[p_idx]\n",
    "            sens_idx = 0\n",
    "            while sens_idx < len(sens_loop_values):\n",
    "                sens_val = sens_loop_values[sens_idx]\n",
    "                inst_idx = 0\n",
    "                while inst_idx < instances:\n",
    "                    graph_seed = base_seed + param_counter + inst_idx * 13\n",
    "                    trial_idx = 0\n",
    "                    while trial_idx < trials:\n",
    "                        sim_seed = base_seed + param_counter + inst_idx * 101 + trial_idx * 7\n",
    "                        task = {\n",
    "                            'model': graph_model_name, 'N': N,\n",
    "                            'fixed_params': fixed_params.copy(),\n",
    "                            # Explicitly include primary sweep param name/value\n",
    "                            primary_param_col_name: p_val,\n",
    "                            'instance': inst_idx, 'trial': trial_idx,\n",
    "                            'graph_seed': graph_seed, 'sim_seed': sim_seed,\n",
    "                            'rule_param_name': sensitivity_param if use_sensitivity else None, # Only add if doing sensitivity\n",
    "                            'rule_param_value': sens_val if use_sensitivity else None\n",
    "                        }\n",
    "                        all_task_params.append(task)\n",
    "                        param_counter += 1\n",
    "                        trial_idx += 1\n",
    "                    inst_idx += 1\n",
    "                sens_idx += 1\n",
    "            p_idx += 1\n",
    "        n_idx += 1\n",
    "\n",
    "    return all_task_params\n",
    "\n",
    "\n",
    "# --- 2. Graph Generation (Copied from Phase 1 Cell 2) ---\n",
    "def generate_graph(model_name, params, N, seed):\n",
    "    \"\"\"Generates a graph using NetworkX.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    G = nx.Graph()\n",
    "    try:\n",
    "        # Prepare parameters for NetworkX functions\n",
    "        gen_params = params.copy()\n",
    "        # Find the parameter key ending with '_value' (e.g., 'p_value')\n",
    "        base_param_name = None\n",
    "        param_key_iterator = iter(gen_params.keys())\n",
    "        current_key = next(param_key_iterator, None)\n",
    "        while current_key is not None:\n",
    "            if isinstance(current_key, str) and current_key.endswith('_value'):\n",
    "                base_param_name = current_key.replace('_value', '')\n",
    "                break\n",
    "            current_key = next(param_key_iterator, None)\n",
    "\n",
    "        # Rename key if generate_graph expects base name (e.g., 'p' instead of 'p_value')\n",
    "        if base_param_name is not None:\n",
    "            value_key = base_param_name + '_value'\n",
    "            if value_key in gen_params:\n",
    "                 gen_params[base_param_name] = gen_params.pop(value_key)\n",
    "\n",
    "\n",
    "        # Generate graph based on model name\n",
    "        if model_name == 'WS':\n",
    "            k = gen_params.get('k_neighbors', 4)\n",
    "            p_rewire = gen_params.get('p', 0.1)  # Expects 'p' key now\n",
    "            k = int(k)\n",
    "            # Ensure k is even and >= 2\n",
    "            if k < 2: k = 2\n",
    "            if k % 2 != 0: k = k - 1\n",
    "            k = min(k, N - 1) # Ensure k < N\n",
    "            if N > k:\n",
    "                G = nx.watts_strogatz_graph(n=N, k=k, p=p_rewire, seed=seed)\n",
    "            else:\n",
    "                # Fallback for small N relative to k (or if k calculation fails)\n",
    "                warnings.warn(f\"WS N={N} <= k={k}. Generating complete graph instead.\", RuntimeWarning)\n",
    "                G = nx.complete_graph(N)\n",
    "        elif model_name == 'SBM':\n",
    "            n_communities = gen_params.get('n_communities', 2)\n",
    "            p_intra = gen_params.get('p_intra', 0.2) # Expects 'p_intra'\n",
    "            p_inter = gen_params.get('p_inter', 0.01)\n",
    "            if N < n_communities:\n",
    "                n_communities = N # Cannot have more communities than nodes\n",
    "                warnings.warn(f\"SBM N={N} < n_communities={gen_params.get('n_communities')}. Setting n_communities=N.\", RuntimeWarning)\n",
    "            if n_communities <= 0:\n",
    "                 raise ValueError(\"Number of communities must be positive.\")\n",
    "            # Calculate community sizes as evenly as possible\n",
    "            sizes = [] # Initialize empty list\n",
    "            base_size = N // n_communities\n",
    "            remainder = N % n_communities\n",
    "            i = 0\n",
    "            while i < n_communities:\n",
    "                 current_size = base_size\n",
    "                 if i < remainder:\n",
    "                      current_size = current_size + 1\n",
    "                 sizes.append(current_size)\n",
    "                 i = i + 1\n",
    "\n",
    "            # Check for zero-sized communities which SBM doesn't allow\n",
    "            if 0 in sizes:\n",
    "                 raise ValueError(f\"SBM calculation resulted in zero-sized community for N={N}, communities={n_communities}\")\n",
    "\n",
    "            # Create probability matrix\n",
    "            probs = []\n",
    "            row_idx = 0\n",
    "            while row_idx < n_communities:\n",
    "                 row = []\n",
    "                 col_idx = 0\n",
    "                 while col_idx < n_communities:\n",
    "                      if row_idx == col_idx:\n",
    "                           row.append(p_intra) # Intra-community probability\n",
    "                      else:\n",
    "                           row.append(p_inter) # Inter-community probability\n",
    "                      col_idx = col_idx + 1\n",
    "                 probs.append(row)\n",
    "                 row_idx = row_idx + 1\n",
    "            G = nx.stochastic_block_model(sizes=sizes, p=probs, seed=seed)\n",
    "        elif model_name == 'RGG':\n",
    "            radius = gen_params.get('radius', 0.1) # Expects 'radius'\n",
    "            G = nx.random_geometric_graph(n=N, radius=radius, seed=seed)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown graph model: {model_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        G = nx.Graph() # Return empty graph on failure\n",
    "        warnings.warn(f\"Graph generation failed for {model_name} N={N} with params {params}: {e}\", RuntimeWarning)\n",
    "        # Optionally print traceback for debugging\n",
    "        traceback.print_exc(limit=1)\n",
    "\n",
    "\n",
    "    # Relabel nodes to strings if needed\n",
    "    num_nodes_generated = G.number_of_nodes()\n",
    "    if num_nodes_generated > 0:\n",
    "         needs_relabel = False\n",
    "         node_iter = iter(G.nodes())\n",
    "         try:\n",
    "              first_node = next(node_iter)\n",
    "              if not isinstance(first_node, str):\n",
    "                   needs_relabel = True\n",
    "              # Check remaining only if first wasn't a string (optimization)\n",
    "              if needs_relabel is False: # Only check others if the first was okay\n",
    "                   while True:\n",
    "                       node = next(node_iter)\n",
    "                       if not isinstance(node, str):\n",
    "                           needs_relabel = True\n",
    "                           break # Found one, no need to check more\n",
    "         except StopIteration:\n",
    "              pass # Finished iterating\n",
    "\n",
    "         if needs_relabel:\n",
    "              node_mapping = {}\n",
    "              node_idx = 0\n",
    "              original_nodes = list(G.nodes()) # Create a list to iterate over original nodes\n",
    "              while node_idx < len(original_nodes):\n",
    "                   node = original_nodes[node_idx]\n",
    "                   node_mapping[node] = str(node) # Use string representation\n",
    "                   node_idx = node_idx + 1\n",
    "              G = nx.relabel_nodes(G, node_mapping, copy=False) # Use copy=False for efficiency\n",
    "    return G\n",
    "\n",
    "# --- 3. Metrics Calculation Helpers (Copied from Phase 1 Cell 2) ---\n",
    "def calculate_variance_norm(final_states_array):\n",
    "    \"\"\"Calculates variance across nodes, averaged across dimensions.\"\"\"\n",
    "    if final_states_array is None: return np.nan\n",
    "    # Ensure it's a numpy array\n",
    "    if not isinstance(final_states_array, np.ndarray):\n",
    "        try: final_states_array = np.array(final_states_array)\n",
    "        except Exception: return np.nan\n",
    "\n",
    "    num_nodes = final_states_array.shape[0]\n",
    "    if num_nodes == 0: return 0.0 # Variance is 0 for no nodes\n",
    "    # Check for sufficient dimensions\n",
    "    if final_states_array.ndim < 2: return np.var(final_states_array) if num_nodes > 0 else 0.0 # Handle 1D case\n",
    "\n",
    "    try:\n",
    "        variance_per_dim = np.var(final_states_array, axis=0)\n",
    "        mean_variance = np.mean(variance_per_dim)\n",
    "        # Check for NaN/Inf in result\n",
    "        if np.isnan(mean_variance) or np.isinf(mean_variance):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return mean_variance\n",
    "    except Exception as e_var:\n",
    "        warnings.warn(f\"Variance norm calculation failed: {e_var}\", RuntimeWarning)\n",
    "        return np.nan\n",
    "\n",
    "def calculate_entropy_binned(data_vector, bins=10, range_lims=(-1.5, 1.5)):\n",
    "    \"\"\"Calculates Shannon entropy for a single dimension using numpy histogram.\"\"\"\n",
    "    if data_vector is None: return np.nan\n",
    "    # Ensure data_vector is numpy array\n",
    "    if not isinstance(data_vector, np.ndarray):\n",
    "        try: data_vector = np.array(data_vector)\n",
    "        except Exception: return np.nan\n",
    "\n",
    "    if data_vector.size <= 1: return 0.0 # Entropy is 0 for single point or empty\n",
    "    try:\n",
    "        valid_data = data_vector[~np.isnan(data_vector)] # Filter NaNs\n",
    "        if valid_data.size <= 1: return 0.0\n",
    "        # Check range validity\n",
    "        use_dynamic_range = False\n",
    "        if range_lims is None or len(range_lims) != 2 or range_lims[0] >= range_lims[1]:\n",
    "            warnings.warn(f\"Invalid/missing range_lims for entropy: {range_lims}. Using data min/max.\", RuntimeWarning)\n",
    "            use_dynamic_range = True\n",
    "\n",
    "        if use_dynamic_range:\n",
    "            min_val = np.min(valid_data)\n",
    "            max_val = np.max(valid_data)\n",
    "            # If all values are the same, entropy is 0\n",
    "            if abs(min_val - max_val) < 1e-9: return 0.0\n",
    "            hist_range = (min_val, max_val)\n",
    "        else:\n",
    "            hist_range = range_lims\n",
    "\n",
    "        # Check number of bins\n",
    "        if not isinstance(bins, int) or bins <= 0:\n",
    "            warnings.warn(f\"Invalid bins value: {bins}. Using default 10.\", RuntimeWarning)\n",
    "            bins = 10\n",
    "\n",
    "        counts, bin_edges = np.histogram(valid_data, bins=bins, range=hist_range)\n",
    "        # Filter out zero counts before calculating probabilities\n",
    "        non_zero_counts = counts[counts > 0]\n",
    "        if non_zero_counts.size == 0: return 0.0 # No counts in any bin within the range\n",
    "\n",
    "        # Calculate entropy using scipy's entropy function (base=None means natural log)\n",
    "        entropy_value = calculate_scipy_entropy(non_zero_counts, base=None)\n",
    "        # Normalize by log(number of bins) if desired (optional, commented out)\n",
    "        # max_entropy = np.log(bins)\n",
    "        # normalized_entropy = entropy_value / max_entropy if max_entropy > 0 else 0.0\n",
    "        return entropy_value\n",
    "    except Exception as e_ent:\n",
    "        warnings.warn(f\"Entropy calculation failed: {e_ent}\", RuntimeWarning)\n",
    "        return np.nan\n",
    "\n",
    "def calculate_pairwise_dot_energy(final_states_array, adj_matrix_coo):\n",
    "    \"\"\"Calculates E = -0.5 * sum_{i<j} A[i,j] * dot(Si, Sj) using numpy and sparse COO\"\"\"\n",
    "    total_energy = 0.0\n",
    "    if final_states_array is None: return np.nan\n",
    "    # Ensure it's a numpy array\n",
    "    if not isinstance(final_states_array, np.ndarray):\n",
    "         try: final_states_array = np.array(final_states_array)\n",
    "         except Exception: return np.nan\n",
    "\n",
    "    num_nodes = final_states_array.shape[0]\n",
    "    state_dim = final_states_array.shape[1] if final_states_array.ndim > 1 else 1\n",
    "    if num_nodes == 0: return 0.0\n",
    "    if adj_matrix_coo is None: return 0.0\n",
    "\n",
    "    try:\n",
    "        # Ensure COO format\n",
    "        if not isinstance(adj_matrix_coo, coo_matrix):\n",
    "             try: adj_matrix_coo = coo_matrix(adj_matrix_coo)\n",
    "             except Exception: warnings.warn(\"Failed to convert adj matrix to COO for energy calc.\", RuntimeWarning); return np.nan\n",
    "\n",
    "        # Pre-extract COO data\n",
    "        adj_row = adj_matrix_coo.row\n",
    "        adj_col = adj_matrix_coo.col\n",
    "        adj_data = adj_matrix_coo.data\n",
    "        num_edges = len(adj_data)\n",
    "        edge_idx = 0\n",
    "\n",
    "        # Iterate through sparse matrix non-zero elements using while loop\n",
    "        while edge_idx < num_edges:\n",
    "            i = adj_row[edge_idx]\n",
    "            j = adj_col[edge_idx]\n",
    "            weight = adj_data[edge_idx]\n",
    "\n",
    "            # Process only upper triangle (i < j) to avoid double counting for undirected graphs\n",
    "            # Also ensure indices are within bounds\n",
    "            if i < j and i < num_nodes and j < num_nodes:\n",
    "                state_i = final_states_array[i, :]\n",
    "                state_j = final_states_array[j, :]\n",
    "                # Check if states are valid before dot product\n",
    "                if not np.isnan(state_i).any() and not np.isnan(state_j).any():\n",
    "                     dot_product = np.dot(state_i, state_j)\n",
    "                     if not np.isnan(dot_product): # Check dot product result\n",
    "                         total_energy = total_energy + (weight * dot_product)\n",
    "                     # else: warnings.warn(f\"NaN dot product encountered for edge ({i},{j}). Skipping.\", RuntimeWarning) # Reduce verbosity\n",
    "                # else: warnings.warn(f\"NaN state vector encountered for node {i} or {j}. Skipping edge.\", RuntimeWarning) # Reduce verbosity\n",
    "            # else: # Debugging index issues if needed\n",
    "            #     if not (i < num_nodes and j < num_nodes): warnings.warn(f\"Index out of bounds ({i},{j} vs N={num_nodes}). Skipping edge.\", RuntimeWarning)\n",
    "\n",
    "            edge_idx = edge_idx + 1 # Increment loop counter\n",
    "\n",
    "        # Apply the -0.5 factor\n",
    "        final_energy = -0.5 * total_energy\n",
    "        return final_energy\n",
    "    except Exception as e_en:\n",
    "        warnings.warn(f\"Energy calculation failed: {e_en}\", RuntimeWarning)\n",
    "        traceback.print_exc(limit=1)\n",
    "        return np.nan\n",
    "\n",
    "# --- 3.5 NEW: Phase 2 Metrics Calculation Helpers ---\n",
    "def calculate_mean_final_state_entropy(final_states_array, bins=10, range_lims=(-1.5, 1.5)):\n",
    "    \"\"\"Calculates entropy for each dimension and averages.\"\"\"\n",
    "    if final_states_array is None: return np.nan\n",
    "    # Ensure numpy array\n",
    "    if not isinstance(final_states_array, np.ndarray):\n",
    "        try: final_states_array = np.array(final_states_array)\n",
    "        except Exception: return np.nan\n",
    "\n",
    "    if final_states_array.ndim < 2: return np.nan # Need at least 2 dimensions (node, state_dim)\n",
    "    num_dims = final_states_array.shape[1]\n",
    "    if num_dims == 0: return 0.0 # No dimensions, entropy is 0\n",
    "\n",
    "    entropies = []\n",
    "    dim_idx = 0\n",
    "    while dim_idx < num_dims:\n",
    "        entropy_val = calculate_entropy_binned(final_states_array[:, dim_idx], bins=bins, range_lims=range_lims)\n",
    "        # Only append valid entropy values\n",
    "        if not np.isnan(entropy_val):\n",
    "            entropies.append(entropy_val)\n",
    "        dim_idx = dim_idx + 1\n",
    "\n",
    "    # Calculate mean only if we have valid entropies\n",
    "    if len(entropies) > 0:\n",
    "        mean_entropy = np.mean(entropies)\n",
    "        return mean_entropy\n",
    "    else:\n",
    "        # Return NaN if no valid entropies were calculated (e.g., all input arrays were constant)\n",
    "        return np.nan\n",
    "\n",
    "def calculate_relaxation_time(avg_change_history, conv_thresh, perturbation_end_step):\n",
    "    \"\"\"Estimates steps to reconverge after perturbation end.\"\"\"\n",
    "    # Check if history is valid and long enough\n",
    "    if avg_change_history is None: return np.nan\n",
    "    if not isinstance(avg_change_history, (list, np.ndarray)): return np.nan\n",
    "    history_len = len(avg_change_history)\n",
    "    # Check if perturbation end step is valid within the history\n",
    "    if not isinstance(perturbation_end_step, int) or perturbation_end_step < 0 or perturbation_end_step >= history_len:\n",
    "        return np.nan # Invalid end step\n",
    "\n",
    "    steps_after_perturb = 0\n",
    "    index = perturbation_end_step # Start checking from the step AFTER perturbation ends\n",
    "    converged_after_perturb = False\n",
    "    while index < history_len:\n",
    "        current_avg_change = avg_change_history[index]\n",
    "        # Check if change is below threshold\n",
    "        if current_avg_change < conv_thresh:\n",
    "            converged_after_perturb = True\n",
    "            break # Stop counting at first convergence\n",
    "        steps_after_perturb += 1\n",
    "        index += 1\n",
    "\n",
    "    # Return steps if converged, otherwise return NaN (or max steps checked?)\n",
    "    # Returning NaN indicates it *never* reached threshold within the observed history\n",
    "    if converged_after_perturb:\n",
    "        return steps_after_perturb\n",
    "    else:\n",
    "        return np.nan # Indicate failure to converge after perturbation within max_steps\n",
    "\n",
    "def calculate_perturbation_spread(final_states_perturbed, final_states_baseline, threshold=0.1):\n",
    "    \"\"\"Calculates fraction of nodes significantly changed by perturbation (Euclidean distance).\"\"\"\n",
    "    # Basic validation\n",
    "    if final_states_perturbed is None or final_states_baseline is None: return np.nan\n",
    "    if not isinstance(final_states_perturbed, np.ndarray) or not isinstance(final_states_baseline, np.ndarray): return np.nan\n",
    "    if final_states_perturbed.shape != final_states_baseline.shape: return np.nan\n",
    "\n",
    "    num_nodes = final_states_perturbed.shape[0]\n",
    "    if num_nodes == 0: return 0.0 # No nodes, no spread\n",
    "\n",
    "    try:\n",
    "        # Calculate Euclidean distance squared for efficiency, then take sqrt\n",
    "        diff_sq = (final_states_perturbed - final_states_baseline)**2\n",
    "        # Sum squared differences across state dimensions\n",
    "        dist_sq_per_node = np.sum(diff_sq, axis=1)\n",
    "        # Take square root to get Euclidean distance\n",
    "        distances = np.sqrt(dist_sq_per_node)\n",
    "\n",
    "        # Handle potential NaNs in distances if states had NaNs (should be filtered earlier ideally)\n",
    "        valid_distances = distances[~np.isnan(distances)]\n",
    "        if valid_distances.size == 0: return 0.0 # No valid distances to compare\n",
    "\n",
    "        # Count nodes where distance exceeds threshold\n",
    "        nodes_affected = np.sum(valid_distances > threshold)\n",
    "        # Calculate fraction relative to the number of nodes with valid distances\n",
    "        spread_fraction = float(nodes_affected) / float(valid_distances.size)\n",
    "        return spread_fraction\n",
    "    except Exception as e_spread:\n",
    "        warnings.warn(f\"Perturbation spread calculation failed: {e_spread}\", RuntimeWarning)\n",
    "        return np.nan\n",
    "\n",
    "# --- 4. Core PyTorch Step Function (Corrected JIT dim argument) ---\n",
    "# JIT requires type hints for rule params\n",
    "@torch.jit.script\n",
    "def hdc_5d_step_vectorized_torch(adj_sparse_tensor, current_states_tensor,\n",
    "                                 rule_params_activation_threshold: float, rule_params_activation_increase_rate: float,\n",
    "                                 rule_params_activation_decay_rate: float, rule_params_inhibition_threshold: float, # Unused but kept for signature\n",
    "                                 rule_params_inhibition_increase_rate: float, # Unused\n",
    "                                 rule_params_inhibition_decay_rate: float,\n",
    "                                 rule_params_inhibition_feedback_threshold: float, rule_params_inhibition_feedback_strength: float,\n",
    "                                 rule_params_diffusion_factor: float, rule_params_noise_level: float,\n",
    "                                 rule_params_harmonic_factor: float, rule_params_w_decay_rate: float,\n",
    "                                 rule_params_x_decay_rate: float, rule_params_y_decay_rate: float,\n",
    "                                 device: torch.device):\n",
    "    \"\"\" PyTorch implementation of the 5D HDC step function for GPU (JIT Compatible). \"\"\"\n",
    "    num_nodes = current_states_tensor.shape[0]\n",
    "    state_dim = current_states_tensor.shape[1] # Should be 5, but check if possible\n",
    "    if num_nodes == 0:\n",
    "        return current_states_tensor, torch.tensor(0.0, device=device)\n",
    "\n",
    "    # Extract states - Ensure slicing works if state_dim < 5\n",
    "    current_u = current_states_tensor[:, 0]\n",
    "    current_v = current_states_tensor[:, 1] if state_dim > 1 else torch.zeros_like(current_u)\n",
    "    current_w = current_states_tensor[:, 2] if state_dim > 2 else torch.zeros_like(current_u)\n",
    "    current_x = current_states_tensor[:, 3] if state_dim > 3 else torch.zeros_like(current_u)\n",
    "    current_y = current_states_tensor[:, 4] if state_dim > 4 else torch.zeros_like(current_u)\n",
    "\n",
    "    # Neighbor aggregation\n",
    "    adj_float = adj_sparse_tensor.float()\n",
    "    sum_neighbor_states = torch.sparse.mm(adj_float, current_states_tensor)\n",
    "    # Calculate degrees robustly\n",
    "    # *** CORRECTED LINE: Use dim=(1,) ***\n",
    "    degrees = torch.sparse.sum(adj_float, dim=(1,)).to_dense() # Convert degrees to dense tensor\n",
    "    degrees = degrees.unsqueeze(1) # Reshape to [N, 1] for broadcasting\n",
    "    # Avoid division by zero for isolated nodes\n",
    "    degrees = torch.max(degrees, torch.tensor(1.0, device=device))\n",
    "    mean_neighbor_states = sum_neighbor_states / degrees\n",
    "    # Use only 'u' dimension for activation influence\n",
    "    neighbor_u_sum = sum_neighbor_states[:, 0]\n",
    "    activation_influences = neighbor_u_sum # Using sum for activation influence\n",
    "\n",
    "    # Initialize Deltas\n",
    "    delta_u = torch.zeros_like(current_u)\n",
    "    delta_v = torch.zeros_like(current_v) # Need even if state_dim < 2 for stacking later\n",
    "    delta_w = torch.zeros_like(current_w) # Need even if state_dim < 3\n",
    "    delta_x = torch.zeros_like(current_x) # Need even if state_dim < 4\n",
    "    delta_y = torch.zeros_like(current_y) # Need even if state_dim < 5\n",
    "\n",
    "    # Apply Activation rules (u dimension)\n",
    "    act_increase_mask = activation_influences > rule_params_activation_threshold\n",
    "    increase_u_val = rule_params_activation_increase_rate * (1.0 - current_u) # Base increase towards 1\n",
    "    # Apply increase only where mask is True\n",
    "    delta_u = torch.where(act_increase_mask, delta_u + increase_u_val, delta_u)\n",
    "    # Apply decay universally\n",
    "    delta_u = delta_u - (rule_params_activation_decay_rate * current_u)\n",
    "\n",
    "    # Apply Inhibition rules (v dimension) - Only if state_dim > 1\n",
    "    if state_dim > 1:\n",
    "        inh_fb_mask = current_u > rule_params_inhibition_feedback_threshold\n",
    "        increase_v_val = rule_params_inhibition_feedback_strength * (1.0 - current_v) # Base increase towards 1\n",
    "        # Apply increase only where mask is True\n",
    "        delta_v = torch.where(inh_fb_mask, delta_v + increase_v_val, delta_v)\n",
    "        # Apply decay universally\n",
    "        delta_v = delta_v - (rule_params_inhibition_decay_rate * current_v)\n",
    "\n",
    "    # Apply Other decays (w, x, y dimensions) - Only if state_dim > 2, 3, 4 respectively\n",
    "    if state_dim > 2:\n",
    "        delta_w = delta_w - (rule_params_w_decay_rate * current_w)\n",
    "    if state_dim > 3:\n",
    "        delta_x = delta_x - (rule_params_x_decay_rate * current_x)\n",
    "    if state_dim > 4:\n",
    "        delta_y = delta_y - (rule_params_y_decay_rate * current_y)\n",
    "\n",
    "    # Combine deltas - Stack only the dimensions that exist\n",
    "    delta_list = [delta_u]\n",
    "    if state_dim > 1: delta_list.append(delta_v)\n",
    "    if state_dim > 2: delta_list.append(delta_w)\n",
    "    if state_dim > 3: delta_list.append(delta_x)\n",
    "    if state_dim > 4: delta_list.append(delta_y)\n",
    "    delta_states = torch.stack(delta_list, dim=1)\n",
    "\n",
    "    next_states_intermediate = current_states_tensor + delta_states\n",
    "\n",
    "    # Diffusion\n",
    "    diffusion_change = rule_params_diffusion_factor * (mean_neighbor_states - current_states_tensor)\n",
    "    next_states_intermediate = next_states_intermediate + diffusion_change\n",
    "\n",
    "    # Harmonic Term (acts on u dimension)\n",
    "    # Check harmonic factor is non-zero using a tolerance for floating point comparison\n",
    "    if abs(rule_params_harmonic_factor) > 1e-9:\n",
    "        # Ensure degrees has shape [N] for multiplication with sin output\n",
    "        harmonic_effect = rule_params_harmonic_factor * degrees.squeeze(-1) * torch.sin(neighbor_u_sum)\n",
    "        # Add effect only to the 'u' dimension (index 0)\n",
    "        next_states_intermediate[:, 0] = next_states_intermediate[:, 0] + harmonic_effect\n",
    "\n",
    "    # Noise\n",
    "    # Apply noise with the same shape as the current state tensor\n",
    "    noise = torch.rand_like(current_states_tensor).uniform_(-rule_params_noise_level, rule_params_noise_level)\n",
    "    next_states_noisy = next_states_intermediate + noise\n",
    "\n",
    "    # Clip\n",
    "    next_states_clipped = torch.clamp(next_states_noisy, min=-1.5, max=1.5)\n",
    "\n",
    "    # Change metric (average absolute change across all nodes and dimensions)\n",
    "    avg_state_change = torch.mean(torch.abs(next_states_clipped - current_states_tensor))\n",
    "\n",
    "    return next_states_clipped, avg_state_change\n",
    "\n",
    "\n",
    "# --- 5. MODIFIED Simulation Instance Runner for Phase 2 ---\n",
    "def run_single_instance_phase2(\n",
    "    graph, N, instance_params, trial_seed, rule_params_in,\n",
    "    max_steps, conv_thresh, state_dim,\n",
    "    calculate_energy=False, store_energy_history=False, # Inherited from Phase 1\n",
    "    energy_type='pairwise_dot', metrics_to_calc=None, device=None, # Inherited\n",
    "    # --- Phase 2 Additions ---\n",
    "    store_state_history=False, # Flag to store history\n",
    "    state_history_interval=1,  # Interval for storing history\n",
    "    perturbation_params=None, # Dictionary with perturbation details\n",
    "    phase2_metrics_to_calc=None # List of Phase 2 specific metrics\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Runs one NA simulation, MODIFIED for Phase 2.\n",
    "    Includes state history storage, perturbation application, and calculation\n",
    "    of Phase 2 metrics (e.g., relaxation time, spread, state entropy).\n",
    "    \"\"\"\n",
    "    # --- Combine metric lists ---\n",
    "    all_metrics_requested = (metrics_to_calc or []) + (phase2_metrics_to_calc or [])\n",
    "    all_metrics_requested = sorted(list(set(all_metrics_requested))) # Unique sorted list\n",
    "\n",
    "    # --- Default Error Result ---\n",
    "    nan_results = {metric: np.nan for metric in all_metrics_requested}\n",
    "    # Add standard outputs expected from Phase 1 worker\n",
    "    nan_results.update({\n",
    "        'convergence_time': 0, 'termination_reason': 'error_before_start',\n",
    "        'final_state_vector': None, 'final_energy': np.nan, 'energy_monotonic': False,\n",
    "        'error_message': 'Initialization failed'\n",
    "    })\n",
    "    # Add Phase 2 specific outputs that might be expected\n",
    "    nan_results.update({\n",
    "        'state_history': None, 'avg_change_history': None,\n",
    "        'relaxation_time': np.nan, 'perturbation_spread': np.nan,\n",
    "        'mean_final_state_entropy': np.nan,\n",
    "        'baseline_state_for_spread': None\n",
    "    })\n",
    "    # Identify primary sweep param for error reporting\n",
    "    primary_metric_name_default = instance_params.get('primary_metric', 'variance_norm')\n",
    "    nan_results['order_parameter'] = np.nan\n",
    "    nan_results['metric_name'] = primary_metric_name_default\n",
    "    nan_results['sensitivity_param_name'] = instance_params.get('rule_param_name')\n",
    "    nan_results['sensitivity_param_value'] = instance_params.get('rule_param_value')\n",
    "    # Find the sweep parameter key dynamically\n",
    "    param_key_nan = 'unknown_sweep_param' # Default\n",
    "    param_key_iter = iter(instance_params.keys())\n",
    "    current_key = next(param_key_iter, None)\n",
    "    while current_key is not None:\n",
    "         if isinstance(current_key, str) and current_key.endswith('_value'):\n",
    "              param_key_nan = current_key\n",
    "              break\n",
    "         current_key = next(param_key_iter, None)\n",
    "    nan_results[param_key_nan] = instance_params.get(param_key_nan, np.nan)\n",
    "\n",
    "    # --- Main Try-Except Block ---\n",
    "    try:\n",
    "        # --- Setup (Similar to Phase 1, ensure device handling) ---\n",
    "        if graph is None or graph.number_of_nodes() == 0:\n",
    "             nan_results['termination_reason']='empty_graph'; nan_results['error_message']='Received empty graph'; return nan_results\n",
    "\n",
    "        # Handle device specification (string or torch.device)\n",
    "        local_device = None\n",
    "        if isinstance(device, torch.device):\n",
    "            local_device = device\n",
    "        elif isinstance(device, str):\n",
    "            try:\n",
    "                local_device = torch.device(device)\n",
    "            except Exception as e_dev:\n",
    "                nan_results['termination_reason'] = 'device_error'; nan_results['error_message'] = f'Invalid device string: {device}, Error: {e_dev}'; return nan_results\n",
    "        else:\n",
    "            # Default to CPU if device argument is invalid or missing\n",
    "            local_device = torch.device('cpu')\n",
    "\n",
    "        # Seeding\n",
    "        np.random.seed(trial_seed); torch.manual_seed(trial_seed)\n",
    "        if local_device.type == 'cuda':\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(trial_seed)\n",
    "            else:\n",
    "                # Handle case where CUDA is requested but not available\n",
    "                nan_results['termination_reason'] = 'cuda_error'; nan_results['error_message'] = f'CUDA specified ({device}) but unavailable on worker.'; return nan_results\n",
    "\n",
    "        # Graph Processing (Adjacency Matrix)\n",
    "        node_list = sorted(list(graph.nodes())); num_nodes = len(node_list); adj_scipy_coo = None; adj_sparse_tensor = None\n",
    "        try:\n",
    "             # Get COO sparse matrix using NetworkX\n",
    "             adj_scipy_coo = nx.adjacency_matrix(graph, nodelist=node_list, weight=None).tocoo() # Use weight=None for unweighted adjacency\n",
    "             adj_indices = torch.LongTensor(np.vstack((adj_scipy_coo.row, adj_scipy_coo.col)))\n",
    "             adj_values = torch.ones(len(adj_scipy_coo.data), dtype=torch.float32) # Use 1 for unweighted graph\n",
    "             adj_shape = adj_scipy_coo.shape\n",
    "             # Create sparse tensor on the target device\n",
    "             adj_sparse_tensor = torch.sparse_coo_tensor(adj_indices, adj_values, adj_shape, device=local_device)\n",
    "        except Exception as adj_e:\n",
    "             nan_results['termination_reason'] = 'adj_error'; nan_results['error_message'] = f'Adj matrix creation failed: {adj_e}'; return nan_results\n",
    "\n",
    "        # Rule Parameters\n",
    "        rule_params = rule_params_in.copy()\n",
    "        if instance_params.get('rule_param_name') and instance_params.get('rule_param_value') is not None:\n",
    "             rule_params[instance_params['rule_param_name']] = instance_params['rule_param_value']\n",
    "        # Extract individual params for JIT function, with defaults\n",
    "        rp_act_thresh=float(rule_params.get('activation_threshold', 0.5)); rp_act_inc=float(rule_params.get('activation_increase_rate', 0.15)); rp_act_dec=float(rule_params.get('activation_decay_rate', 0.05))\n",
    "        rp_inh_thresh=float(rule_params.get('inhibition_threshold', 0.5)); rp_inh_inc=float(rule_params.get('inhibition_increase_rate', 0.1)); rp_inh_dec=float(rule_params.get('inhibition_decay_rate', 0.1))\n",
    "        rp_inh_fb_thresh=float(rule_params.get('inhibition_feedback_threshold', 0.6)); rp_inh_fb_str=float(rule_params.get('inhibition_feedback_strength', 0.3))\n",
    "        rp_diff=float(rule_params.get('diffusion_factor', 0.05)); rp_noise=float(rule_params.get('noise_level', 0.001)); rp_harm=float(rule_params.get('harmonic_factor', 0.05))\n",
    "        rp_w_dec=float(rule_params.get('w_decay_rate', 0.05)); rp_x_dec=float(rule_params.get('x_decay_rate', 0.05)); rp_y_dec=float(rule_params.get('y_decay_rate', 0.05))\n",
    "\n",
    "        # --- Initialization ---\n",
    "        # Initial state tensor on the target device\n",
    "        initial_states_tensor = torch.FloatTensor(num_nodes, state_dim).uniform_(-0.1, 0.1).to(local_device)\n",
    "        current_states_tensor = initial_states_tensor\n",
    "\n",
    "        # History Storage Initialization\n",
    "        state_history_list = [] # Stores state numpy arrays (on CPU)\n",
    "        avg_change_history_list = [] # Stores scalar avg change values\n",
    "        energy_history_np = [] # Stores scalar energy values (if calculated)\n",
    "\n",
    "        if store_state_history and state_history_interval > 0:\n",
    "             # Ensure interval is at least 1\n",
    "             current_state_history_interval = max(1, int(state_history_interval))\n",
    "             # Store initial state (step 0)\n",
    "             state_history_list.append(current_states_tensor.cpu().numpy().copy())\n",
    "\n",
    "        if calculate_energy and store_energy_history:\n",
    "            try:\n",
    "                 initial_energy = calculate_pairwise_dot_energy(current_states_tensor.cpu().numpy(), adj_scipy_coo)\n",
    "                 energy_history_np.append(initial_energy)\n",
    "            except Exception: energy_history_np.append(np.nan) # Handle potential error\n",
    "\n",
    "        # Perturbation Setup\n",
    "        is_perturbation_run = False # Default\n",
    "        perturb_start = -1; perturb_end = -1; perturb_nodes_indices = []; perturb_dim = -1; perturb_val = 0.0\n",
    "        baseline_final_state_for_spread = None # Store unperturbed state if needed\n",
    "\n",
    "        if perturbation_params is not None and isinstance(perturbation_params, dict):\n",
    "            perturb_start = perturbation_params.get('apply_at_step', -1)\n",
    "            perturb_duration = perturbation_params.get('duration_steps', 0)\n",
    "            perturb_node_frac = perturbation_params.get('target_node_fraction', 0)\n",
    "            perturb_dim = perturbation_params.get('target_dimension', -1)\n",
    "            perturb_val = perturbation_params.get('perturbation_value', 0.0)\n",
    "\n",
    "            # Validate perturbation params\n",
    "            valid_perturb_params = True\n",
    "            if not isinstance(perturb_start, int) or perturb_start < 0 or perturb_start >= max_steps: valid_perturb_params = False\n",
    "            if not isinstance(perturb_duration, int) or perturb_duration <= 0: valid_perturb_params = False\n",
    "            if not isinstance(perturb_node_frac, (float, int)) or not (0 < perturb_node_frac <= 1.0): valid_perturb_params = False\n",
    "            if not isinstance(perturb_dim, int) or not (0 <= perturb_dim < state_dim): valid_perturb_params = False\n",
    "            # Allow perturbation value to be any float\n",
    "\n",
    "            if valid_perturb_params:\n",
    "                 is_perturbation_run = True\n",
    "                 perturb_end = perturb_start + perturb_duration # End step is exclusive\n",
    "                 # Select nodes to perturb\n",
    "                 num_perturb_nodes = max(1, int(num_nodes * perturb_node_frac))\n",
    "                 # Ensure we don't select more nodes than available\n",
    "                 num_perturb_nodes = min(num_perturb_nodes, num_nodes)\n",
    "                 perturb_nodes_indices = np.random.choice(num_nodes, num_perturb_nodes, replace=False).tolist()\n",
    "                 # print(f\"Worker {trial_seed}: Perturbation enabled. Steps {perturb_start}-{perturb_end}, Nodes: {len(perturb_nodes_indices)}, Dim: {perturb_dim}, Val: {perturb_val}\") # Debug print\n",
    "            else:\n",
    "                 warnings.warn(f\"Invalid perturbation parameters: {perturbation_params}. Disabling perturbation.\", RuntimeWarning)\n",
    "                 is_perturbation_run = False\n",
    "\n",
    "\n",
    "        # --- Simulation Loop ---\n",
    "        termination_reason = \"max_steps_reached\"\n",
    "        steps_run = 0\n",
    "        avg_change_cpu = torch.inf # Initialize with infinity\n",
    "        next_states_tensor = None # Define before loop\n",
    "\n",
    "        step = 0\n",
    "        while step < max_steps:\n",
    "            steps_run = step + 1\n",
    "\n",
    "            # --- Apply Perturbation ---\n",
    "            perturbation_active_this_step = False\n",
    "            input_state_tensor = current_states_tensor # Default input is current state\n",
    "            if is_perturbation_run:\n",
    "                 is_within_perturb_window = step >= perturb_start and step < perturb_end\n",
    "                 if is_within_perturb_window:\n",
    "                      perturbation_active_this_step = True\n",
    "                      # Create a copy to modify for perturbation step\n",
    "                      perturbed_state_tensor = current_states_tensor.clone()\n",
    "                      # Clamp the target dimension for the selected nodes\n",
    "                      perturbed_state_tensor[perturb_nodes_indices, perturb_dim] = perturb_val\n",
    "                      # Use the perturbed state as input for this step's calculation\n",
    "                      input_state_tensor = perturbed_state_tensor\n",
    "                      # Note: input_state_tensor now references perturbed_state_tensor\n",
    "                 # else: input_state_tensor remains current_states_tensor\n",
    "\n",
    "            # --- Execute GPU Step ---\n",
    "            try:\n",
    "                next_states_tensor, avg_change_tensor = hdc_5d_step_vectorized_torch(\n",
    "                    adj_sparse_tensor, input_state_tensor, # Use potentially perturbed input\n",
    "                    rp_act_thresh, rp_act_inc, rp_act_dec, rp_inh_thresh, rp_inh_inc, rp_inh_dec,\n",
    "                    rp_inh_fb_thresh, rp_inh_fb_str, rp_diff, rp_noise, rp_harm,\n",
    "                    rp_w_dec, rp_x_dec, rp_y_dec, local_device\n",
    "                )\n",
    "            except Exception as step_e:\n",
    "                 termination_reason = \"error_in_gpu_step\"; nan_results['termination_reason'] = termination_reason; nan_results['convergence_time'] = steps_run\n",
    "                 nan_results['error_message'] = f\"GPU step {steps_run} fail: {type(step_e).__name__}: {step_e}|TB:{traceback.format_exc(limit=1)}\"\n",
    "                 # Try to salvage last valid state\n",
    "                 try: final_states_np_err = current_states_tensor.cpu().numpy(); nan_results['final_state_vector'] = final_states_np_err.flatten()\n",
    "                 except Exception: pass\n",
    "                 # Clean up GPU memory before returning error\n",
    "                 # Use 'del ...' with checks for existence\n",
    "                 if 'adj_sparse_tensor' in locals(): del adj_sparse_tensor\n",
    "                 if 'current_states_tensor' in locals(): del current_states_tensor\n",
    "                 if 'initial_states_tensor' in locals(): del initial_states_tensor\n",
    "                 if 'next_states_tensor' in locals() and next_states_tensor is not None: del next_states_tensor\n",
    "                 if 'input_state_tensor' in locals() and input_state_tensor is not None: del input_state_tensor\n",
    "                 if 'perturbed_state_tensor' in locals() and perturbed_state_tensor is not None: del perturbed_state_tensor\n",
    "                 if local_device.type == 'cuda': torch.cuda.empty_cache()\n",
    "                 return nan_results # Return error dict\n",
    "\n",
    "            # --- Store History ---\n",
    "            avg_change_cpu = avg_change_tensor.item() # Get Python float\n",
    "            avg_change_history_list.append(avg_change_cpu)\n",
    "\n",
    "            # Store state history at specified intervals\n",
    "            if store_state_history and state_history_interval > 0:\n",
    "                # Check if current step is a multiple of interval OR the very last step\n",
    "                is_storage_step = (step % current_state_history_interval == 0)\n",
    "                is_last_step = (step == max_steps - 1)\n",
    "                if is_storage_step or is_last_step:\n",
    "                     state_history_list.append(next_states_tensor.cpu().numpy().copy())\n",
    "\n",
    "            # Store energy history if requested\n",
    "            if calculate_energy and store_energy_history:\n",
    "                 try:\n",
    "                     current_energy = calculate_pairwise_dot_energy(next_states_tensor.cpu().numpy(), adj_scipy_coo)\n",
    "                     energy_history_np.append(current_energy)\n",
    "                 except Exception: energy_history_np.append(np.nan) # Handle potential error\n",
    "\n",
    "            # --- Check Convergence (unless perturbation is active) ---\n",
    "            converged = False\n",
    "            # Only check for convergence if perturbation is NOT currently active\n",
    "            if not perturbation_active_this_step:\n",
    "                 if avg_change_cpu < conv_thresh:\n",
    "                      converged = True\n",
    "                      termination_reason = f\"convergence_at_step_{step+1}\"\n",
    "                      # If this is a perturbation run, store the baseline state IF convergence happened *before* perturbation start\n",
    "                      if is_perturbation_run and step < perturb_start and baseline_final_state_for_spread is None:\n",
    "                           baseline_final_state_for_spread = next_states_tensor.cpu().numpy().copy()\n",
    "\n",
    "            # Update state for next iteration\n",
    "            current_states_tensor = next_states_tensor\n",
    "\n",
    "            # Break loop if converged (and not during perturbation phase)\n",
    "            # We need to ensure the simulation runs AT LEAST until perturb_end if perturbing\n",
    "            should_break = False\n",
    "            if converged:\n",
    "                 if is_perturbation_run:\n",
    "                      # If perturbing, only break if convergence happens *after* perturbation window ends\n",
    "                      if step >= perturb_end:\n",
    "                           should_break = True\n",
    "                 else:\n",
    "                      # If not perturbing, break immediately upon convergence\n",
    "                      should_break = True\n",
    "\n",
    "            if should_break:\n",
    "                 break # Exit loop\n",
    "\n",
    "            # Increment step counter\n",
    "            step = step + 1\n",
    "        # --- End Simulation Loop ---\n",
    "\n",
    "        # If loop finished due to max_steps, capture baseline state if perturbing\n",
    "        if is_perturbation_run and termination_reason == \"max_steps_reached\" and baseline_final_state_for_spread is None:\n",
    "             # Find the state just before perturbation began, if history allows\n",
    "             if store_state_history and perturb_start > 0:\n",
    "                  history_index_before_perturb = perturb_start // current_state_history_interval # Index in history list\n",
    "                  if history_index_before_perturb < len(state_history_list):\n",
    "                       baseline_final_state_for_spread = state_history_list[history_index_before_perturb]\n",
    "                  # else: Could not find pre-perturbation state in history\n",
    "             # else: Cannot determine baseline if history not stored\n",
    "\n",
    "        # --- Final State & Metrics ---\n",
    "        final_states_np = current_states_tensor.cpu().numpy() # Get final state to CPU\n",
    "\n",
    "        # --- Create Results Dictionary ---\n",
    "        results = {\n",
    "            'convergence_time': steps_run,\n",
    "            'termination_reason': termination_reason,\n",
    "            'final_state_vector': final_states_np.flatten(), # Flatten for easy saving/PCA later\n",
    "            'error_message': None\n",
    "        }\n",
    "\n",
    "        # --- Add Sweep Parameters to Results ---\n",
    "        # Find sweep key dynamically\n",
    "        param_key = 'unknown_sweep_param' # Default\n",
    "        param_key_iter_res = iter(instance_params.keys())\n",
    "        current_key_res = next(param_key_iter_res, None)\n",
    "        while current_key_res is not None:\n",
    "             if isinstance(current_key_res, str) and current_key_res.endswith('_value'):\n",
    "                  param_key = current_key_res\n",
    "                  break\n",
    "             current_key_res = next(param_key_iter_res, None)\n",
    "\n",
    "        if param_key != 'unknown_sweep_param':\n",
    "            results[param_key] = instance_params[param_key]\n",
    "        else:\n",
    "            results['unknown_sweep_param'] = np.nan # Fallback\n",
    "        results['sensitivity_param_name'] = instance_params.get('rule_param_name')\n",
    "        results['sensitivity_param_value'] = instance_params.get('rule_param_value')\n",
    "\n",
    "        # --- Calculate Standard Metrics (Phase 1 Style) ---\n",
    "        metric_idx = 0\n",
    "        if metrics_to_calc is None: metrics_to_calc = [] # Ensure it's a list\n",
    "        while metric_idx < len(metrics_to_calc):\n",
    "             metric = metrics_to_calc[metric_idx]\n",
    "             if metric == 'variance_norm':\n",
    "                  results[metric] = calculate_variance_norm(final_states_np)\n",
    "             elif metric == 'entropy_dim_0': # Example specific dim entropy\n",
    "                  if state_dim > 0:\n",
    "                      # Check if final_states_np has enough columns\n",
    "                      if final_states_np.shape[1] > 0:\n",
    "                           results[metric] = calculate_entropy_binned(final_states_np[:, 0])\n",
    "                      else: results[metric] = np.nan\n",
    "                  else: results[metric] = np.nan\n",
    "             # Add other standard metrics here if needed\n",
    "             # else:\n",
    "             #     # Ensure not overwriting calculated metrics like final_energy\n",
    "             #     if metric not in results:\n",
    "             #          results[metric] = np.nan\n",
    "             metric_idx = metric_idx + 1\n",
    "\n",
    "        # Energy Calculation (Final state)\n",
    "        is_monotonic_result = np.nan # Use NaN for unknown/not calculated\n",
    "        if calculate_energy:\n",
    "            results['final_energy'] = calculate_pairwise_dot_energy(final_states_np, adj_scipy_coo)\n",
    "            # Monotonicity check (if history stored)\n",
    "            if store_energy_history and len(energy_history_np) > 1:\n",
    "                 valid_energy_hist = np.array(energy_history_np)\n",
    "                 valid_energy_hist = valid_energy_hist[~np.isnan(valid_energy_hist)] # Remove NaNs\n",
    "                 if len(valid_energy_hist) > 1:\n",
    "                      diffs = np.diff(valid_energy_hist)\n",
    "                      # Check if all differences are non-positive (allowing for small numerical errors)\n",
    "                      is_monotonic_result = bool(np.all(diffs <= 1e-6))\n",
    "                 else: is_monotonic_result = True # Considered monotonic if only 0/1 valid points\n",
    "            else: is_monotonic_result = np.nan # Cannot determine without history\n",
    "            results['energy_monotonic'] = is_monotonic_result\n",
    "        else:\n",
    "            results['final_energy'] = np.nan\n",
    "            results['energy_monotonic'] = np.nan # Can't determine without calculation\n",
    "\n",
    "        # Primary Order Parameter\n",
    "        primary_metric_name = instance_params.get('primary_metric', 'variance_norm')\n",
    "        # Use .get() on results dict to fetch the calculated value\n",
    "        results['order_parameter'] = results.get(primary_metric_name, np.nan)\n",
    "        results['metric_name'] = primary_metric_name\n",
    "\n",
    "        # --- Calculate Phase 2 Metrics ---\n",
    "        # Store avg change sequence (can be useful for debugging convergence)\n",
    "        results['avg_change_history'] = avg_change_history_list if store_state_history else None\n",
    "\n",
    "        # Store state history if requested (potentially large!)\n",
    "        # Consider only storing if specifically needed downstream\n",
    "        results['state_history'] = state_history_list if store_state_history else None\n",
    "\n",
    "        # Mean Final State Entropy (Example Info Metric)\n",
    "        if 'mean_final_state_entropy' in all_metrics_requested:\n",
    "             results['mean_final_state_entropy'] = calculate_mean_final_state_entropy(final_states_np)\n",
    "        else: # Ensure key exists if not requested\n",
    "            results['mean_final_state_entropy'] = np.nan\n",
    "\n",
    "\n",
    "        # Perturbation Metrics\n",
    "        if is_perturbation_run:\n",
    "             # Relaxation Time\n",
    "             if 'relaxation_time' in all_metrics_requested:\n",
    "                   # Ensure perturb_end is valid before calculating\n",
    "                   pert_end_idx = perturb_end if perturb_end is not None and perturb_end >=0 else -1\n",
    "                   results['relaxation_time'] = calculate_relaxation_time(avg_change_history_list, conv_thresh, pert_end_idx)\n",
    "             else: results['relaxation_time'] = np.nan\n",
    "\n",
    "             # Perturbation Spread (Requires baseline run result - comparison done *after* collecting results)\n",
    "             # Store the baseline state captured before/during loop\n",
    "             results['baseline_state_for_spread'] = baseline_final_state_for_spread\n",
    "             # Placeholder for spread - calculated later\n",
    "             results['perturbation_spread'] = np.nan\n",
    "\n",
    "        else: # Ensure keys exist even if perturbation didn't run\n",
    "             results['relaxation_time'] = np.nan\n",
    "             results['perturbation_spread'] = np.nan\n",
    "             results['baseline_state_for_spread'] = None\n",
    "\n",
    "\n",
    "        # --- Final Cleanup ---\n",
    "        # Use 'del ...' with checks for existence more robustly\n",
    "        if 'adj_sparse_tensor' in locals(): del adj_sparse_tensor\n",
    "        if 'current_states_tensor' in locals(): del current_states_tensor\n",
    "        if 'initial_states_tensor' in locals(): del initial_states_tensor\n",
    "        if 'next_states_tensor' in locals() and next_states_tensor is not None: del next_states_tensor\n",
    "        if 'input_state_tensor' in locals() and input_state_tensor is not None: del input_state_tensor\n",
    "        if 'perturbed_state_tensor' in locals() and perturbed_state_tensor is not None: del perturbed_state_tensor\n",
    "        if local_device.type == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "        return results # Return success results\n",
    "\n",
    "    # --- Top-Level Exception Handling ---\n",
    "    except Exception as worker_e:\n",
    "         tb_str = traceback.format_exc(limit=2) # Get more traceback info\n",
    "         nan_results['termination_reason'] = 'unhandled_worker_error'\n",
    "         nan_results['error_message'] = f\"Unhandled Worker Error: {type(worker_e).__name__}: {worker_e} | TB: {tb_str}\"\n",
    "         # Try to capture final state if possible\n",
    "         try:\n",
    "             if 'current_states_tensor' in locals() and current_states_tensor is not None:\n",
    "                 nan_results['final_state_vector'] = current_states_tensor.cpu().numpy().flatten()\n",
    "         except Exception: pass # Ignore errors during error handling\n",
    "         # Attempt cleanup even during error\n",
    "         try:\n",
    "             if 'adj_sparse_tensor' in locals(): del adj_sparse_tensor\n",
    "             if 'current_states_tensor' in locals(): del current_states_tensor\n",
    "             if 'initial_states_tensor' in locals(): del initial_states_tensor\n",
    "             if 'next_states_tensor' in locals() and next_states_tensor is not None: del next_states_tensor\n",
    "             if 'input_state_tensor' in locals() and input_state_tensor is not None: del input_state_tensor\n",
    "             if 'perturbed_state_tensor' in locals() and perturbed_state_tensor is not None: del perturbed_state_tensor\n",
    "             if 'local_device' in locals() and local_device is not None and local_device.type == 'cuda':\n",
    "                 torch.cuda.empty_cache()\n",
    "         except NameError: pass # Variables might not be defined if error happened early\n",
    "         return nan_results\n",
    "\n",
    "# --- 6. Fitting Function (Copied from Phase 1 Cell 2) ---\n",
    "def reversed_sigmoid_func(x, A, x0, k, C):\n",
    "    \"\"\"Reversed sigmoid function (decreasing S-shape). Includes numerical stability.\"\"\"\n",
    "    try:\n",
    "        # Ensure input is a numpy array\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        # Calculate the exponent term, clipping to prevent overflow/underflow\n",
    "        exp_term = k * (x - x0)\n",
    "        # Clip range determined empirically or theoretically; -700 to 700 covers most double precision\n",
    "        exp_term = np.clip(exp_term, -700, 700)\n",
    "        # Calculate denominator, adding small epsilon if needed, though clip often suffices\n",
    "        denominator = 1.0 + np.exp(exp_term)\n",
    "        # Avoid division by zero, although np.exp result should be positive\n",
    "        denominator = np.where(denominator == 0, 1e-300, denominator) # Use a very small number\n",
    "        # Calculate the final result\n",
    "        result = A / denominator + C\n",
    "        # Handle potential NaNs or Infs resulting from extreme values (though clip helps)\n",
    "        result = np.nan_to_num(result, nan=np.nan, posinf=np.nan, neginf=np.nan)\n",
    "        return result\n",
    "    except Exception as e_sig:\n",
    "        warnings.warn(f\"Sigmoid calculation failed: {e_sig}\", RuntimeWarning)\n",
    "        # Return an array of NaNs with the same shape as input x on error\n",
    "        return np.full_like(x, np.nan, dtype=float)\n",
    "\n",
    "\n",
    "# --- 7. UMAP Helper Function ---\n",
    "def run_umap_analysis(data_matrix, umap_params, random_state=42):\n",
    "    \"\"\" Performs UMAP dimensionality reduction. \"\"\"\n",
    "    if not UMAP_AVAILABLE:\n",
    "        print(\"UMAP not available. Skipping UMAP analysis.\")\n",
    "        return None, None\n",
    "\n",
    "    # Check if data_matrix is valid\n",
    "    if data_matrix is None:\n",
    "         print(\"‚ö†Ô∏è UMAP input data_matrix is None. Skipping.\")\n",
    "         return None, None\n",
    "    if not isinstance(data_matrix, np.ndarray):\n",
    "         print(\"‚ö†Ô∏è UMAP input data_matrix is not a NumPy array. Skipping.\")\n",
    "         return None, None\n",
    "\n",
    "    # Check minimum number of samples required by UMAP (n_neighbors)\n",
    "    n_neighbors_umap = umap_params.get('n_neighbors', 5) # Default UMAP n_neighbors is 15, but use 5 as a safe minimum check\n",
    "    if data_matrix.shape[0] < n_neighbors_umap:\n",
    "        print(f\"‚ö†Ô∏è Not enough data points ({data_matrix.shape[0]}) for UMAP with n_neighbors={n_neighbors_umap}. Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"  Running UMAP with params: {umap_params} on data shape {data_matrix.shape}\")\n",
    "    try:\n",
    "        # Initialize UMAP reducer\n",
    "        reducer = umap.UMAP(\n",
    "            n_neighbors=umap_params.get('n_neighbors', 15),\n",
    "            min_dist=umap_params.get('min_dist', 0.1),\n",
    "            n_components=umap_params.get('n_components', 2),\n",
    "            metric=umap_params.get('metric', 'euclidean'),\n",
    "            random_state=random_state,\n",
    "            verbose=False # Keep console clean\n",
    "        )\n",
    "        # Fit and transform the data\n",
    "        embedding = reducer.fit_transform(data_matrix)\n",
    "        print(f\"  UMAP embedding generated with shape: {embedding.shape}\")\n",
    "        return embedding, reducer # Return both embedding and the fitted reducer model\n",
    "    except Exception as e_umap:\n",
    "        print(f\"‚ùå Error during UMAP analysis: {e_umap}\")\n",
    "        traceback.print_exc(limit=2)\n",
    "        return None, None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Phase 2 Helper functions defined (incl. modified worker `run_single_instance_phase2`, UMAP helper, JIT fix).\")\n",
    "print(\"\\nCell 2 execution complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f7c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 4: Order Parameter Function Definitions (Emergenics - Full) ---\n",
      "‚úÖ Cell 4: Order parameter functions defined (some assume older dict format, added array-based calculator).\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Order Parameter Function Definitions (Emergenics - Full)\n",
    "# Description: Defines functions to compute order parameters from 5D simulation states.\n",
    "# Includes calculation of flattened state vector.\n",
    "# Adheres strictly to one statement per line after colons.\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "print(\"\\n--- Cell 4: Order Parameter Function Definitions (Emergenics - Full) ---\")\n",
    "\n",
    "# --- Helper: Convert State Dictionary to Numpy Array ---\n",
    "# This function was part of an older design using state dictionaries.\n",
    "# The current worker `run_single_instance_phase2` returns numpy arrays directly.\n",
    "# Keeping it here for potential backward compatibility or alternative use cases.\n",
    "def state_dict_to_array(state_dict, node_list_local, state_dim):\n",
    "    \"\"\"Converts a state dictionary {node_id: state_vector} to a NumPy array [N, D].\"\"\"\n",
    "    num_nodes = 0\n",
    "    if isinstance(node_list_local, list):\n",
    "        num_nodes = len(node_list_local)\n",
    "\n",
    "    # Initialize array with NaNs\n",
    "    state_array = np.full((num_nodes, state_dim), np.nan, dtype=float)\n",
    "\n",
    "    # Check if input is a valid dictionary\n",
    "    if not isinstance(state_dict, dict):\n",
    "        warnings.warn(\"state_dict_to_array received non-dict input.\", RuntimeWarning)\n",
    "        return state_array # Return NaN array\n",
    "\n",
    "    # Check if node list is valid\n",
    "    if num_nodes == 0:\n",
    "         warnings.warn(\"state_dict_to_array received empty node list.\", RuntimeWarning)\n",
    "         return state_array # Return NaN array\n",
    "\n",
    "    # Define a default state vector (e.g., NaNs or zeros) for missing nodes\n",
    "    default_state_vec = np.full(state_dim, np.nan, dtype=float)\n",
    "\n",
    "    # Iterate through the provided node list to ensure correct order\n",
    "    node_idx = 0\n",
    "    while node_idx < num_nodes:\n",
    "        node_id = node_list_local[node_idx]\n",
    "        # Get the state vector from the dictionary, use default if missing\n",
    "        state_vec = state_dict.get(node_id, default_state_vec)\n",
    "\n",
    "        # Validate the retrieved state vector\n",
    "        vector_is_valid = False\n",
    "        if isinstance(state_vec, np.ndarray):\n",
    "             if state_vec.shape == (state_dim,):\n",
    "                  vector_is_valid = True\n",
    "\n",
    "        # Assign the vector to the array if valid, otherwise NaNs remain\n",
    "        if vector_is_valid:\n",
    "            state_array[node_idx, :] = state_vec\n",
    "        # else: state_array remains NaN at this row\n",
    "\n",
    "        node_idx = node_idx + 1 # Increment loop counter\n",
    "\n",
    "    return state_array\n",
    "\n",
    "# --- Helper: Get state values for a specific dimension ---\n",
    "# Also part of the older dictionary-based design.\n",
    "def get_state_dimension_values(state_dict, node_list_local, dim_index, state_dim):\n",
    "    \"\"\"Extracts values for a single state dimension from a state dictionary.\"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(state_dict, dict) or not state_dict:\n",
    "        warnings.warn(\"get_state_dimension_values: Invalid state_dict.\", RuntimeWarning)\n",
    "        return np.array([], dtype=float) # Return empty array\n",
    "    if not isinstance(node_list_local, list) or not node_list_local:\n",
    "        warnings.warn(\"get_state_dimension_values: Invalid node_list_local.\", RuntimeWarning)\n",
    "        return np.array([], dtype=float)\n",
    "    if not isinstance(dim_index, int) or not (0 <= dim_index < state_dim):\n",
    "        warnings.warn(f\"get_state_dimension_values: Invalid dim_index {dim_index} for state_dim {state_dim}.\", RuntimeWarning)\n",
    "        return np.array([], dtype=float)\n",
    "\n",
    "    # Extract values\n",
    "    values_list = []\n",
    "    default_val = np.nan # Value to use if node or state is invalid\n",
    "\n",
    "    node_idx = 0\n",
    "    while node_idx < len(node_list_local):\n",
    "        node_id = node_list_local[node_idx]\n",
    "        state_vec = state_dict.get(node_id) # Get state vector or None\n",
    "\n",
    "        # Check if the retrieved state vector is valid\n",
    "        value_to_append = default_val # Assume invalid initially\n",
    "        if isinstance(state_vec, np.ndarray):\n",
    "             if state_vec.shape == (state_dim,):\n",
    "                  # Vector is valid shape, extract the value at the specified dimension\n",
    "                  value_at_dim = state_vec[dim_index]\n",
    "                  # Check if the extracted value is a valid number (not NaN/Inf)\n",
    "                  if np.isfinite(value_at_dim):\n",
    "                       value_to_append = value_at_dim\n",
    "\n",
    "        values_list.append(value_to_append) # Append the value (or default NaN)\n",
    "        node_idx = node_idx + 1 # Increment loop counter\n",
    "\n",
    "    # Convert the list of values to a NumPy array\n",
    "    return np.array(values_list, dtype=float)\n",
    "\n",
    "# --- Order Parameter Functions (using state dictionaries - adapt if using arrays directly) ---\n",
    "# These functions assume the older state_dict format. They would need modification\n",
    "# or replacement if operating directly on the final_state_np array from the worker.\n",
    "\n",
    "def compute_variance_norm(state_dict, node_list_local, state_dim):\n",
    "    \"\"\"Calculates variance of the L2 norm of state vectors across nodes.\"\"\"\n",
    "    norms = []\n",
    "    # Input validation\n",
    "    dict_is_valid = isinstance(state_dict, dict)\n",
    "    node_list_valid = isinstance(node_list_local, list) and len(node_list_local) > 0\n",
    "\n",
    "    if dict_is_valid and node_list_valid:\n",
    "        node_idx = 0\n",
    "        while node_idx < len(node_list_local): # Iterate through nodes\n",
    "            node = node_list_local[node_idx]\n",
    "            vec = state_dict.get(node) # Get state vector for the node\n",
    "\n",
    "            # Check if the vector is valid\n",
    "            vec_is_valid_type = isinstance(vec, np.ndarray)\n",
    "            vec_is_valid_shape = False\n",
    "            if vec_is_valid_type:\n",
    "                 if vec.shape == (state_dim,):\n",
    "                      vec_is_valid_shape = True\n",
    "\n",
    "            if vec_is_valid_shape:\n",
    "                 # Calculate L2 norm (Euclidean distance from origin)\n",
    "                 try:\n",
    "                      norm_val = np.linalg.norm(vec)\n",
    "                      # Check if norm is a valid finite number\n",
    "                      norm_is_valid_number = False\n",
    "                      if not (np.isnan(norm_val) or np.isinf(norm_val)):\n",
    "                           norm_is_valid_number = True\n",
    "\n",
    "                      if norm_is_valid_number:\n",
    "                           norms.append(norm_val) # Add valid norm to list\n",
    "                 except Exception as e_norm:\n",
    "                      warnings.warn(f\"Norm calculation failed for node {node}: {e_norm}\", RuntimeWarning)\n",
    "                      # Do not append norm if calculation fails\n",
    "\n",
    "            node_idx = node_idx + 1 # Increment loop counter\n",
    "\n",
    "    # Calculate variance if we have at least two valid norms\n",
    "    variance_value = np.nan # Default to NaN\n",
    "    have_enough_valid_norms = len(norms) >= 2 # Need at least 2 points for variance\n",
    "    if have_enough_valid_norms:\n",
    "         try:\n",
    "             variance_value = np.var(norms) # Calculate variance of the collected norms\n",
    "         except Exception as e_var:\n",
    "              warnings.warn(f\"Variance calculation failed: {e_var}\", RuntimeWarning)\n",
    "              variance_value = np.nan # Set to NaN on error\n",
    "\n",
    "    return variance_value\n",
    "\n",
    "\n",
    "def compute_variance_dim_N(state_dict, node_list_local, dim_index, state_dim):\n",
    "    \"\"\"Calculates variance of state values for a specific dimension across nodes.\"\"\"\n",
    "    # Get state values for the specified dimension using helper function\n",
    "    state_values = get_state_dimension_values(state_dict, node_list_local, dim_index, state_dim)\n",
    "\n",
    "    # Filter out NaN values\n",
    "    valid_values = state_values[~np.isnan(state_values)]\n",
    "\n",
    "    # Calculate variance if enough valid values exist\n",
    "    variance_value = np.nan # Default to NaN\n",
    "    have_enough_valid_values = valid_values.size >= 2 # Need at least 2 points for variance\n",
    "    if have_enough_valid_values:\n",
    "         try:\n",
    "             variance_value = np.var(valid_values) # Calculate variance\n",
    "         except Exception as e_var:\n",
    "             warnings.warn(f\"Variance calculation failed for dim {dim_index}: {e_var}\", RuntimeWarning)\n",
    "             variance_value = np.nan # Set to NaN on error\n",
    "\n",
    "    return variance_value\n",
    "\n",
    "\n",
    "def compute_shannon_entropy_dim_N(state_dict, node_list_local, dim_index, state_dim, num_bins=10, state_range=(-1.0, 1.0)):\n",
    "    \"\"\"Calculates Shannon entropy for a specific dimension using histogram binning.\"\"\"\n",
    "    # Get state values for the specified dimension\n",
    "    state_values = get_state_dimension_values(state_dict, node_list_local, dim_index, state_dim)\n",
    "\n",
    "    # Filter out NaN values\n",
    "    valid_values = state_values[~np.isnan(state_values)]\n",
    "\n",
    "    # Proceed if there are valid values\n",
    "    shannon_entropy_value = np.nan # Default to NaN\n",
    "    have_valid_values = valid_values.size > 0\n",
    "    if have_valid_values:\n",
    "        try:\n",
    "             # Use numpy histogram to get counts in bins\n",
    "             counts, bin_edges = np.histogram(valid_values, bins=num_bins, range=state_range)\n",
    "             # Calculate total number of valid counts\n",
    "             total_counts = counts.sum()\n",
    "\n",
    "             # Calculate entropy only if there are counts\n",
    "             if total_counts > 0:\n",
    "                 # Calculate probabilities by dividing counts by total counts\n",
    "                 probabilities = counts / total_counts\n",
    "                 # Filter out probabilities equal to zero (log(0) is undefined)\n",
    "                 non_zero_probabilities = probabilities[probabilities > 0]\n",
    "\n",
    "                 # Calculate Shannon entropy using scipy if there are non-zero probabilities\n",
    "                 if non_zero_probabilities.size > 0:\n",
    "                     shannon_entropy_value = scipy_entropy(non_zero_probabilities, base=None) # Use natural log\n",
    "                 else:\n",
    "                     # If all probabilities are zero (can happen with weird binning/data)\n",
    "                     shannon_entropy_value = 0.0\n",
    "             else:\n",
    "                 # If no counts in any bin (e.g., all data outside range)\n",
    "                 shannon_entropy_value = 0.0\n",
    "\n",
    "        except Exception as e:\n",
    "             warnings.warn(f\"Shannon entropy calculation failed for dim {dim_index}: {e}\", RuntimeWarning)\n",
    "             shannon_entropy_value = np.nan # Set to NaN on error\n",
    "    else:\n",
    "        # If no valid (non-NaN) values were found for the dimension\n",
    "        shannon_entropy_value = 0.0 # Or np.nan depending on desired behavior for empty input\n",
    "\n",
    "    return shannon_entropy_value\n",
    "\n",
    "# --- Functions below are less relevant with the new worker but kept for context ---\n",
    "\n",
    "def count_attractors_5d(final_states_dict_list, node_list_local, state_dim, tolerance=1e-3):\n",
    "    \"\"\"Counts unique final states (attractors) across multiple simulation trials.\"\"\"\n",
    "    # Input validation\n",
    "    list_is_valid = isinstance(final_states_dict_list, list) and len(final_states_dict_list) > 0\n",
    "    node_list_is_valid = isinstance(node_list_local, list) and len(node_list_local) > 0\n",
    "    if not list_is_valid or not node_list_is_valid:\n",
    "        warnings.warn(\"count_attractors: Invalid input list or node list.\", RuntimeWarning)\n",
    "        return 0 # Return 0 attractors for invalid input\n",
    "\n",
    "    num_trials = len(final_states_dict_list)\n",
    "    num_nodes = len(node_list_local)\n",
    "    # Create a 3D numpy array to store all final states [trial, node, dim]\n",
    "    final_states_array_3d = np.full((num_trials, num_nodes, state_dim), np.nan, dtype=float)\n",
    "\n",
    "    # Convert each state dictionary in the list to an array and store it\n",
    "    trial_idx = 0\n",
    "    while trial_idx < num_trials:\n",
    "        state_dict = final_states_dict_list[trial_idx]\n",
    "        if isinstance(state_dict, dict):\n",
    "            # Use helper function to convert dict to array\n",
    "            state_array = state_dict_to_array(state_dict, node_list_local, state_dim)\n",
    "            final_states_array_3d[trial_idx, :, :] = state_array\n",
    "        # else: keep NaNs if input wasn't a dict\n",
    "        trial_idx = trial_idx + 1\n",
    "\n",
    "    # --- Identify and Filter Valid Trials ---\n",
    "    # A trial is valid if not all its state values are NaN\n",
    "    valid_trials_mask = ~np.isnan(final_states_array_3d).all(axis=(1, 2))\n",
    "    num_valid_trials = np.sum(valid_trials_mask)\n",
    "\n",
    "    if num_valid_trials == 0:\n",
    "        return 0 # No valid final states found\n",
    "\n",
    "    # Select only the valid trials\n",
    "    final_states_array_valid = final_states_array_3d[valid_trials_mask, :, :]\n",
    "\n",
    "    # --- Process Valid States for Uniqueness ---\n",
    "    # Reshape the valid states into a 2D array [valid_trial, flattened_state_vector]\n",
    "    # Each row represents the complete state of the network for one trial\n",
    "    num_features = num_nodes * state_dim\n",
    "    final_states_reshaped = final_states_array_valid.reshape(num_valid_trials, num_features)\n",
    "\n",
    "    # Round the states to the specified tolerance to group similar attractors\n",
    "    # Determine number of decimal places based on tolerance\n",
    "    rounding_decimals = 3 # Default\n",
    "    if tolerance > 0:\n",
    "        # Calculate decimals needed, handling potential log10(0)\n",
    "        try:\n",
    "             rounding_decimals = int(-np.log10(tolerance))\n",
    "             if rounding_decimals < 0 : rounding_decimals = 0 # Ensure non-negative\n",
    "        except ValueError:\n",
    "             rounding_decimals = 15 # Use high precision if tolerance is extremely small\n",
    "    # Apply rounding\n",
    "    rounded_states = np.round(final_states_reshaped, decimals=rounding_decimals)\n",
    "\n",
    "    # --- Count Unique Attractors ---\n",
    "    num_attractors = -1 # Default error value\n",
    "    try:\n",
    "        # Use np.unique along axis=0 to find unique rows (unique flattened states)\n",
    "        unique_attractor_rows = np.unique(rounded_states, axis=0)\n",
    "        # The number of unique rows is the number of distinct attractors found\n",
    "        num_attractors = unique_attractor_rows.shape[0]\n",
    "    except MemoryError:\n",
    "        warnings.warn(\"MemoryError during attractor counting (np.unique). Large state space.\", RuntimeWarning)\n",
    "        num_attractors = -1 # Indicate memory error\n",
    "    except Exception as e_uniq:\n",
    "        warnings.warn(f\"Error during np.unique for attractor counting: {e_uniq}.\", RuntimeWarning)\n",
    "        num_attractors = -1 # Indicate other error\n",
    "\n",
    "    return num_attractors\n",
    "\n",
    "\n",
    "def convergence_time_metric_5d(state_history_dict_list, node_list_local, state_dim, tolerance=1e-3):\n",
    "    \"\"\"Calculates convergence time based on average state change between steps.\"\"\"\n",
    "    # Input validation\n",
    "    history_is_valid = isinstance(state_history_dict_list, list)\n",
    "    history_is_long_enough = history_is_valid and len(state_history_dict_list) >= 2\n",
    "    node_list_is_valid = isinstance(node_list_local, list) and len(node_list_local) > 0\n",
    "\n",
    "    if not history_is_long_enough or not node_list_is_valid:\n",
    "        return np.nan # Cannot determine convergence without sufficient history or nodes\n",
    "\n",
    "    history_length = len(state_history_dict_list)\n",
    "    convergence_step = -1 # Initialize: -1 means not converged yet\n",
    "    previous_state_array = None # Store the state from the previous step\n",
    "\n",
    "    # Iterate through the history starting from the first step (index 0)\n",
    "    step_index = 0\n",
    "    while step_index < history_length:\n",
    "        current_state_dict = state_history_dict_list[step_index]\n",
    "        # Convert current state dict to array\n",
    "        is_valid_dict = isinstance(current_state_dict, dict)\n",
    "        if not is_valid_dict:\n",
    "            warnings.warn(f\"Non-dict state found at step {step_index}. Cannot calculate convergence.\", RuntimeWarning)\n",
    "            return np.nan # Cannot proceed with invalid state format\n",
    "\n",
    "        current_state_array = state_dict_to_array(current_state_dict, node_list_local, state_dim)\n",
    "\n",
    "        # Check if the conversion was successful (no all-NaN array)\n",
    "        current_state_is_valid = not np.isnan(current_state_array).all()\n",
    "\n",
    "        # --- Compare with previous step (if available and valid) ---\n",
    "        is_after_first_step = step_index > 0\n",
    "        previous_state_is_available = previous_state_array is not None\n",
    "\n",
    "        if is_after_first_step and previous_state_is_available and current_state_is_valid:\n",
    "            # Calculate absolute difference between current and previous state arrays\n",
    "            abs_difference = np.abs(current_state_array - previous_state_array)\n",
    "\n",
    "            # --- Calculate Mean Absolute Change ---\n",
    "            # Consider only elements where both current and previous states are valid (not NaN)\n",
    "            valid_mask = ~np.isnan(current_state_array) & ~np.isnan(previous_state_array)\n",
    "            can_compare = np.any(valid_mask) # Check if there's anything to compare\n",
    "\n",
    "            mean_absolute_change = 0.0 # Default if no comparison possible\n",
    "            if can_compare:\n",
    "                # Calculate mean change only over valid elements\n",
    "                mean_absolute_change = np.mean(abs_difference[valid_mask])\n",
    "\n",
    "            # Check if change is below the tolerance threshold\n",
    "            change_below_threshold = mean_absolute_change < tolerance\n",
    "            if change_below_threshold:\n",
    "                convergence_step = step_index # Record the step *at which convergence was detected*\n",
    "                break # Exit the loop once convergence is detected\n",
    "\n",
    "        # Update previous state for the next iteration, only if current state was valid\n",
    "        if current_state_is_valid:\n",
    "             previous_state_array = current_state_array\n",
    "        else:\n",
    "             # If current state is invalid, we can't use it as previous for next step\n",
    "             previous_state_array = None\n",
    "             warnings.warn(f\"Invalid state array encountered at step {step_index}. Resetting comparison.\", RuntimeWarning)\n",
    "\n",
    "\n",
    "        step_index = step_index + 1 # Increment loop counter\n",
    "\n",
    "    # --- Determine final return value ---\n",
    "    convergence_detected = convergence_step != -1\n",
    "    result_convergence_time = np.nan # Default if never converged\n",
    "\n",
    "    if convergence_detected:\n",
    "        result_convergence_time = convergence_step # Return the step index\n",
    "    else:\n",
    "        # If loop finished without convergence, return total steps run (or NaN)\n",
    "        # Using history_length - 1 might be confusing, NaN indicates never converged within history\n",
    "        result_convergence_time = np.nan\n",
    "\n",
    "    return result_convergence_time\n",
    "\n",
    "\n",
    "# Primary function called by worker - calculates metrics AND returns flattened state\n",
    "# MODIFIED to use numpy array input directly, reflecting run_single_instance_phase2 output\n",
    "def calculate_metrics_and_state_from_array(final_state_array, config_local):\n",
    "    \"\"\"Calculates order parameters from a final state NumPy array.\"\"\"\n",
    "    results = {}\n",
    "    if final_state_array is None or not isinstance(final_state_array, np.ndarray) or final_state_array.ndim != 2:\n",
    "         warnings.warn(\"calculate_metrics_and_state_from_array: Invalid final_state_array input.\")\n",
    "         results['variance_norm'] = np.nan\n",
    "         results[f'variance_dim_0'] = np.nan # Assuming dim 0 is default analysis dim\n",
    "         results[f'entropy_dim_0'] = np.nan\n",
    "         results['final_state_flat'] = None\n",
    "         return results\n",
    "\n",
    "    # Get params safely\n",
    "    state_dim = config_local.get('STATE_DIM', 5)\n",
    "    analysis_dim = config_local.get(\"ANALYSIS_STATE_DIM\", 0) # Which dim to analyze specifically\n",
    "    # Ensure analysis_dim is valid\n",
    "    if not (0 <= analysis_dim < state_dim):\n",
    "         analysis_dim = 0 # Default to 0 if invalid\n",
    "         warnings.warn(f\"Invalid ANALYSIS_STATE_DIM in config. Defaulting to 0.\", RuntimeWarning)\n",
    "\n",
    "    bins = config_local.get(\"ORDER_PARAM_BINS\", 10)\n",
    "    s_range = tuple(config_local.get(\"STATE_RANGE\", (-1.5, 1.5))) # Use wider range consistent with clipping\n",
    "\n",
    "    # Calculate metrics directly from the array\n",
    "    # Variance Norm (Average variance across dimensions)\n",
    "    results['variance_norm'] = calculate_variance_norm(final_state_array)\n",
    "\n",
    "    # Variance of a specific dimension\n",
    "    variance_dim_key = f'variance_dim_{analysis_dim}'\n",
    "    results[variance_dim_key] = np.nan # Default\n",
    "    if state_dim > analysis_dim:\n",
    "        try: results[variance_dim_key] = np.var(final_state_array[:, analysis_dim])\n",
    "        except Exception: pass # Keep NaN on error\n",
    "\n",
    "    # Entropy of a specific dimension\n",
    "    entropy_dim_key = f'entropy_dim_{analysis_dim}'\n",
    "    results[entropy_dim_key] = np.nan # Default\n",
    "    if state_dim > analysis_dim:\n",
    "        results[entropy_dim_key] = calculate_entropy_binned(final_state_array[:, analysis_dim], bins=bins, range_lims=s_range)\n",
    "\n",
    "    # Get flattened state for PCA/UMAP (handle potential errors)\n",
    "    final_state_flat_list = None\n",
    "    try:\n",
    "        # Check if array contains NaNs/Infs before flattening\n",
    "        array_is_valid = not (np.isnan(final_state_array).any() or np.isinf(final_state_array).any())\n",
    "        if array_is_valid:\n",
    "            # Flatten the 2D array [N, D] into a 1D vector\n",
    "            final_state_flat_list = final_state_array.flatten().tolist() # Convert to list for JSON possibly\n",
    "        else:\n",
    "            # Set to None if the array contains invalid numbers\n",
    "            final_state_flat_list = None\n",
    "            warnings.warn(\"Final state array contains NaN/Inf, cannot flatten.\", RuntimeWarning)\n",
    "    except Exception as e_flat:\n",
    "        warnings.warn(f\"Could not flatten state: {e_flat}\")\n",
    "        final_state_flat_list = None # Indicate failure\n",
    "\n",
    "    results['final_state_flat'] = final_state_flat_list\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Cell 4: Order parameter functions defined (some assume older dict format, added array-based calculator).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9bdc16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 5: Rule Definition (Conceptual CPU 5D HDC / RSV Update Step) ---\n",
      "‚ö†Ô∏è Note: This CPU implementation is for conceptual understanding/testing.\n",
      "   Actual sweeps use the GPU version (`hdc_5d_step_vectorized_torch`).\n",
      "‚úÖ Cell 5: Conceptual 5D HDC / RSV CPU simulation step function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Define Graph Automaton Update Rule (5D HDC / RSV) - Emergenics\n",
    "# Description: Implements the 5D HDC / RSV update rule function `simulation_step_5D_HDC_RSV`.\n",
    "#              This is the conceptual CPU-based implementation. The actual sweeps use\n",
    "#              the GPU version `hdc_5d_step_vectorized_torch` from Cell 2/worker_utils.py.\n",
    "# Adheres strictly to one statement per line after colons.\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import traceback\n",
    "\n",
    "print(\"\\n--- Cell 5: Rule Definition (Conceptual CPU 5D HDC / RSV Update Step) ---\")\n",
    "print(\"‚ö†Ô∏è Note: This CPU implementation is for conceptual understanding/testing.\")\n",
    "print(\"   Actual sweeps use the GPU version (`hdc_5d_step_vectorized_torch`).\")\n",
    "\n",
    "\n",
    "# Helper function for element-wise clipping (CPU version)\n",
    "def clip_vector_cpu(vec, clip_range):\n",
    "    \"\"\"Clips a NumPy vector element-wise.\"\"\"\n",
    "    # Check if input is a numpy array\n",
    "    if not isinstance(vec, np.ndarray):\n",
    "        return vec # Return input as is if not an array\n",
    "    # Check clip range format\n",
    "    if not isinstance(clip_range, (list, tuple)) or len(clip_range) != 2:\n",
    "        warnings.warn(\"Invalid clip_range format. Clipping skipped.\", RuntimeWarning)\n",
    "        return vec\n",
    "    min_val = clip_range[0]\n",
    "    max_val = clip_range[1]\n",
    "    if min_val >= max_val:\n",
    "         warnings.warn(\"Invalid clip_range: min >= max. Clipping skipped.\", RuntimeWarning)\n",
    "         return vec\n",
    "    # Apply clipping using numpy.clip\n",
    "    clipped_vec = np.clip(vec, min_val, max_val)\n",
    "    return clipped_vec\n",
    "\n",
    "# Main 5D HDC / RSV Simulation Step Function (CPU Conceptual Implementation)\n",
    "def simulation_step_5D_HDC_RSV(\n",
    "    graph, current_states_dict,\n",
    "    node_list_local, node_to_int_local, # Mappings assumed provided if using dicts\n",
    "    rule_params_local):\n",
    "    \"\"\"\n",
    "    Performs one step of the 5D HDC/RSV simulation using CPU and state dictionaries.\n",
    "    This is a conceptual implementation; GPU version is used for performance.\n",
    "    \"\"\"\n",
    "    # --- Input Validation & Setup ---\n",
    "    if not isinstance(graph, nx.Graph):\n",
    "         warnings.warn(\"simulation_step: graph is not a NetworkX graph.\", RuntimeWarning)\n",
    "         return None, None, -1.0 # Indicate error\n",
    "    if not isinstance(current_states_dict, dict):\n",
    "         warnings.warn(\"simulation_step: current_states_dict is not a dictionary.\", RuntimeWarning)\n",
    "         return None, None, -1.0\n",
    "    if not isinstance(node_list_local, list) or not node_list_local:\n",
    "          warnings.warn(\"simulation_step: node_list_local is invalid.\", RuntimeWarning)\n",
    "          return None, None, -1.0\n",
    "    if not isinstance(node_to_int_local, dict):\n",
    "           warnings.warn(\"simulation_step: node_to_int_local is invalid.\", RuntimeWarning)\n",
    "           return None, None, -1.0\n",
    "    if not isinstance(rule_params_local, dict):\n",
    "         warnings.warn(\"simulation_step: rule_params_local is not a dictionary.\", RuntimeWarning)\n",
    "         return None, None, -1.0\n",
    "\n",
    "    num_nodes = len(node_list_local)\n",
    "    state_dim = 5 # Hardcoded for this specific model\n",
    "\n",
    "    # Handle empty graph case\n",
    "    if num_nodes == 0:\n",
    "        return {}, None, 0.0 # Return empty dict, None pheromones, 0 change\n",
    "\n",
    "    # --- Get Rule Parameters ---\n",
    "    # Use .get() for safe access with defaults\n",
    "    alpha = rule_params_local.get('hcd_alpha', 0.1) # Learning rate / step size\n",
    "    clip_range = rule_params_local.get('hcd_clip_range', [-1.0, 1.0])\n",
    "    use_bundling = rule_params_local.get('use_neighbor_bundling', True) # HDC bundling concept\n",
    "    use_weights = rule_params_local.get('use_graph_weights', False) # Currently unused, assumes unweighted\n",
    "    noise_level = rule_params_local.get('noise_level', 0.001)\n",
    "    # Define default state for nodes missing from dict (shouldn't happen if initialized properly)\n",
    "    default_state = np.array([0.0] * state_dim, dtype=float)\n",
    "\n",
    "    # --- Prepare Data Structures ---\n",
    "    # Find the data type of the first valid state vector for consistency\n",
    "    first_valid_state = default_state # Initialize with default\n",
    "    dtype_found = False\n",
    "    iter_node_list = iter(node_list_local)\n",
    "    while not dtype_found:\n",
    "        try:\n",
    "             node_id = next(iter_node_list)\n",
    "             state = current_states_dict.get(node_id)\n",
    "             if state is not None and isinstance(state, np.ndarray) and state.shape == (state_dim,):\n",
    "                  first_valid_state = state\n",
    "                  dtype_found = True\n",
    "        except StopIteration:\n",
    "             break # No valid states found\n",
    "    state_dtype = first_valid_state.dtype # Get dtype (e.g., float64)\n",
    "\n",
    "    # Convert state dictionary to NumPy array for efficient access\n",
    "    # Using a loop for clarity as requested\n",
    "    current_states_array = np.empty((num_nodes, state_dim), dtype=state_dtype)\n",
    "    idx = 0\n",
    "    while idx < num_nodes:\n",
    "         node_id_current = node_list_local[idx]\n",
    "         state_vector = current_states_dict.get(node_id_current, default_state)\n",
    "         # Ensure vector shape matches state_dim, otherwise use default\n",
    "         if isinstance(state_vector, np.ndarray) and state_vector.shape == (state_dim,):\n",
    "              current_states_array[idx, :] = state_vector\n",
    "         else:\n",
    "              current_states_array[idx, :] = default_state # Fallback to default\n",
    "         idx = idx + 1\n",
    "\n",
    "    # Create array for next states, initialized with current states\n",
    "    next_states_array = current_states_array.copy()\n",
    "\n",
    "    # --- Calculate Updates Node by Node ---\n",
    "    avg_change_accumulator = 0.0 # Accumulate total change\n",
    "    nodes_updated_count = 0 # Count nodes processed successfully\n",
    "\n",
    "    # Get adjacency view for efficient neighbor access\n",
    "    adj = graph.adj\n",
    "\n",
    "    # Iterate through each node using the node list\n",
    "    node_idx = 0\n",
    "    while node_idx < num_nodes:\n",
    "        node_id = node_list_local[node_idx]\n",
    "        current_node_state = current_states_array[node_idx, :]\n",
    "\n",
    "        # 1. Bundle Neighbors (HDC concept)\n",
    "        bundled_neighbor_vector = np.zeros(state_dim, dtype=state_dtype)\n",
    "        if use_bundling:\n",
    "            # Get neighbors for the current node\n",
    "            neighbors_dict = adj.get(node_id, {}) # Returns empty dict if node has no neighbors\n",
    "            valid_neighbors = []\n",
    "            neighbor_iter = iter(neighbors_dict.keys())\n",
    "            stop_neighbor_iter = False\n",
    "            while not stop_neighbor_iter:\n",
    "                try:\n",
    "                     neighbor_id = next(neighbor_iter)\n",
    "                     # Check if neighbor is in our known node mapping (important for consistency)\n",
    "                     if neighbor_id in node_to_int_local:\n",
    "                          valid_neighbors.append(neighbor_id)\n",
    "                except StopIteration:\n",
    "                     stop_neighbor_iter = True\n",
    "\n",
    "            # If there are valid neighbors, calculate bundled vector\n",
    "            if len(valid_neighbors) > 0:\n",
    "                 # Get integer indices of neighbors\n",
    "                 neighbor_indices = []\n",
    "                 neighbor_idx = 0\n",
    "                 while neighbor_idx < len(valid_neighbors):\n",
    "                      neighbor_id = valid_neighbors[neighbor_idx]\n",
    "                      int_index = node_to_int_local.get(neighbor_id, -1) # Get index from mapping\n",
    "                      # Ensure index is valid\n",
    "                      if 0 <= int_index < num_nodes:\n",
    "                           neighbor_indices.append(int_index)\n",
    "                      neighbor_idx = neighbor_idx + 1\n",
    "\n",
    "                 # Sum the state vectors of valid neighbors\n",
    "                 if len(neighbor_indices) > 0:\n",
    "                      # Indexing numpy array with list of indices selects rows\n",
    "                      bundled_vector_sum = np.sum(current_states_array[neighbor_indices, :], axis=0)\n",
    "                      # Apply clipping to the bundled sum\n",
    "                      bundled_neighbor_vector = clip_vector_cpu(bundled_vector_sum, clip_range)\n",
    "\n",
    "\n",
    "        # 2. Calculate RSV scalar (deviation norm)\n",
    "        # Deviation vector: difference between node's state and bundled neighbors\n",
    "        deviation_vector = current_node_state - bundled_neighbor_vector\n",
    "        rsv_scalar = 0.0 # Default value\n",
    "        try:\n",
    "            # Calculate L2 norm of the deviation vector\n",
    "            norm_val = np.linalg.norm(deviation_vector)\n",
    "            # Assign norm to scalar if it's a valid finite number\n",
    "            if not (np.isnan(norm_val) or np.isinf(norm_val)):\n",
    "                rsv_scalar = norm_val\n",
    "        except Exception as e_norm_calc:\n",
    "            warnings.warn(f\"RSV scalar norm calculation failed for node {node_id}: {e_norm_calc}\", RuntimeWarning)\n",
    "            # Keep rsv_scalar as 0.0 on error\n",
    "\n",
    "        # 3. Apply Update Rule\n",
    "        # Calculate update term based on deviation and scalar\n",
    "        # Update moves state away from neighbors based on deviation magnitude\n",
    "        update_term = alpha * rsv_scalar * (-deviation_vector)\n",
    "        # Calculate potential next state before noise/clipping\n",
    "        potential_next_state = current_node_state + update_term\n",
    "\n",
    "        # 4. Add Noise\n",
    "        # Generate noise vector with same dimension as state\n",
    "        noise_vector = np.random.uniform(-noise_level, noise_level, size=state_dim).astype(state_dtype)\n",
    "        state_after_noise = potential_next_state + noise_vector\n",
    "\n",
    "        # 5. Apply Clipping\n",
    "        # Ensure final state stays within bounds\n",
    "        final_next_state = clip_vector_cpu(state_after_noise, clip_range)\n",
    "\n",
    "        # Store the calculated next state in the array\n",
    "        next_states_array[node_idx, :] = final_next_state\n",
    "\n",
    "        # --- Accumulate Change Metric ---\n",
    "        try:\n",
    "            # Calculate the magnitude of change for this node\n",
    "            node_change = np.linalg.norm(final_next_state - current_node_state)\n",
    "            # Add to accumulator if valid\n",
    "            if not (np.isnan(node_change) or np.isinf(node_change)):\n",
    "                avg_change_accumulator = avg_change_accumulator + node_change\n",
    "                nodes_updated_count = nodes_updated_count + 1\n",
    "        except Exception as e_change_calc:\n",
    "             warnings.warn(f\"Node change calculation failed for node {node_id}: {e_change_calc}\", RuntimeWarning)\n",
    "\n",
    "        # Increment node index for the main loop\n",
    "        node_idx = node_idx + 1\n",
    "    # --- End Node Loop ---\n",
    "\n",
    "    # Calculate Average Change across all updated nodes\n",
    "    average_change = 0.0\n",
    "    if nodes_updated_count > 0:\n",
    "        average_change = avg_change_accumulator / nodes_updated_count\n",
    "\n",
    "    # Convert the NumPy array back to a state dictionary\n",
    "    next_states_dict = {}\n",
    "    idx = 0\n",
    "    while idx < num_nodes:\n",
    "        node_id_final = node_list_local[idx]\n",
    "        next_states_dict[node_id_final] = next_states_array[idx, :]\n",
    "        idx = idx + 1\n",
    "\n",
    "    # Return the new state dictionary, None for pheromones (not used here), and average change\n",
    "    return next_states_dict, None, average_change\n",
    "\n",
    "    # Error Handling (Catch unexpected errors during the process)\n",
    "    # Note: Specific errors handled within loops, this is a general fallback\n",
    "    # except Exception as e:\n",
    "    #      print(f\"‚ùå‚ùå‚ùå Error in simulation_step_5D_HDC_RSV: {e}\")\n",
    "    #      traceback.print_exc()\n",
    "    #      return None, None, -1.0 # Indicate error state\n",
    "\n",
    "\n",
    "print(\"‚úÖ Cell 5: Conceptual 5D HDC / RSV CPU simulation step function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0dab8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 6: Simulation Runner Definition (Emergenics - Resumable, CPU Dict-Based) ---\n",
      "‚ö†Ô∏è Note: This runner uses the CPU implementation from Cell 5.\n",
      "‚úÖ Cell 6: CPU 5D HDC State Initializer and Simulation Runner defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Simulation Runner Function (Emergenics - Resumable)\n",
    "# Description: Defines the simulation runner using the 5D HDC/RSV step function (Cell 5).\n",
    "# Handles state dictionaries, manages checkpointing/resuming. Reduced verbosity.\n",
    "# Adheres strictly to one statement per line after colons.\n",
    "# Note: This runner uses the CPU implementation and state dictionaries.\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "print(\"\\n--- Cell 6: Simulation Runner Definition (Emergenics - Resumable, CPU Dict-Based) ---\")\n",
    "print(\"‚ö†Ô∏è Note: This runner uses the CPU implementation from Cell 5.\")\n",
    "\n",
    "# --- State Initialization Function (5D HDC) ---\n",
    "# Needs `clip_vector_cpu` defined (should be available from Cell 5)\n",
    "def initialize_states_5D_HDC(node_list_local, config_local):\n",
    "    \"\"\"Initializes 5D HDC states based on config_local settings using CPU.\"\"\"\n",
    "    # Validate required config keys\n",
    "    if 'INIT_MODE' not in config_local:\n",
    "        raise ValueError(\"Missing INIT_MODE in config_local for initialization.\")\n",
    "    if 'STATE_DIM' not in config_local:\n",
    "        raise ValueError(\"Missing STATE_DIM in config_local for initialization.\")\n",
    "\n",
    "    init_mode = config_local['INIT_MODE']\n",
    "    state_dim = config_local['STATE_DIM']\n",
    "\n",
    "    # Get default state, ensure it's a numpy array\n",
    "    default_state_cfg = config_local.get('DEFAULT_INACTIVE_STATE', [0.0]*state_dim)\n",
    "    try:\n",
    "        default_state = np.array(default_state_cfg, dtype=float)\n",
    "        if default_state.shape != (state_dim,): # Check shape\n",
    "             raise ValueError(f\"DEFAULT_INACTIVE_STATE shape mismatch: expected ({state_dim},), got {default_state.shape}\")\n",
    "    except Exception as e_def_state:\n",
    "        raise ValueError(f\"Invalid DEFAULT_INACTIVE_STATE in config: {e_def_state}\")\n",
    "\n",
    "    # Get parameters for 'random_normal' mode\n",
    "    mean = config_local.get('INIT_NORMAL_MEAN', 0.0)\n",
    "    stddev = config_local.get('INIT_NORMAL_STDDEV', 0.1)\n",
    "    # Get clipping range from rule parameters if available\n",
    "    rule_params_init = config_local.get('rule_params', {})\n",
    "    clip_range = rule_params_init.get('hcd_clip_range', [-1.0, 1.0]) # Default clip range\n",
    "\n",
    "    # Validate node list\n",
    "    if not isinstance(node_list_local, list) or len(node_list_local) == 0:\n",
    "         warnings.warn(\"initialize_states received empty or invalid node list.\", RuntimeWarning)\n",
    "         return {} # Return empty dictionary\n",
    "\n",
    "    num_nodes = len(node_list_local)\n",
    "    states = {} # Initialize empty dictionary to store states\n",
    "\n",
    "    # --- Initialization Logic ---\n",
    "    node_idx = 0\n",
    "    while node_idx < num_nodes: # Iterate through nodes\n",
    "        node_id = node_list_local[node_idx]\n",
    "        state_vector = None # Initialize vector for this node\n",
    "\n",
    "        if init_mode == 'random_normal':\n",
    "            # Generate random state from normal distribution\n",
    "            random_state = np.random.normal(loc=mean, scale=stddev, size=state_dim).astype(default_state.dtype)\n",
    "            # Clip the generated random state\n",
    "            state_vector = clip_vector_cpu(random_state, clip_range) # Use CPU clipping helper\n",
    "        elif init_mode == 'zeros':\n",
    "            # Use the default state (which should be zeros or configured value)\n",
    "            state_vector = default_state.copy() # Use copy to avoid modifying the default\n",
    "        else:\n",
    "            # Handle unknown initialization mode\n",
    "            warnings.warn(f\"Unknown INIT_MODE '{init_mode}'. Using default state.\", RuntimeWarning)\n",
    "            state_vector = default_state.copy()\n",
    "\n",
    "        # Assign the generated state vector to the node ID in the dictionary\n",
    "        states[node_id] = state_vector\n",
    "        node_idx = node_idx + 1 # Increment loop counter\n",
    "\n",
    "    return states\n",
    "\n",
    "# --- Main Simulation Runner (CPU Dict-Based) ---\n",
    "def run_simulation_5D_HDC_RSV(\n",
    "    graph_obj, initial_states_dict, config_local,\n",
    "    max_steps=None, convergence_thresh=None,\n",
    "    node_list_local=None, node_to_int_local=None, # Needed for CPU step func\n",
    "    output_dir=None, checkpoint_interval=50,\n",
    "    checkpoint_filename=\"sim_checkpoint.pkl\",\n",
    "    progress_desc=\"Simulating 5D (CPU)\", leave_progress=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Runs CA simulation with 5D HDC/RSV rule (CPU version from Cell 5),\n",
    "    using state dictionaries, with checkpointing support.\n",
    "    \"\"\"\n",
    "    # --- Prerequisite Checks ---\n",
    "    args_valid = True\n",
    "    missing_or_invalid = []\n",
    "    # Check graph object\n",
    "    if graph_obj is None or not isinstance(graph_obj, nx.Graph):\n",
    "        args_valid = False; missing_or_invalid.append(\"graph_obj\")\n",
    "    # Check initial states dictionary\n",
    "    if initial_states_dict is None or not isinstance(initial_states_dict, dict):\n",
    "        args_valid = False; missing_or_invalid.append(\"initial_states_dict\")\n",
    "    # Check configuration dictionary\n",
    "    if config_local is None or 'rule_params' not in config_local:\n",
    "        args_valid = False; missing_or_invalid.append(\"config_local (with rule_params)\")\n",
    "    # Check simulation control parameters\n",
    "    if max_steps is None or not isinstance(max_steps, int) or max_steps <= 0:\n",
    "        args_valid = False; missing_or_invalid.append(\"max_steps (positive integer)\")\n",
    "    if convergence_thresh is None or not isinstance(convergence_thresh, (float, int)) or convergence_thresh < 0:\n",
    "        args_valid = False; missing_or_invalid.append(\"convergence_thresh (non-negative number)\")\n",
    "    # Check node list and mapping needed by the CPU step function\n",
    "    if node_list_local is None or not isinstance(node_list_local, list) or not node_list_local:\n",
    "        args_valid = False; missing_or_invalid.append(\"node_list_local (non-empty list)\")\n",
    "    if node_to_int_local is None or not isinstance(node_to_int_local, dict):\n",
    "        args_valid = False; missing_or_invalid.append(\"node_to_int_local (dict)\")\n",
    "    # Check checkpointing parameters if enabled\n",
    "    checkpointing_enabled = False # Default to disabled\n",
    "    if output_dir is not None and isinstance(checkpoint_interval, int) and checkpoint_interval > 0 and checkpoint_interval <= max_steps:\n",
    "         if isinstance(output_dir, str) and isinstance(checkpoint_filename, str):\n",
    "              checkpointing_enabled = True\n",
    "         else:\n",
    "              args_valid = False; missing_or_invalid.append(\"checkpoint output_dir/filename (strings)\")\n",
    "    elif output_dir is not None:\n",
    "         # Checkpoint interval invalid or disabled\n",
    "         warnings.warn(\"Checkpointing specified (output_dir provided) but interval is invalid or zero. Checkpointing disabled.\", RuntimeWarning)\n",
    "\n",
    "\n",
    "    # Raise error if any argument is invalid\n",
    "    if not args_valid:\n",
    "        raise ValueError(f\"‚ùå Invalid/Missing arguments for simulation runner: {missing_or_invalid}\")\n",
    "\n",
    "    # --- Checkpoint Handling ---\n",
    "    checkpoint_path = None\n",
    "    if checkpointing_enabled:\n",
    "        # Ensure output directory exists for checkpointing\n",
    "        try:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            checkpoint_path = os.path.join(output_dir, checkpoint_filename)\n",
    "        except OSError as e_dir:\n",
    "            warnings.warn(f\"Could not create output directory '{output_dir}' for checkpointing. Checkpointing disabled. Error: {e_dir}\", RuntimeWarning)\n",
    "            checkpointing_enabled = False # Disable if dir creation fails\n",
    "\n",
    "\n",
    "    start_step = 0 # Step to start simulation from (0 for fresh start)\n",
    "    current_states = {} # Holds the current state dictionary\n",
    "    state_history = [] # List to store state dictionaries at each step (can consume memory)\n",
    "    avg_change_history_runner = [] # Store avg change from simulation step\n",
    "\n",
    "    # Try loading from checkpoint if path exists\n",
    "    checkpoint_load_successful = False\n",
    "    if checkpointing_enabled and checkpoint_path is not None and os.path.exists(checkpoint_path):\n",
    "        print(f\"  Attempting to load checkpoint from: {checkpoint_path}\")\n",
    "        try:\n",
    "            with open(checkpoint_path, 'rb') as f:\n",
    "                checkpoint_data = pickle.load(f)\n",
    "            # Validate checkpoint data structure\n",
    "            if isinstance(checkpoint_data, dict) and 'last_saved_step' in checkpoint_data and 'current_states_dict' in checkpoint_data:\n",
    "                 start_step = checkpoint_data.get('last_saved_step', -1) + 1 # Start from step AFTER the saved one\n",
    "                 saved_states = checkpoint_data.get('current_states_dict', {})\n",
    "                 # Ensure loaded states are numpy arrays (pickle might handle this, but check)\n",
    "                 current_states = {}\n",
    "                 for node_id, state_vec in saved_states.items():\n",
    "                     if isinstance(state_vec, np.ndarray):\n",
    "                         current_states[node_id] = state_vec\n",
    "                     else:\n",
    "                         # Attempt conversion if not already array, fallback to default\n",
    "                         try: current_states[node_id] = np.array(state_vec, dtype=float)\n",
    "                         except Exception: current_states[node_id] = np.full(config_local['STATE_DIM'], np.nan, dtype=float)\n",
    "\n",
    "                 # Add loaded state to history start\n",
    "                 state_history = [copy.deepcopy(current_states)]\n",
    "                 # Load previous avg change if available (optional)\n",
    "                 last_avg_change_chkpt = checkpoint_data.get('last_avg_change', np.nan)\n",
    "                 avg_change_history_runner = [last_avg_change_chkpt] # Start history with last known change\n",
    "\n",
    "                 simulation_already_completed = start_step >= max_steps\n",
    "                 if simulation_already_completed:\n",
    "                     print(f\"  Checkpoint indicates simulation already completed at step {start_step-1}. Returning.\")\n",
    "                     # Need to decide what state_history to return - potentially load full history if saved?\n",
    "                     # For now, returning just the final state loaded.\n",
    "                     termination_reason_chkpt = checkpoint_data.get('termination_reason', 'completed_via_checkpoint')\n",
    "                     return [copy.deepcopy(current_states)], termination_reason_chkpt # Return list with one state dict\n",
    "                 else:\n",
    "                     print(f\"  Resuming simulation from step {start_step}.\")\n",
    "                     checkpoint_load_successful = True # Mark successful load\n",
    "            else:\n",
    "                 warnings.warn(\"Checkpoint file invalid format. Starting fresh.\", RuntimeWarning)\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Checkpoint load failed: {e}. Starting fresh.\", RuntimeWarning)\n",
    "            # Reset variables if load fails\n",
    "            start_step = 0\n",
    "            current_states = {}\n",
    "            state_history = []\n",
    "            avg_change_history_runner = []\n",
    "\n",
    "\n",
    "    # --- Initialize if not resuming successfully ---\n",
    "    if not checkpoint_load_successful:\n",
    "        # Perform deep copy of initial states to avoid modifying the original dict\n",
    "        current_states = copy.deepcopy(initial_states_dict)\n",
    "        # Start history with the initial state\n",
    "        state_history = [copy.deepcopy(current_states)]\n",
    "        start_step = 0 # Ensure starting from step 0\n",
    "\n",
    "\n",
    "    # --- Simulation Loop ---\n",
    "    termination_reason = \"unknown\" # Initialize termination reason\n",
    "    start_sim_time = time.time()\n",
    "    last_avg_change = np.nan # Store the average change from the last step\n",
    "\n",
    "    # Get rule parameters once before the loop for efficiency\n",
    "    simulation_rule_parameters = config_local['rule_params']\n",
    "\n",
    "    # Setup progress bar if leave_progress is True\n",
    "    step_iterator = range(start_step, max_steps)\n",
    "    use_tqdm = leave_progress # Control tqdm display\n",
    "    if use_tqdm:\n",
    "         pbar = tqdm(step_iterator, desc=progress_desc, leave=leave_progress, initial=start_step, total=max_steps)\n",
    "    else:\n",
    "         pbar = step_iterator # Use plain range if no progress bar\n",
    "\n",
    "\n",
    "    # Main simulation loop using 'for' loop over range iterator\n",
    "    step = start_step # Initialize step counter for logic inside loop\n",
    "    for current_step_index in pbar: # current_step_index goes from start_step to max_steps-1\n",
    "        step = current_step_index # Use 'step' for consistency with internal logic\n",
    "\n",
    "        # --- Execute one simulation step using the CPU function ---\n",
    "        next_states, _, avg_change = simulation_step_5D_HDC_RSV(\n",
    "            graph_obj, current_states,\n",
    "            node_list_local, node_to_int_local, # Pass mappings\n",
    "            simulation_rule_parameters\n",
    "        )\n",
    "\n",
    "        # --- Handle Simulation Step Failure ---\n",
    "        simulation_step_failed = next_states is None or avg_change < 0 # avg_change < 0 indicates error in step func\n",
    "        if simulation_step_failed:\n",
    "            print(f\"\\n‚ùå Error occurred during simulation step {step+1}. Halting.\")\n",
    "            termination_reason = f\"error_at_step_{step+1}\"\n",
    "            if use_tqdm: pbar.close() # Close progress bar on error\n",
    "            # Return history up to the point of failure\n",
    "            return state_history, termination_reason\n",
    "\n",
    "        # --- Store Results of Successful Step ---\n",
    "        # Append the newly calculated state dictionary to the history (use deepcopy)\n",
    "        state_history.append(copy.deepcopy(next_states))\n",
    "        # Update the current state for the next iteration\n",
    "        current_states = next_states\n",
    "        # Store the average change from this step\n",
    "        last_avg_change = avg_change\n",
    "        avg_change_history_runner.append(last_avg_change)\n",
    "\n",
    "        # Update progress bar description with average change\n",
    "        if use_tqdm:\n",
    "             pbar.set_postfix({'AvgChange': f\"{avg_change:.6f}\"})\n",
    "\n",
    "        # --- Check for Convergence ---\n",
    "        # Check if average change is below the threshold\n",
    "        converged = False\n",
    "        if last_avg_change < convergence_thresh:\n",
    "             converged = True\n",
    "             termination_reason = f\"convergence_at_step_{step+1}\"\n",
    "             if use_tqdm: pbar.close() # Close progress bar on convergence\n",
    "             break # Exit the simulation loop\n",
    "\n",
    "        # --- Save Checkpoint Periodically ---\n",
    "        is_last_iteration = step == max_steps - 1 # Check if it's the final iteration\n",
    "        is_checkpoint_step = False\n",
    "        if checkpointing_enabled and checkpoint_interval > 0:\n",
    "             # Checkpoint at steps 49, 99, 149 etc. if interval is 50\n",
    "             if (step + 1) % checkpoint_interval == 0:\n",
    "                  is_checkpoint_step = True\n",
    "\n",
    "        # Save checkpoint if it's a checkpoint step AND not the very last iteration\n",
    "        should_save_checkpoint = checkpointing_enabled and is_checkpoint_step and not is_last_iteration\n",
    "        if should_save_checkpoint:\n",
    "            # Prepare data to save in checkpoint file\n",
    "            checkpoint_data_to_save = {\n",
    "                'last_saved_step': step, # Save the index of the step just completed\n",
    "                'current_states_dict': current_states, # Save the state *after* the completed step\n",
    "                'termination_reason': termination_reason, # Store current reason (might be 'unknown')\n",
    "                'last_avg_change': last_avg_change # Store avg change from this step\n",
    "                # Optionally save full state history if needed, but increases file size significantly\n",
    "                # 'full_state_history': state_history\n",
    "            }\n",
    "            # Save checkpoint atomically (write to temp file, then replace)\n",
    "            try:\n",
    "                temp_path = checkpoint_path + \".tmp\" # Temporary file path\n",
    "                with open(temp_path, 'wb') as f_tmp:\n",
    "                    pickle.dump(checkpoint_data_to_save, f_tmp)\n",
    "                # Replace the old checkpoint file with the new temporary file\n",
    "                os.replace(temp_path, checkpoint_path)\n",
    "                # Optional: print confirmation\n",
    "                # if use_tqdm: pbar.set_description(f\"{progress_desc} (Chkpt @{step+1})\")\n",
    "            except Exception as e:\n",
    "                # Print warning if checkpoint saving fails but continue simulation\n",
    "                print(f\"\\n‚ö†Ô∏è Checkpoint saving failed at step {step+1}: {e}\")\n",
    "\n",
    "    # --- After the Loop ---\n",
    "    # This block executes if the loop finishes without breaking (i.e., max steps reached)\n",
    "    # or if break happened (convergence)\n",
    "    else: # This is the 'for...else' construct, runs if loop finished normally\n",
    "        if use_tqdm and not converged: # Ensure progress bar is closed if loop finishes\n",
    "            pbar.close()\n",
    "        # If termination reason wasn't set by convergence, set it to max_steps_reached\n",
    "        if termination_reason == \"unknown\":\n",
    "             termination_reason = \"max_steps_reached\"\n",
    "\n",
    "\n",
    "    end_sim_time = time.time()\n",
    "    total_sim_time = end_sim_time - start_sim_time\n",
    "    if use_tqdm: # Print final time if progress bar was used\n",
    "        print(f\"  Simulation loop finished. Reason: {termination_reason}. Time: {total_sim_time:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- Final Cleanup: Remove Checkpoint File ---\n",
    "    # Remove checkpoint file only if simulation completed successfully (not error)\n",
    "    # and checkpointing was enabled and the file exists\n",
    "    remove_checkpoint = False\n",
    "    if checkpointing_enabled and checkpoint_path is not None:\n",
    "        if not termination_reason.startswith(\"error\"):\n",
    "             if os.path.exists(checkpoint_path):\n",
    "                 remove_checkpoint = True\n",
    "\n",
    "    if remove_checkpoint:\n",
    "        try:\n",
    "            os.remove(checkpoint_path)\n",
    "            print(f\"  Removed final checkpoint file: {checkpoint_path}\")\n",
    "        except OSError as e_rem:\n",
    "            warnings.warn(f\"Could not remove checkpoint file '{checkpoint_path}': {e_rem}\", RuntimeWarning)\n",
    "\n",
    "\n",
    "    # Return the full state history and the final termination reason\n",
    "    return state_history, termination_reason\n",
    "\n",
    "print(\"‚úÖ Cell 6: CPU 5D HDC State Initializer and Simulation Runner defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99899367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 7: Graph Generation Functions ---\n",
      "‚úÖ Cell 7: Graph generation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Graph Generation Functions (Emergenics)\n",
    "# Description: Defines functions to generate networks (WS, SBM, RGG).\n",
    "# Adheres strictly to one statement per line after colons.\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "print(\"\\n--- Cell 7: Graph Generation Functions ---\")\n",
    "\n",
    "def generate_ws_graph(n_nodes, k_neighbors, rewiring_prob, seed=None):\n",
    "    \"\"\"Generates a Watts-Strogatz small-world graph with input validation.\"\"\"\n",
    "    # --- Input Validation ---\n",
    "    # Check n_nodes\n",
    "    if not isinstance(n_nodes, int) or n_nodes <= 0:\n",
    "         raise ValueError(f\"n_nodes must be a positive integer, got {n_nodes}\")\n",
    "    # Check k_neighbors\n",
    "    if not isinstance(k_neighbors, int) or k_neighbors <= 0:\n",
    "         warnings.warn(f\"WS k ({k_neighbors}) must be a positive integer. Setting k=2.\", RuntimeWarning)\n",
    "         k_neighbors = 2 # Default to minimal valid k\n",
    "\n",
    "    # Ensure k is even\n",
    "    if k_neighbors % 2 != 0:\n",
    "        original_k = k_neighbors\n",
    "        # Decrease k by 1 to make it even, ensuring it's at least 2\n",
    "        k_neighbors = max(2, k_neighbors - 1)\n",
    "        warnings.warn(f\"WS k ({original_k}) must be even. Setting k={k_neighbors}.\", RuntimeWarning)\n",
    "\n",
    "    # Ensure k is less than n_nodes\n",
    "    if k_neighbors >= n_nodes:\n",
    "        original_k = k_neighbors\n",
    "        # Set k to the largest possible even value less than n_nodes\n",
    "        # Max k is n-1. If n-1 is odd, use n-2. If n-1 is even, use n-1 (if n>1)\n",
    "        max_possible_k = n_nodes - 1\n",
    "        if max_possible_k % 2 != 0:\n",
    "             corrected_k = max(2, max_possible_k - 1) # Need at least k=2\n",
    "        else:\n",
    "             corrected_k = max(2, max_possible_k)\n",
    "\n",
    "        warnings.warn(f\"WS k ({original_k}) >= n ({n_nodes}). Setting k={corrected_k}.\", RuntimeWarning)\n",
    "        k_neighbors = corrected_k\n",
    "\n",
    "    # Check rewiring probability\n",
    "    if not isinstance(rewiring_prob, (float, int)) or not (0 <= rewiring_prob <= 1):\n",
    "         raise ValueError(f\"rewiring_prob must be between 0 and 1, got {rewiring_prob}\")\n",
    "\n",
    "\n",
    "    # --- Generate Graph ---\n",
    "    ws_graph = None # Initialize to None\n",
    "    try:\n",
    "        # Call NetworkX function with validated parameters\n",
    "        ws_graph = nx.watts_strogatz_graph(n=n_nodes, k=k_neighbors, p=rewiring_prob, seed=seed)\n",
    "    except nx.NetworkXError as e:\n",
    "        # Catch potential errors during graph generation itself\n",
    "        print(f\"‚ùå Error generating WS graph (n={n_nodes}, k={k_neighbors}, p={rewiring_prob}): {e}\")\n",
    "        ws_graph = None # Ensure None is returned on failure\n",
    "\n",
    "    # Return the generated graph (or None if generation failed)\n",
    "    return ws_graph\n",
    "\n",
    "\n",
    "def generate_sbm_graph(n_nodes, block_sizes_list, p_intra_community, p_inter_community, seed=None):\n",
    "    \"\"\"Generates a Stochastic Block Model graph with input validation.\"\"\"\n",
    "    # --- Input Validation ---\n",
    "    if not isinstance(n_nodes, int) or n_nodes <= 0:\n",
    "         raise ValueError(f\"n_nodes must be a positive integer, got {n_nodes}\")\n",
    "    if not isinstance(block_sizes_list, list) or not block_sizes_list:\n",
    "         raise ValueError(\"block_sizes_list must be a non-empty list of integers.\")\n",
    "    # Check block sizes are positive integers\n",
    "    block_idx = 0\n",
    "    valid_blocks = True\n",
    "    sum_block_sizes = 0\n",
    "    while block_idx < len(block_sizes_list):\n",
    "         size = block_sizes_list[block_idx]\n",
    "         if not isinstance(size, int) or size <= 0:\n",
    "              valid_blocks = False\n",
    "              break\n",
    "         sum_block_sizes = sum_block_sizes + size\n",
    "         block_idx = block_idx + 1\n",
    "    if not valid_blocks:\n",
    "         raise ValueError(\"block_sizes_list must contain only positive integers.\")\n",
    "    # Check if block sizes sum to n_nodes\n",
    "    if sum_block_sizes != n_nodes:\n",
    "         warnings.warn(f\"SBM block sizes sum ({sum_block_sizes}) != n_nodes ({n_nodes}). Check generation logic.\", RuntimeWarning)\n",
    "         # Do not raise error, NetworkX might handle this, but warn user.\n",
    "    # Check probabilities\n",
    "    if not (isinstance(p_intra_community, (float, int)) and 0 <= p_intra_community <= 1):\n",
    "        raise ValueError(f\"p_intra_community must be between 0 and 1, got {p_intra_community}\")\n",
    "    if not (isinstance(p_inter_community, (float, int)) and 0 <= p_inter_community <= 1):\n",
    "        raise ValueError(f\"p_inter_community must be between 0 and 1, got {p_inter_community}\")\n",
    "\n",
    "    num_blocks = len(block_sizes_list)\n",
    "\n",
    "    # --- Construct Probability Matrix ---\n",
    "    # Initialize empty list for the matrix\n",
    "    probability_matrix = []\n",
    "    # Outer loop for rows\n",
    "    row_idx = 0\n",
    "    while row_idx < num_blocks:\n",
    "        # Initialize empty list for the current row\n",
    "        current_row_probabilities = []\n",
    "        # Inner loop for columns\n",
    "        col_idx = 0\n",
    "        while col_idx < num_blocks:\n",
    "            # Check if it's a diagonal element (intra-community)\n",
    "            if row_idx == col_idx:\n",
    "                current_row_probabilities.append(p_intra_community)\n",
    "            else: # Off-diagonal element (inter-community)\n",
    "                current_row_probabilities.append(p_inter_community)\n",
    "            col_idx = col_idx + 1 # Increment column index\n",
    "        # Append the completed row to the matrix\n",
    "        probability_matrix.append(current_row_probabilities)\n",
    "        row_idx = row_idx + 1 # Increment row index\n",
    "\n",
    "\n",
    "    # --- Generate Graph ---\n",
    "    sbm_graph = None # Initialize to None\n",
    "    try:\n",
    "        # Call NetworkX SBM function\n",
    "        sbm_graph = nx.stochastic_block_model(sizes=block_sizes_list, p=probability_matrix, seed=seed)\n",
    "    except Exception as e:\n",
    "        # Catch potential errors during graph generation\n",
    "        print(f\"‚ùå Error generating SBM graph (sizes={block_sizes_list}, p_in={p_intra_community}, p_out={p_inter_community}): {e}\")\n",
    "        sbm_graph = None # Ensure None is returned on failure\n",
    "\n",
    "    return sbm_graph\n",
    "\n",
    "\n",
    "def generate_rgg_graph(n_nodes, connection_radius, seed=None):\n",
    "    \"\"\"Generates a Random Geometric Graph with input validation.\"\"\"\n",
    "    # --- Input Validation ---\n",
    "    if not isinstance(n_nodes, int) or n_nodes <= 0:\n",
    "         raise ValueError(f\"n_nodes must be a positive integer, got {n_nodes}\")\n",
    "    # Radius should typically be positive, NetworkX might handle 0 but warn.\n",
    "    if not isinstance(connection_radius, (float, int)) or connection_radius < 0:\n",
    "         raise ValueError(f\"connection_radius must be non-negative, got {connection_radius}\")\n",
    "    if connection_radius == 0:\n",
    "         warnings.warn(\"connection_radius is 0, graph will likely have no edges.\", RuntimeWarning)\n",
    "\n",
    "    # --- Seed Position Generation (if seed provided) ---\n",
    "    # Note: NetworkX's random_geometric_graph uses np.random if seed is not None,\n",
    "    # but we can also seed the standard `random` module if positions were generated manually.\n",
    "    # Since NetworkX handles it, explicit seeding here might be redundant unless using manual pos.\n",
    "    # if seed is not None:\n",
    "    #     random.seed(seed) # Seed standard random module if needed elsewhere\n",
    "\n",
    "    # --- Generate Graph ---\n",
    "    # NetworkX's rgg handles position generation internally if 'pos' is not provided.\n",
    "    rgg_graph = None # Initialize to None\n",
    "    try:\n",
    "        # Call NetworkX function, passing the seed directly\n",
    "        rgg_graph = nx.random_geometric_graph(n=n_nodes, radius=connection_radius, seed=seed)\n",
    "        # Optionally, store positions if needed later:\n",
    "        # pos = nx.get_node_attributes(rgg_graph, 'pos')\n",
    "    except Exception as e:\n",
    "        # Catch potential errors during graph generation\n",
    "        print(f\"‚ùå Error generating RGG graph (n={n_nodes}, r={connection_radius}): {e}\")\n",
    "        rgg_graph = None # Ensure None is returned on failure\n",
    "\n",
    "    return rgg_graph\n",
    "\n",
    "print(\"‚úÖ Cell 7: Graph generation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ba3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imported Phase 1 worker function: run_single_instance\n",
      "\n",
      "--- Cell 8: Run Parametric Sweep (GPU - Phase 1 WS Sweep - Completed State) ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "‚ùå FATAL: Error loading configuration for sweep: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 107\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Validate loaded parameters\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m system_sizes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m param_values:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSYSTEM_SIZES or parameter values list is empty in config.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå FATAL: Missing essential key \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in configuration dictionary. Run Cell 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e_conf:\n\u001b[0;32m--> 113\u001b[0m      \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå FATAL: Error loading configuration for sweep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me_conf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTARGET_MODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSweep Parameter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(param_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ‚ùå FATAL: Error loading configuration for sweep: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# Cell 8: Run Parametric Sweep (Phase 1 Baseline - WS Model - Corrected Emptiness Check)\n",
    "# Description: Runs the primary WS sweep as a baseline using the original Phase 1\n",
    "#              worker (`run_single_instance`) for direct comparison.\n",
    "#              Corrects the check for empty system_sizes/param_values.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import traceback\n",
    "import json # Import json for loading config\n",
    "\n",
    "# *** Import ORIGINAL Worker Function from Phase 1 utils ***\n",
    "# This cell runs the BASELINE Phase 1 sweep, so it needs the original worker\n",
    "try:\n",
    "    # Check if it was already imported, otherwise import\n",
    "    if 'run_single_instance' not in globals():\n",
    "        from worker_utils import run_single_instance\n",
    "        print(\"‚úÖ Imported ORIGINAL Phase 1 worker: run_single_instance from worker_utils.py\")\n",
    "    else:\n",
    "        # Optional: Add check if the existing one is the Phase 2 version\n",
    "        # This logic might be complex, maybe just re-import or trust context\n",
    "        print(\"‚ÑπÔ∏è Using pre-existing 'run_single_instance' (ensure it's the Phase 1 version for this cell).\")\n",
    "except ImportError:\n",
    "    raise ImportError(\"‚ùå ERROR: Cannot import Phase 1 worker 'run_single_instance' from worker_utils.py.\")\n",
    "\n",
    "# *** Ensure Helpers Defined (From Cell 2) ***\n",
    "if 'generate_graph' not in globals(): raise NameError(\"generate_graph not defined. Run Cell 2.\")\n",
    "if 'get_sweep_parameters' not in globals(): raise NameError(\"get_sweep_parameters not defined. Run Cell 2.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Cell 8: Run Parametric Sweep (Phase 1 Baseline - WS Model - Corrected Emptiness Check) ---\")\n",
    "\n",
    "# --- Configuration Loading ---\n",
    "config = {}\n",
    "analysis_error_sweep = False\n",
    "try:\n",
    "    # Use the config loaded in Cell 1\n",
    "    if 'config' in globals() and isinstance(globals()['config'], dict) and 'OUTPUT_DIR' in globals()['config']:\n",
    "        config = globals()['config']\n",
    "        print(f\"  Using configuration loaded in Cell 1 (Experiment: {config.get('EXPERIMENT_NAME', 'N/A')})\")\n",
    "    else:\n",
    "        # Attempt to load Phase 2 config if global config is missing (less ideal)\n",
    "        output_dir_base_ph2 = \"emergenics_phase2_results\"\n",
    "        exp_pattern_ph2 = \"Emergenics_Phase2\" # Adjust pattern if needed\n",
    "        if os.path.isdir(output_dir_base_ph2):\n",
    "            all_subdirs_ph2 = [d for d in os.listdir(output_dir_base_ph2) if os.path.isdir(os.path.join(output_dir_base_ph2, d)) and d.startswith(exp_pattern_ph2)]\n",
    "            if all_subdirs_ph2:\n",
    "                latest_run_dir_ph2 = max([os.path.join(output_dir_base_ph2, d) for d in all_subdirs_ph2], key=os.path.getmtime)\n",
    "                config_path_ph2 = os.path.join(latest_run_dir_ph2, \"run_config_phase2.json\")\n",
    "                if os.path.exists(config_path_ph2):\n",
    "                    with open(config_path_ph2, 'r') as f_ph2:\n",
    "                        config = json.load(f_ph2)\n",
    "                    print(f\"  Loaded configuration from latest Phase 2 run: {latest_run_dir_ph2}\")\n",
    "                else:\n",
    "                    raise FileNotFoundError(\"Config file not found in latest Phase 2 dir.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(\"No Phase 2 experiment directories found.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Phase 2 base directory not found.\")\n",
    "\n",
    "    # Extract necessary parameters from the loaded config\n",
    "    TARGET_MODEL = 'WS' # Hardcode to WS for this baseline run\n",
    "    graph_params_all = config.get('GRAPH_MODEL_PARAMS', {}) # Use original Phase 1 ranges\n",
    "    if not isinstance(graph_params_all, dict): graph_params_all = {} # Ensure dict\n",
    "    graph_model_params = graph_params_all.get(TARGET_MODEL, {})\n",
    "    if not isinstance(graph_model_params, dict): graph_model_params = {} # Ensure dict\n",
    "\n",
    "    param_name = None; param_values = None; primary_param_key_found = False\n",
    "    param_key_iter = iter(graph_model_params.keys())\n",
    "    current_key = next(param_key_iter, None)\n",
    "    while current_key is not None:\n",
    "         values = graph_model_params[current_key]\n",
    "         if isinstance(values, (list, np.ndarray)):\n",
    "             param_name = current_key.replace('_values', '')\n",
    "             param_values = values\n",
    "             primary_param_key_found = True\n",
    "             # break # Assuming only one sweep param per model in config\n",
    "         current_key = next(param_key_iter, None)\n",
    "\n",
    "    if not primary_param_key_found:\n",
    "         # Handle cases like RGG if needed, though hardcoded to WS here\n",
    "         if TARGET_MODEL=='RGG' and 'radius_values' in graph_model_params:\n",
    "              param_name='radius'\n",
    "              param_values=graph_model_params['radius_values']\n",
    "         else:\n",
    "              # Default if sweep param not found\n",
    "              param_name = 'p' # Default for WS if not found\n",
    "              param_values = np.logspace(-5, 0, 20) # Default values\n",
    "              warnings.warn(f\"Sweep param values not found for {TARGET_MODEL} in loaded config. Using default range.\")\n",
    "\n",
    "    system_sizes = config.get('SYSTEM_SIZES', [300, 500, 700])\n",
    "    num_instances = config.get('NUM_INSTANCES_PER_PARAM', 10)\n",
    "    num_trials = config.get('NUM_TRIALS_PER_INSTANCE', 3)\n",
    "    rule_params_base = config.get('RULE_PARAMS', {})\n",
    "    max_steps = config.get('MAX_SIMULATION_STEPS', 200)\n",
    "    conv_thresh = config.get('CONVERGENCE_THRESHOLD', 1e-4)\n",
    "    state_dim = config.get('STATE_DIM', 5)\n",
    "    workers = config.get('PARALLEL_WORKERS', os.cpu_count())\n",
    "    output_dir = config.get('OUTPUT_DIR', \".\") # Use Phase 2 output dir for saving these baseline results\n",
    "    exp_name = config.get('EXPERIMENT_NAME', \"Phase2_BaselineRun\")\n",
    "    calculate_energy = config.get('CALCULATE_ENERGY', True)\n",
    "    store_energy_history = config.get('STORE_ENERGY_HISTORY', False) # Phase 1 setting\n",
    "    energy_type = config.get('ENERGY_FUNCTIONAL_TYPE', 'pairwise_dot')\n",
    "    primary_metric = config.get('PRIMARY_ORDER_PARAMETER', 'variance_norm')\n",
    "    all_metrics = config.get('ORDER_PARAMETERS_TO_ANALYZE', ['variance_norm', 'entropy_dim_0', 'final_energy'])\n",
    "\n",
    "    # *** CORRECTED VALIDATION ***\n",
    "    # Check if lists/arrays are empty using len() which works for both lists and numpy arrays\n",
    "    if len(system_sizes) == 0 or (param_values is not None and len(param_values) == 0) or param_values is None:\n",
    "        raise ValueError(\"SYSTEM_SIZES or parameter values list is empty or None in config.\")\n",
    "\n",
    "except (ValueError, KeyError) as e_key:\n",
    "     raise KeyError(f\"‚ùå FATAL: Missing essential key or invalid value ('{e_key}') in configuration dictionary. Run Cell 1.\")\n",
    "except Exception as e_conf:\n",
    "     raise RuntimeError(f\"‚ùå FATAL: Error loading configuration for sweep: {e_conf}\")\n",
    "\n",
    "\n",
    "print(f\"Target Model: {TARGET_MODEL}\")\n",
    "print(f\"Sweep Parameter: {param_name} (Values: {len(param_values)})\")\n",
    "print(f\"System Sizes: {system_sizes}\")\n",
    "print(f\"Using {workers} workers.\")\n",
    "print(f\"Results will be saved relative to: {output_dir}\")\n",
    "\n",
    "# --- Device Check ---\n",
    "if torch.cuda.is_available(): device = torch.device('cuda:0')\n",
    "else: device = torch.device('cpu')\n",
    "print(f\"  Using device: {device}\")\n",
    "\n",
    "# --- Prepare Sweep Tasks ---\n",
    "# Use Phase 1 style sweep generation (no perturbation etc.)\n",
    "sweep_tasks = get_sweep_parameters(\n",
    "    graph_model_name=TARGET_MODEL,\n",
    "    model_params=graph_model_params,\n",
    "    system_sizes=system_sizes,\n",
    "    instances=num_instances,\n",
    "    trials=num_trials\n",
    ")\n",
    "print(f\"Prepared {len(sweep_tasks)} {TARGET_MODEL} baseline tasks across {len(system_sizes)} sizes.\")\n",
    "\n",
    "# --- Setup Logging & Partial Results ---\n",
    "# Save baseline results with a distinct name in the *Phase 2* output directory\n",
    "baseline_log_file = os.path.join(output_dir, f\"{exp_name}_{TARGET_MODEL}_baseline_sweep.log\")\n",
    "baseline_partial_results_file = os.path.join(output_dir, f\"{exp_name}_{TARGET_MODEL}_baseline_sweep_partial.pkl\")\n",
    "completed_tasks_signatures = set(); all_results_list = []\n",
    "\n",
    "# Robust loading logic for baseline files\n",
    "if os.path.exists(baseline_log_file):\n",
    "    try:\n",
    "        with open(baseline_log_file, 'r') as f: completed_tasks_signatures = set(line.strip() for line in f)\n",
    "    except Exception as e_load_log: print(f\" Warning: Could not load baseline log: {e_load_log}\")\n",
    "if os.path.exists(baseline_partial_results_file):\n",
    "    try:\n",
    "        with open(baseline_partial_results_file, 'rb') as f: all_results_list = pickle.load(f)\n",
    "        # Rebuild signatures from loaded data if log was incomplete/corrupt\n",
    "        if all_results_list and not completed_tasks_signatures:\n",
    "             temp_df_signatures = pd.DataFrame(all_results_list)\n",
    "             param_value_key_load = param_name + '_value'\n",
    "             if all(k in temp_df_signatures.columns for k in ['N', param_value_key_load, 'instance', 'trial']):\n",
    "                  completed_tasks_signatures = set( f\"N={row['N']}_{param_name}={row[param_value_key_load]:.5f}_inst={row['instance']}_trial={row['trial']}\" for _, row in temp_df_signatures.iterrows() )\n",
    "                  print(f\" Rebuilt {len(completed_tasks_signatures)} signatures from partial results.\")\n",
    "             del temp_df_signatures\n",
    "    except Exception as e_load_pkl: print(f\" Warning: Could not load baseline partial results: {e_load_pkl}\"); all_results_list = []\n",
    "print(f\"Loaded {len(completed_tasks_signatures)} completed task signatures and {len(all_results_list)} previous baseline results.\")\n",
    "\n",
    "# Filter tasks\n",
    "tasks_to_run = []; param_value_key_filter = param_name + '_value'\n",
    "task_idx_filter = 0\n",
    "while task_idx_filter < len(sweep_tasks):\n",
    "    task_params = sweep_tasks[task_idx_filter]\n",
    "    # Basic check for required keys before creating signature\n",
    "    if param_value_key_filter not in task_params or 'N' not in task_params or 'instance' not in task_params or 'trial' not in task_params:\n",
    "        warnings.warn(f\"Skipping task due to missing keys: {task_params}\", RuntimeWarning)\n",
    "        task_idx_filter += 1\n",
    "        continue\n",
    "    # Format float for signature robustly\n",
    "    try: p_val_fmt = f\"{task_params[param_value_key_filter]:.5f}\"\n",
    "    except (TypeError, ValueError): p_val_fmt = str(task_params[param_value_key_filter]) # Fallback\n",
    "    task_sig = f\"N={task_params['N']}_{param_name}={p_val_fmt}_inst={task_params['instance']}_trial={task_params['trial']}\"\n",
    "    if task_sig not in completed_tasks_signatures:\n",
    "        tasks_to_run.append(task_params)\n",
    "    task_idx_filter += 1\n",
    "\n",
    "\n",
    "# --- Execute Sweep in Parallel ---\n",
    "if tasks_to_run:\n",
    "    print(f\"Executing {len(tasks_to_run)} new {TARGET_MODEL} baseline tasks (Device: {device}, Workers: {workers})...\")\n",
    "    try: # Set spawn method\n",
    "        if mp.get_start_method(allow_none=True) != 'spawn': mp.set_start_method('spawn', force=True); print(\"  Set multiprocessing start method to 'spawn'.\")\n",
    "    except Exception as e_spawn: print(f\" Warning: Could not force spawn method: {e_spawn}\")\n",
    "\n",
    "    start_time = time.time(); futures = []; pool_broken_flag = False\n",
    "    # Define executor instance robustly\n",
    "    actual_workers = min(workers, os.cpu_count() if os.cpu_count() else 1) # Limit workers if needed\n",
    "    executor_instance = ProcessPoolExecutor(max_workers=actual_workers)\n",
    "    try:\n",
    "        # Submit tasks using a while loop\n",
    "        task_idx_submit = 0\n",
    "        while task_idx_submit < len(tasks_to_run):\n",
    "            task_params = tasks_to_run[task_idx_submit]\n",
    "            param_value_key_submit = param_name + '_value'\n",
    "            if param_value_key_submit not in task_params:\n",
    "                 warnings.warn(f\"Skipping submit task missing key {param_value_key_submit}: {task_params}\")\n",
    "                 task_idx_submit += 1; continue\n",
    "\n",
    "            # Combine fixed params and sweep param for graph generation\n",
    "            graph_gen_params = task_params.get('fixed_params', {}).copy()\n",
    "            graph_gen_params[param_name] = task_params[param_value_key_submit]\n",
    "\n",
    "            G = generate_graph( task_params['model'], graph_gen_params, task_params['N'], task_params['graph_seed'] )\n",
    "\n",
    "            if G is None or G.number_of_nodes() == 0:\n",
    "                 warnings.warn(f\"Skipping task due to failed graph generation: {task_params}\")\n",
    "                 task_idx_submit += 1; continue # Skip failed graph gen\n",
    "\n",
    "            # Submit using the ORIGINAL run_single_instance\n",
    "            future = executor_instance.submit(\n",
    "                run_single_instance, # Calling the Phase 1 worker\n",
    "                graph=G, N=task_params['N'], instance_params=task_params, trial_seed=task_params['sim_seed'],\n",
    "                rule_params_in=rule_params_base, max_steps=max_steps, conv_thresh=conv_thresh, state_dim=state_dim,\n",
    "                calculate_energy=calculate_energy, store_energy_history=store_energy_history,\n",
    "                energy_type=energy_type, metrics_to_calc=all_metrics,\n",
    "                device=str(device) # Pass device name as string\n",
    "            )\n",
    "            futures.append((future, task_params))\n",
    "            task_idx_submit += 1\n",
    "\n",
    "        # Collect results using a while loop and iterator\n",
    "        pbar = tqdm(total=len(futures), desc=f\"{TARGET_MODEL} Baseline Sweep\", mininterval=2.0)\n",
    "        log_frequency = max(1, len(futures) // 50); save_frequency = max(20, len(futures) // 10)\n",
    "        tasks_processed_since_save = 0\n",
    "        completed_futures_iterator = as_completed(dict(futures).keys()) # Create iterator from futures\n",
    "        futures_processed_count = 0\n",
    "\n",
    "        with open(baseline_log_file, 'a') as f_log:\n",
    "             while futures_processed_count < len(futures):\n",
    "                 if pool_broken_flag:\n",
    "                      # If pool breaks, attempt to update progress bar for remaining tasks\n",
    "                      remaining_updates = len(futures) - futures_processed_count\n",
    "                      pbar.update(remaining_updates)\n",
    "                      break # Exit the loop\n",
    "                 try:\n",
    "                      future = next(completed_futures_iterator) # Get the next completed future\n",
    "                      task_params = dict(futures)[future] # Find corresponding task params\n",
    "\n",
    "                      result_dict = future.result(timeout=1200) # Increased timeout\n",
    "                      if result_dict:\n",
    "                           full_result = copy.deepcopy(task_params)\n",
    "                           full_result.update(result_dict)\n",
    "                           all_results_list.append(full_result); tasks_processed_since_save += 1\n",
    "                           # Log completion\n",
    "                           param_value_key_log = param_name + '_value'\n",
    "                           if result_dict.get('error_message') is None and param_value_key_log in task_params:\n",
    "                                try: p_val_fmt_log = f\"{task_params[param_value_key_log]:.5f}\"\n",
    "                                except (TypeError, ValueError): p_val_fmt_log = str(task_params[param_value_key_log])\n",
    "                                task_sig = f\"N={task_params['N']}_{param_name}={p_val_fmt_log}_inst={task_params['instance']}_trial={task_params['trial']}\"\n",
    "                                f_log.write(f\"{task_sig}\\n\")\n",
    "                                if futures_processed_count % log_frequency == 0: f_log.flush() # Flush periodically\n",
    "                 except StopIteration: # Should not happen if loop condition is correct, but safe catch\n",
    "                      break\n",
    "                 except Exception as e_collect:\n",
    "                      if \"Broken\" in str(e_collect) or \"abruptly\" in str(e_collect) or isinstance(e_collect, TypeError):\n",
    "                           print(f\"\\n‚ùå ERROR: Pool broke during result collection. Exception: {type(e_collect).__name__}: {e_collect}\")\n",
    "                           pool_broken_flag = True\n",
    "                           # Continue loop to update progress bar, but don't process more results\n",
    "                      else:\n",
    "                           # Log other errors if needed, but don't break pool flag\n",
    "                           warnings.warn(f\"Error processing future result: {type(e_collect).__name__}: {e_collect}\", RuntimeWarning)\n",
    "                           # traceback.print_exc(limit=1) # Optional detail\n",
    "                 finally:\n",
    "                      pbar.update(1)\n",
    "                      futures_processed_count += 1\n",
    "                      # Save partial results periodically\n",
    "                      should_save_partial = tasks_processed_since_save >= save_frequency or futures_processed_count == len(futures)\n",
    "                      if should_save_partial and tasks_processed_since_save > 0 :\n",
    "                           try:\n",
    "                               with open(baseline_partial_results_file, 'wb') as f_partial:\n",
    "                                   pickle.dump(all_results_list, f_partial)\n",
    "                               tasks_processed_since_save = 0 # Reset counter after successful save\n",
    "                           except Exception as e_save_part: print(f\" Warning: Failed to save partial results: {e_save_part}\")\n",
    "\n",
    "    except KeyboardInterrupt: print(\"\\nExecution interrupted by user.\")\n",
    "    except Exception as main_e: print(f\"\\n‚ùå ERROR during parallel execution setup: {main_e}\"); traceback.print_exc(limit=2)\n",
    "    finally:\n",
    "        pbar.close(); print(\"Shutting down executor...\"); executor_instance.shutdown(wait=True, cancel_futures=False); print(\"Executor shut down.\") # Allow completing futures on shutdown\n",
    "        try: # Final save attempt\n",
    "            with open(baseline_partial_results_file, 'wb') as f_partial: pickle.dump(all_results_list, f_partial)\n",
    "        except Exception as e_final_save: print(f\" Warning: Final save failed: {e_final_save}\")\n",
    "        end_time = time.time(); print(f\"\\n‚úÖ Parallel execution block completed ({end_time - start_time:.1f}s).\")\n",
    "else: print(f\"‚úÖ No new baseline tasks to run for {TARGET_MODEL} sweep.\")\n",
    "\n",
    "\n",
    "# --- Process Final Results ---\n",
    "print(\"\\nProcessing final baseline results...\")\n",
    "# *** Initialize global variable to empty DataFrame ***\n",
    "global_sweep_results_baseline = pd.DataFrame() # Use a distinct name for baseline results\n",
    "# ****************************************************\n",
    "if not all_results_list: print(\"‚ö†Ô∏è No baseline results collected.\")\n",
    "else:\n",
    "    try: # Add try-except around DataFrame creation and processing\n",
    "        final_results_df = pd.DataFrame(all_results_list)\n",
    "        # --- Add Check after DataFrame creation ---\n",
    "        print(f\"  DEBUG: Baseline DataFrame created successfully? {'Yes' if not final_results_df.empty else 'NO - DataFrame is empty!'}\")\n",
    "        print(f\"  DEBUG: Baseline DataFrame shape after creation: {final_results_df.shape}\")\n",
    "        # ------------------------------------------\n",
    "\n",
    "        # Error checking from worker runs\n",
    "        if 'error_message' in final_results_df.columns:\n",
    "             failed_run_count = final_results_df['error_message'].notna().sum()\n",
    "             if failed_run_count > 0: warnings.warn(f\"{failed_run_count} baseline runs reported errors.\")\n",
    "\n",
    "        # Ensure primary order parameter exists\n",
    "        if primary_metric not in final_results_df.columns and 'order_parameter' not in final_results_df.columns:\n",
    "            warnings.warn(f\"Primary metric '{primary_metric}' not found! Cannot set 'order_parameter'.\")\n",
    "        elif primary_metric in final_results_df.columns:\n",
    "             # Create or overwrite 'order_parameter' column using the primary metric\n",
    "             final_results_df['order_parameter'] = final_results_df[primary_metric]\n",
    "             final_results_df['metric_name'] = primary_metric # Store which metric was used\n",
    "        else:\n",
    "             # If primary metric is missing but 'order_parameter' exists, trust it but warn\n",
    "             warnings.warn(f\"Primary metric '{primary_metric}' missing, using existing 'order_parameter'.\")\n",
    "\n",
    "\n",
    "        print(f\"Collected baseline results from {final_results_df.shape[0]} total attempted runs.\")\n",
    "        final_csv_path = os.path.join(output_dir, f\"{exp_name}_{TARGET_MODEL}_baseline_sweep_results.csv\")\n",
    "        try:\n",
    "            final_results_df.to_csv(final_csv_path, index=False); print(f\"‚úÖ Final {TARGET_MODEL} baseline sweep results saved to CSV.\")\n",
    "            # *** Explicitly assign to global variable ***\n",
    "            global_sweep_results_baseline = final_results_df\n",
    "            print(f\"  DEBUG: Assigned baseline results to global_sweep_results_baseline.\")\n",
    "            # *******************************************\n",
    "        except Exception as e_save:\n",
    "             print(f\"‚ùå Error saving final baseline CSV: {e_save}\")\n",
    "             print(\"  DEBUG: Global variable 'global_sweep_results_baseline' might be empty due to save failure.\")\n",
    "    except Exception as e_proc:\n",
    "        print(f\"‚ùå ERROR during final baseline results processing: {e_proc}\")\n",
    "        traceback.print_exc(limit=2)\n",
    "        print(\"  DEBUG: Global variable 'global_sweep_results_baseline' will be empty due to processing error.\")\n",
    "\n",
    "\n",
    "# *** Add Final Check at the very end of the cell ***\n",
    "print(\"\\n--- Final Check within Cell 8 ---\")\n",
    "if 'global_sweep_results_baseline' in globals() and isinstance(global_sweep_results_baseline, pd.DataFrame) and not global_sweep_results_baseline.empty:\n",
    "    print(f\"  ‚úÖ global_sweep_results_baseline DataFrame exists and is not empty. Shape: {global_sweep_results_baseline.shape}\")\n",
    "    # print(global_sweep_results_baseline.head()) # Optional: print head to verify\n",
    "else:\n",
    "    print(f\"  ‚ùå global_sweep_results_baseline DataFrame is MISSING or EMPTY at the end of Cell 8!\")\n",
    "    print(f\"     Type: {type(globals().get('global_sweep_results_baseline'))}\")\n",
    "    if 'final_results_df' in locals():\n",
    "         print(f\"     (Local final_results_df existed with shape: {final_results_df.shape})\")\n",
    "    else:\n",
    "         print(\"     (Local final_results_df did not exist)\")\n",
    "# *************************************************\n",
    "\n",
    "print(f\"\\n‚úÖ Cell 8: Parametric baseline sweep for {TARGET_MODEL} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08f483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 9: Critical Point Analysis (FSS on Susceptibility with Optuna) ---\n",
      "‚úÖ Configuration dictionary loaded.\n",
      "‚ùå FATAL: Missing key ''OUTPUT_DIR'' in loaded configuration.\n",
      "\n",
      "‚ùå Skipping Analysis Steps 9.2-9.3 due to configuration or diagnostic errors.\n",
      "\n",
      "‚úÖ Cell 9: Analysis completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Critical Point Analysis (FSS on Susceptibility with Optuna)\n",
    "# Description: Calculates Susceptibility (Chi). Uses Optuna to find the best FSS parameters\n",
    "#              (pc, gamma/nu, 1/nu) by minimizing collapse error for Chi. Plots the result.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit, minimize # Keep minimize for comparison if needed\n",
    "import warnings\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "import optuna # Import Optuna\n",
    "\n",
    "# --- Suppress Optuna INFO messages for cleaner output ---\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"\\n--- Cell 9: Critical Point Analysis (FSS on Susceptibility with Optuna) ---\")\n",
    "\n",
    "# --- Explicitly Load Configuration ---\n",
    "# Load config dictionary (should exist from Cell 1)\n",
    "config = {}\n",
    "analysis_error = False\n",
    "if 'config' not in globals() or not isinstance(config, dict):\n",
    "     print(\"‚ùå FATAL: Config dictionary missing. Run Cell 1 first.\")\n",
    "     analysis_error = True\n",
    "else:\n",
    "     config = globals()['config'] # Use the existing config\n",
    "     print(f\"‚úÖ Configuration dictionary loaded.\")\n",
    "\n",
    "# Load necessary parameters from config if no error\n",
    "if not analysis_error:\n",
    "    try:\n",
    "        output_dir = config['OUTPUT_DIR']\n",
    "        exp_name = config['EXPERIMENT_NAME']\n",
    "        primary_metric = config.get('PRIMARY_ORDER_PARAMETER', 'variance_norm') # Need M for moments\n",
    "        system_sizes = config.get('SYSTEM_SIZES', [])\n",
    "        # Determine parameter name for the primary sweep (WS model in this case)\n",
    "        param_name = 'p_value' # Default for WS\n",
    "        ws_params = config.get('GRAPH_MODEL_PARAMS', {}).get('WS', {})\n",
    "        ws_param_key = next((k for k in ws_params if k.endswith('_values')), None)\n",
    "        if ws_param_key:\n",
    "            param_name = ws_param_key.replace('_values', '_value') # Get the column name format\n",
    "        else:\n",
    "             warnings.warn(\"Could not dynamically determine WS sweep parameter name from config, defaulting to 'p_value'.\")\n",
    "\n",
    "\n",
    "        num_trials = config.get('NUM_TRIALS_PER_INSTANCE', 1) # For variance calc accuracy check\n",
    "    except KeyError as e_key:\n",
    "        print(f\"‚ùå FATAL: Missing key '{e_key}' in loaded configuration.\")\n",
    "        analysis_error = True\n",
    "    except Exception as config_e:\n",
    "        print(f\"‚ùå FATAL: Failed to load necessary parameters from configuration: {config_e}\")\n",
    "        analysis_error = True\n",
    "\n",
    "# --- Helper Function ---\n",
    "def format_metric(value, fmt):\n",
    "    \"\"\"Safely formats a numerical value using a format string.\"\"\"\n",
    "    # Check if value is valid number (not None, NaN, Inf)\n",
    "    is_valid_number = False\n",
    "    if value is not None:\n",
    "        if isinstance(value, (int, float)):\n",
    "             if np.isfinite(value):\n",
    "                  is_valid_number = True\n",
    "\n",
    "    if is_valid_number:\n",
    "        try:\n",
    "            # Apply formatting\n",
    "            formatted_string = fmt % value\n",
    "            return formatted_string\n",
    "        except (TypeError, ValueError):\n",
    "            # Handle potential formatting errors\n",
    "            return \"Format Error\"\n",
    "    else:\n",
    "        # Return N/A for invalid inputs\n",
    "        return \"N/A\"\n",
    "\n",
    "# --- Diagnostic Check ---\n",
    "if not analysis_error:\n",
    "    print(\"\\n--- Step 9.1: Diagnosing Input Data (`global_sweep_results`) ---\")\n",
    "    # Check if the primary results DataFrame exists and is valid\n",
    "    if 'global_sweep_results' not in globals():\n",
    "        analysis_error = True; print(\"‚ùå FATAL: `global_sweep_results` DataFrame missing (Run Cell 8).\")\n",
    "    elif not isinstance(global_sweep_results, pd.DataFrame):\n",
    "        analysis_error = True; print(\"‚ùå FATAL: `global_sweep_results` is not a Pandas DataFrame.\")\n",
    "    elif global_sweep_results.empty:\n",
    "        analysis_error = True; print(\"‚ùå FATAL: `global_sweep_results` DataFrame is empty.\")\n",
    "    else:\n",
    "        print(f\"  DataFrame Shape: {global_sweep_results.shape}\")\n",
    "        # Check for required columns needed for susceptibility calculation\n",
    "        required_cols = ['N', param_name, primary_metric, 'instance', 'trial']\n",
    "        missing_cols = []\n",
    "        col_idx = 0\n",
    "        while col_idx < len(required_cols):\n",
    "             col = required_cols[col_idx]\n",
    "             if col not in global_sweep_results.columns:\n",
    "                  missing_cols.append(col)\n",
    "             col_idx += 1\n",
    "\n",
    "        if len(missing_cols) > 0:\n",
    "             analysis_error = True; print(f\"‚ùå FATAL: Missing required columns in DataFrame: {missing_cols}.\")\n",
    "        else:\n",
    "             print(f\"  Required columns found: {required_cols}.\")\n",
    "             # Check unique system sizes (N)\n",
    "             unique_N = global_sweep_results['N'].unique()\n",
    "             print(f\"  Unique 'N' values found: {sorted(unique_N)}\")\n",
    "             if len(unique_N) < 2: # Need at least two sizes for FSS\n",
    "                  analysis_error = True; print(f\"‚ùå FATAL: Need at least 2 unique 'N' values for FSS. Found {len(unique_N)}.\")\n",
    "             else:\n",
    "                  print(\"  Sufficient unique 'N' values for FSS.\")\n",
    "                  # Check the primary metric column for NaNs\n",
    "                  print(f\"\\n  Diagnostics for primary metric column ('{primary_metric}'):\")\n",
    "                  metric_col = global_sweep_results[primary_metric]\n",
    "                  total_count = len(metric_col)\n",
    "                  non_nan_count = metric_col.notna().sum()\n",
    "                  nan_count = metric_col.isna().sum()\n",
    "                  print(f\"    Total entries: {total_count}\")\n",
    "                  print(f\"    Non-NaN entries: {non_nan_count}\")\n",
    "                  print(f\"    NaN entries: {nan_count}\")\n",
    "\n",
    "                  if non_nan_count == 0:\n",
    "                       analysis_error = True; print(f\"‚ùå FATAL: Primary metric column '{primary_metric}' contains only NaNs.\")\n",
    "                  else:\n",
    "                       # Display basic statistics if data is valid\n",
    "                       try:\n",
    "                           print(\"    Basic Stats (non-NaN values):\\n\", metric_col.describe())\n",
    "                           print(\"‚úÖ Input data appears valid for susceptibility calculation.\")\n",
    "                       except Exception as desc_e:\n",
    "                            analysis_error = True; print(f\"‚ùå Error getting descriptive statistics: {desc_e}\")\n",
    "\n",
    "# --- Initialize results dictionary for this cell ---\n",
    "global_optuna_fss_chi_results = {} # Store Optuna results specifically for Chi FSS\n",
    "\n",
    "# --- Proceed only if diagnostics passed ---\n",
    "if not analysis_error:\n",
    "    print(f\"\\n--- Step 9.2: Aggregating Susceptibility (œá) ---\")\n",
    "    fss_chi_df = pd.DataFrame() # Initialize empty dataframe\n",
    "    try:\n",
    "        # Calculate variance of the primary order parameter (M) across trials/instances\n",
    "        # Group by system size (N) and the sweep parameter (p)\n",
    "        # Use observed=True to handle potential categorical nature of grouped columns\n",
    "        # Convert primary metric to numeric first, coercing errors to NaN\n",
    "        M_numeric = pd.to_numeric(global_sweep_results[primary_metric], errors='coerce')\n",
    "        # Calculate variance, requires at least 2 data points per group\n",
    "        var_M = global_sweep_results.assign(M_numeric=M_numeric).groupby(['N', param_name], observed=True)['M_numeric'].var()\n",
    "\n",
    "        # Check if variance calculation produced NaNs (indicates insufficient data in some groups)\n",
    "        if var_M.isna().any():\n",
    "            nan_groups = var_M[var_M.isna()].index.tolist()\n",
    "            warnings.warn(f\"NaNs found in Var(M) calculation for {len(nan_groups)} groups, possibly due to insufficient trials/instances per group. These groups will be dropped.\", RuntimeWarning)\n",
    "            # print(\"Groups with NaN variance:\", nan_groups[:5]) # Print first few examples\n",
    "\n",
    "        # Calculate Susceptibility: œá = N * Var(M)\n",
    "        # Multiply the Series var_M by the corresponding 'N' level of its MultiIndex\n",
    "        susceptibility_chi_agg = var_M.index.get_level_values('N') * var_M\n",
    "\n",
    "        # Combine into a DataFrame for FSS analysis\n",
    "        fss_chi_df = pd.DataFrame({'susceptibility_chi': susceptibility_chi_agg}).reset_index()\n",
    "        # Drop rows where susceptibility could not be calculated (resulted in NaN)\n",
    "        fss_chi_df = fss_chi_df.dropna()\n",
    "\n",
    "        # Check if the resulting DataFrame is usable for FSS\n",
    "        if fss_chi_df.empty:\n",
    "             raise ValueError(\"Susceptibility DataFrame is empty after aggregation/dropna.\")\n",
    "        if fss_chi_df['N'].nunique() < 2 :\n",
    "            raise ValueError(f\"Susceptibility DataFrame has < 2 unique sizes ({fss_chi_df['N'].unique()}) after aggregation/dropna.\")\n",
    "\n",
    "        print(f\"  Aggregated Susceptibility (œá) ready for FSS (Entries: {len(fss_chi_df)}).\")\n",
    "        # print(\"  Sample aggregated data:\\n\", fss_chi_df.head())\n",
    "\n",
    "    except KeyError as e_agg_key:\n",
    "        print(f\"‚ùå Error aggregating susceptibility: Missing column {e_agg_key}\")\n",
    "        traceback.print_exc(limit=1)\n",
    "        analysis_error = True\n",
    "    except Exception as agg_chi_e:\n",
    "        print(f\"‚ùå Error aggregating susceptibility: {agg_chi_e}\")\n",
    "        traceback.print_exc(limit=1)\n",
    "        analysis_error = True\n",
    "\n",
    "\n",
    "# --- FSS on Susceptibility using Optuna ---\n",
    "if not analysis_error:\n",
    "    print(f\"\\n--- Step 9.3: FSS on Susceptibility using Optuna ---\")\n",
    "\n",
    "    # --- Prepare Data for Optuna Objective ---\n",
    "    # Extract columns needed for scaling into NumPy arrays\n",
    "    # Ensure data types are appropriate for calculations (float)\n",
    "    try:\n",
    "        Ls_chi = fss_chi_df['N'].values.astype(np.float64) # System sizes (L)\n",
    "        ps_chi = fss_chi_df[param_name].values.astype(np.float64) # Control parameter (p)\n",
    "        Ms_chi = fss_chi_df['susceptibility_chi'].values.astype(np.float64) # Observable (M = Chi)\n",
    "    except KeyError as e_fss_prep:\n",
    "         print(f\"‚ùå Error preparing FSS data: Missing column {e_fss_prep}.\")\n",
    "         analysis_error = True\n",
    "    except Exception as e_fss_prep_other:\n",
    "         print(f\"‚ùå Error preparing FSS data: {e_fss_prep_other}.\")\n",
    "         analysis_error = True\n",
    "\n",
    "\n",
    "    # --- Define Optuna Objective Function ---\n",
    "    # This function calculates collapse error for given trial parameters (pc, gamma/nu, 1/nu)\n",
    "    # It aims to minimize the variance of scaled data points within bins along the scaled x-axis.\n",
    "    def objective_fss_chi(trial):\n",
    "        # Suggest parameters within defined ranges using Optuna's trial object\n",
    "        # Log scale for pc is often useful if pc is expected near zero\n",
    "        pc = trial.suggest_float(\"pc\", 1e-5, 0.1, log=True)\n",
    "        # Range for exponent ratios - adjust based on expected physics or preliminary results\n",
    "        gamma_nu = trial.suggest_float(\"gamma_over_nu\", 0.1, 3.0) # gamma / nu\n",
    "        one_nu = trial.suggest_float(\"one_over_nu\", 0.1, 5.0)   # 1 / nu\n",
    "\n",
    "        # --- Calculate scaled variables & collapse error ---\n",
    "        # Scaling for Susceptibility: Y = Chi * L^(-gamma/nu), X = (p - pc) * L^(1/nu)\n",
    "        # Use 'p' or 'r' depending on the model, pc is the critical value being tested\n",
    "        scaled_x = (ps_chi - pc) * (Ls_chi ** one_nu)\n",
    "        scaled_y = Ms_chi * (Ls_chi ** (-gamma_nu)) # Note the negative sign in exponent for Chi scaling\n",
    "\n",
    "        # Sort data points based on the scaled X values for binning\n",
    "        # This is crucial for calculating variance within defined x-bins\n",
    "        sorted_indices = np.argsort(scaled_x)\n",
    "        scaled_x_sorted = scaled_x[sorted_indices]\n",
    "        scaled_y_sorted = scaled_y[sorted_indices]\n",
    "\n",
    "        # --- Calculate Collapse Error using Binning ---\n",
    "        total_error = 0.0 # Accumulator for variance within bins\n",
    "        num_bins = 20 # Number of bins to divide the scaled x-axis into\n",
    "\n",
    "        try:\n",
    "            # Filter out potential Inf/-Inf values resulting from scaling (e.g., L^large_exponent)\n",
    "            # Only consider points where both scaled_x and scaled_y are finite\n",
    "            valid_indices = np.isfinite(scaled_x_sorted) & np.isfinite(scaled_y_sorted)\n",
    "            if not np.any(valid_indices):\n",
    "                # If no valid points remain after scaling, return infinity (worst possible collapse)\n",
    "                return np.inf\n",
    "\n",
    "            scaled_x_finite = scaled_x_sorted[valid_indices]\n",
    "            scaled_y_finite = scaled_y_sorted[valid_indices]\n",
    "\n",
    "            # Ensure there are enough points for binning\n",
    "            num_valid_points = len(scaled_x_finite)\n",
    "            if num_valid_points < num_bins:\n",
    "                # Reduce bins if fewer points than requested bins\n",
    "                num_bins = max(1, num_valid_points // 2) # Ensure at least one bin\n",
    "\n",
    "            # Determine bin edges based on the range of finite scaled x values\n",
    "            min_x = np.min(scaled_x_finite)\n",
    "            max_x = np.max(scaled_x_finite)\n",
    "\n",
    "            # Handle edge case where all scaled x values are identical\n",
    "            if abs(min_x - max_x) < 1e-9:\n",
    "                # If all x are same, collapse error is just the variance of y\n",
    "                if num_valid_points > 1:\n",
    "                     return np.var(scaled_y_finite)\n",
    "                else:\n",
    "                     return 0.0 # Zero error if only one point\n",
    "\n",
    "            # Define bin edges using linspace\n",
    "            bins = np.linspace(min_x, max_x, num_bins + 1)\n",
    "            # Assign each data point to a bin based on its scaled x value\n",
    "            bin_indices = np.digitize(scaled_x_finite, bins)\n",
    "\n",
    "            # Calculate variance within each bin\n",
    "            non_empty_bin_count = 0 # Count bins with enough data for variance calculation\n",
    "            bin_idx = 1 # Bin indices from digitize start at 1\n",
    "            while bin_idx <= num_bins:\n",
    "                # Select scaled y values belonging to the current bin\n",
    "                y_in_bin = scaled_y_finite[bin_indices == bin_idx]\n",
    "                # Calculate variance only if there are at least 2 points in the bin\n",
    "                if len(y_in_bin) > 1:\n",
    "                    total_error = total_error + np.var(y_in_bin)\n",
    "                    non_empty_bin_count = non_empty_bin_count + 1\n",
    "                bin_idx = bin_idx + 1 # Increment bin index\n",
    "\n",
    "            # Return average variance across non-empty bins (lower is better collapse)\n",
    "            if non_empty_bin_count > 0:\n",
    "                average_variance = total_error / non_empty_bin_count\n",
    "                return average_variance\n",
    "            else:\n",
    "                # If no bins had enough data, return infinity (indicates poor parameter choice)\n",
    "                return np.inf\n",
    "\n",
    "        except Exception as e_obj:\n",
    "            # Catch any unexpected errors during error calculation\n",
    "            warnings.warn(f\"Error in objective function: {e_obj}\", RuntimeWarning)\n",
    "            return np.inf # Return high error on any calculation failure\n",
    "\n",
    "\n",
    "    # --- Run Optuna Study ---\n",
    "    n_optuna_trials = 100 # Number of optimization trials (adjust as needed for convergence)\n",
    "    print(f\"  Running Optuna study ({n_optuna_trials} trials) to find best FSS parameters for Chi...\")\n",
    "    # Create study object, aiming to minimize the objective function (collapse error)\n",
    "    study_chi = optuna.create_study(direction='minimize')\n",
    "    optimization_success = False\n",
    "    try:\n",
    "        # Run the optimization process\n",
    "        study_chi.optimize(\n",
    "            objective_fss_chi, # The function to minimize\n",
    "            n_trials=n_optuna_trials, # Number of trials to run\n",
    "            show_progress_bar=True # Display progress bar\n",
    "        )\n",
    "        optimization_success = True # Mark as successful if optimize completes\n",
    "\n",
    "    except Exception as optuna_err:\n",
    "        print(f\"‚ùå Error during Optuna optimization: {optuna_err}\")\n",
    "        traceback.print_exc(limit=2)\n",
    "        global_optuna_fss_chi_results = {'success': False} # Store failure state\n",
    "        analysis_error = True # Mark analysis as failed\n",
    "\n",
    "    # --- Process and Store Best Results ---\n",
    "    if optimization_success:\n",
    "        if study_chi.best_trial:\n",
    "            best_params = study_chi.best_params # Dictionary of best parameters found\n",
    "            best_value = study_chi.best_value   # The minimum objective value achieved\n",
    "\n",
    "            # Extract best parameters\n",
    "            pc_opt = best_params['pc']\n",
    "            gamma_nu_opt = best_params['gamma_over_nu']\n",
    "            one_nu_opt = best_params['one_over_nu']\n",
    "\n",
    "            # Calculate original exponents gamma and nu from the optimized ratios\n",
    "            nu_opt = np.nan\n",
    "            gamma_opt = np.nan\n",
    "            # Avoid division by zero if 1/nu is very close to zero\n",
    "            if abs(one_nu_opt) > 1e-6:\n",
    "                 nu_opt = 1.0 / one_nu_opt\n",
    "                 gamma_opt = gamma_nu_opt * nu_opt # gamma = (gamma/nu) * nu\n",
    "            else:\n",
    "                 warnings.warn(\"Optuna result 1/nu is too close to zero. Cannot calculate nu and gamma.\", RuntimeWarning)\n",
    "\n",
    "\n",
    "            # Store results in the global dictionary for this cell\n",
    "            global_optuna_fss_chi_results = {\n",
    "                'pc': pc_opt,\n",
    "                'gamma': gamma_opt,\n",
    "                'nu': nu_opt,\n",
    "                'gamma_over_nu': gamma_nu_opt, # Store the ratio directly optimized\n",
    "                'one_over_nu': one_nu_opt,     # Store the ratio directly optimized\n",
    "                'success': True,\n",
    "                'objective': best_value        # Store the best collapse error found\n",
    "            }\n",
    "            print(\"\\n  ‚úÖ Optuna FSS Optimization Successful for Chi:\")\n",
    "            print(f\"     Best Objective Value (Avg Variance): {best_value:.4e}\")\n",
    "            print(f\"     p_c (Optuna) ‚âà {pc_opt:.6f}\")\n",
    "            print(f\"     Œ≥ (Optuna)   ‚âà {format_metric(gamma_opt, '%.4f')}\")\n",
    "            print(f\"     ŒΩ (Optuna)   ‚âà {format_metric(nu_opt, '%.4f')}\")\n",
    "            print(f\"     (Optimized Ratios: Œ≥/ŒΩ ‚âà {gamma_nu_opt:.4f}, 1/ŒΩ ‚âà {one_nu_opt:.4f})\")\n",
    "        else:\n",
    "             print(\"  ‚ùå Optuna study completed but reported no best trial was found.\")\n",
    "             global_optuna_fss_chi_results = {'success': False}\n",
    "\n",
    "\n",
    "    # --- Plot FSS Data Collapse using Optuna Results ---\n",
    "    if global_optuna_fss_chi_results.get('success', False):\n",
    "        print(\"  Generating FSS data collapse plot for Chi using Optuna parameters...\")\n",
    "        # Retrieve best parameters needed for plotting\n",
    "        pc = global_optuna_fss_chi_results['pc']\n",
    "        gamma_nu = global_optuna_fss_chi_results['gamma_over_nu']\n",
    "        one_nu = global_optuna_fss_chi_results['one_over_nu']\n",
    "        nu_val = global_optuna_fss_chi_results['nu'] # For label formatting\n",
    "\n",
    "        # Recalculate scaled variables using the optimal parameters\n",
    "        scaled_x = (ps_chi - pc) * (Ls_chi ** one_nu)\n",
    "        scaled_y = Ms_chi * (Ls_chi ** (-gamma_nu)) # Y = Chi * L^(-gamma/nu)\n",
    "\n",
    "        # Create plot\n",
    "        fig_fss_chi, ax_fss_chi = plt.subplots(figsize=(8, 6))\n",
    "        # Get unique system sizes for plotting legend and colors\n",
    "        unique_Ls_plot = sorted(np.unique(Ls_chi))\n",
    "        # Create color map for different system sizes\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(unique_Ls_plot)))\n",
    "\n",
    "        # Plot data for each system size using a loop\n",
    "        l_idx = 0\n",
    "        while l_idx < len(unique_Ls_plot):\n",
    "            L = unique_Ls_plot[l_idx]\n",
    "            # Create mask to select data for the current system size L\n",
    "            mask = Ls_chi == L\n",
    "            # Scatter plot for this system size\n",
    "            ax_fss_chi.scatter(scaled_x[mask], scaled_y[mask],\n",
    "                               label=f'N={int(L)}', # Legend label\n",
    "                               color=colors[l_idx], # Color based on size\n",
    "                               alpha=0.7, s=20) # Adjust alpha/size for visibility\n",
    "            l_idx += 1 # Increment loop counter\n",
    "\n",
    "        # Configure plot labels and title\n",
    "        xlabel_str = f'$(p - p_c) N^{{1/\\\\nu}}$  (p$_c$‚âà{pc:.4f}, ŒΩ‚âà{format_metric(nu_val,\"%.3f\")})'\n",
    "        ylabel_str = f'$\\\\chi \\\\times N^{{-\\\\gamma/\\\\nu}}$  (Œ≥/ŒΩ‚âà{gamma_nu:.3f})'\n",
    "        ax_fss_chi.set_xlabel(xlabel_str)\n",
    "        ax_fss_chi.set_ylabel(ylabel_str)\n",
    "        ax_fss_chi.set_title(f'FSS Data Collapse for Susceptibility œá (Optuna Fit - WS)')\n",
    "        ax_fss_chi.grid(True, linestyle=':')\n",
    "        ax_fss_chi.legend(title='System Size N')\n",
    "\n",
    "        # Optional: Adjust plot limits based on scaled data range if needed\n",
    "        # x_min, x_max = np.percentile(scaled_x[np.isfinite(scaled_x)], [1, 99])\n",
    "        # y_min, y_max = np.percentile(scaled_y[np.isfinite(scaled_y)], [1, 99])\n",
    "        # ax_fss_chi.set_xlim(x_min * 1.1, x_max * 1.1)\n",
    "        # ax_fss_chi.set_ylim(y_min * 1.1, y_max * 1.1)\n",
    "\n",
    "        plt.tight_layout() # Adjust layout\n",
    "        # Define filename and save the plot\n",
    "        fss_chi_plot_filename = os.path.join(output_dir, f\"{exp_name}_WS_Susceptibility_FSS_collapse_OPTUNA.png\")\n",
    "        try:\n",
    "            plt.savefig(fss_chi_plot_filename, dpi=150)\n",
    "            print(f\"  ‚úÖ FSS Chi Collapse plot (Optuna) saved to: {fss_chi_plot_filename}\")\n",
    "        except Exception as e_save:\n",
    "            print(f\"  ‚ùå Error saving FSS Chi plot: {e_save}\")\n",
    "        plt.close(fig_fss_chi) # Close plot figure to free memory\n",
    "    else:\n",
    "        # Message if Optuna failed or no results found\n",
    "        print(\"  Skipping FSS Chi collapse plot as Optuna optimization did not yield successful results.\")\n",
    "\n",
    "# Final message if analysis was skipped due to initial errors\n",
    "elif analysis_error:\n",
    "    print(\"\\n‚ùå Skipping Analysis Steps 9.2-9.3 due to configuration or diagnostic errors.\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 9: Analysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69df2851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 10: Report Final Critical Parameters (WS Model) ---\n",
      "‚ùå Cannot report final parameters: Optuna FSS Chi optimization failed (success=False).\n",
      "‚ùå Skipping final parameter reporting due to missing or failed analysis results.\n",
      "\n",
      "‚úÖ Cell 10: Final critical parameter reporting completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Report Final Critical Parameters (WS Model)\n",
    "# Description: Reports the final, most reliable estimates for the critical point (pc)\n",
    "#              and exponents (gamma, nu) based on the successful Optuna FSS analysis\n",
    "#              of Susceptibility (Chi) from Cell 9. Beta remains undetermined by this method.\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd # Import pandas for safe checking\n",
    "\n",
    "print(\"\\n--- Cell 10: Report Final Critical Parameters (WS Model) ---\")\n",
    "\n",
    "# --- Prerequisites ---\n",
    "reporting_error = False\n",
    "# Check if config exists (should be loaded by Cell 1 or earlier)\n",
    "if 'config' not in globals() or not isinstance(config, dict):\n",
    "    print(\"‚ùå Cannot report final parameters: Config dictionary missing. Run Cell 1.\")\n",
    "    reporting_error = True\n",
    "\n",
    "# Check for results from Optuna FSS on Chi (from Cell 9)\n",
    "optuna_results_exist = False\n",
    "if 'global_optuna_fss_chi_results' in globals():\n",
    "    if isinstance(global_optuna_fss_chi_results, dict):\n",
    "         if global_optuna_fss_chi_results.get('success', False):\n",
    "              optuna_results_exist = True\n",
    "         else:\n",
    "              print(\"‚ùå Cannot report final parameters: Optuna FSS Chi optimization failed (success=False).\")\n",
    "              reporting_error = True\n",
    "    else:\n",
    "         print(\"‚ùå Cannot report final parameters: Optuna FSS Chi results 'global_optuna_fss_chi_results' is not a dictionary.\")\n",
    "         reporting_error = True\n",
    "else:\n",
    "    print(\"‚ùå Cannot report final parameters: Optuna FSS Chi results 'global_optuna_fss_chi_results' missing (Run Cell 9).\")\n",
    "    reporting_error = True\n",
    "\n",
    "# Load necessary config variables if no error yet\n",
    "output_dir = None\n",
    "exp_name = None\n",
    "primary_metric = None\n",
    "if not reporting_error:\n",
    "     try:\n",
    "          output_dir = config['OUTPUT_DIR']\n",
    "          exp_name = config['EXPERIMENT_NAME']\n",
    "          primary_metric = config.get('PRIMARY_ORDER_PARAMETER', 'variance_norm') # Metric for context\n",
    "     except KeyError as e_key_rep:\n",
    "           print(f\"‚ùå Cannot report final parameters: Missing key '{e_key_rep}' in config.\")\n",
    "           reporting_error = True\n",
    "\n",
    "\n",
    "# --- Report Final Parameters from Optuna FSS Chi ---\n",
    "if not reporting_error:\n",
    "    # Safely get results from the Optuna dictionary using .get() with NaN default\n",
    "    pc_final = global_optuna_fss_chi_results.get('pc', np.nan)\n",
    "    gamma_final = global_optuna_fss_chi_results.get('gamma', np.nan)\n",
    "    nu_final = global_optuna_fss_chi_results.get('nu', np.nan)\n",
    "    success = global_optuna_fss_chi_results.get('success', False) # Should be True here\n",
    "\n",
    "    # Helper function for consistent formatting\n",
    "    def format_report(value, fmt):\n",
    "        try: return fmt % value if pd.notna(value) else \"N/A\"\n",
    "        except (TypeError, ValueError): return \"Format Error\"\n",
    "\n",
    "    print(f\"  ‚úÖ Final Critical Parameters for WS Model Transition (from Susceptibility œá FSS):\")\n",
    "    print(f\"     Critical Point (p_c): {format_report(pc_final, '%.6f')}\")\n",
    "    print(f\"     Exponent Gamma (Œ≥):   {format_report(gamma_final, '%.4f')}\")\n",
    "    print(f\"     Exponent Nu (ŒΩ):      {format_report(nu_final, '%.4f')}\")\n",
    "    print(f\"\\n  Note: Exponent Beta (Œ≤) related to the order parameter ('{primary_metric}')\")\n",
    "    print(\"        could not be reliably determined using standard FSS collapse methods in Phase 1.\")\n",
    "    print(\"        Susceptibility (œá) FSS proved most effective.\")\n",
    "\n",
    "    # --- Save Key Metrics ---\n",
    "    key_metrics_path = os.path.join(output_dir, f\"{exp_name}_key_metrics.json\")\n",
    "    # Load existing metrics if file exists, update with new values\n",
    "    key_metrics = {}\n",
    "    if os.path.exists(key_metrics_path):\n",
    "        try:\n",
    "             with open(key_metrics_path, 'r') as f_read_metrics:\n",
    "                  key_metrics = json.load(f_read_metrics)\n",
    "             # Basic validation if loaded data is a dictionary\n",
    "             if not isinstance(key_metrics, dict):\n",
    "                  warnings.warn(f\"Existing key metrics file '{key_metrics_path}' is not a valid JSON dictionary. Overwriting.\", RuntimeWarning)\n",
    "                  key_metrics = {}\n",
    "        except json.JSONDecodeError as e_load_json:\n",
    "             warnings.warn(f\"Could not decode existing key metrics file '{key_metrics_path}'. Overwriting. Error: {e_load_json}\", RuntimeWarning)\n",
    "             key_metrics = {}\n",
    "        except Exception as e_load:\n",
    "             warnings.warn(f\"Could not load existing key metrics file '{key_metrics_path}'. Overwriting. Error: {e_load}\", RuntimeWarning)\n",
    "             key_metrics = {}\n",
    "\n",
    "    # Update with final WS values (prefixing to avoid name clashes if other models analyzed later)\n",
    "    # Store results specifically from the Chi FSS analysis\n",
    "    key_metrics['final_pc_ws_chi'] = pc_final\n",
    "    key_metrics['final_gamma_ws_chi'] = gamma_final\n",
    "    key_metrics['final_nu_ws_chi'] = nu_final\n",
    "    key_metrics['ws_chi_fss_success'] = success\n",
    "    key_metrics['ws_chi_fss_objective'] = global_optuna_fss_chi_results.get('objective', np.nan)\n",
    "\n",
    "\n",
    "    # Optionally include original FSS results for comparison if needed\n",
    "    # (Assuming original FSS was attempted and stored in global_fss_results_orig)\n",
    "    # if 'global_fss_results_orig' in globals() and isinstance(global_fss_results_orig, dict):\n",
    "    #     if global_fss_results_orig.get('success'):\n",
    "    #         key_metrics['orig_fss_pc_ws_var'] = global_fss_results_orig.get('pc')\n",
    "    #         key_metrics['orig_fss_beta_ws_var'] = global_fss_results_orig.get('beta')\n",
    "    #         key_metrics['orig_fss_nu_ws_var'] = global_fss_results_orig.get('nu')\n",
    "    #     key_metrics['orig_fss_var_success'] = global_fss_results_orig.get('success', False)\n",
    "\n",
    "\n",
    "    # Save the updated metrics dictionary to JSON\n",
    "    try:\n",
    "        with open(key_metrics_path, 'w') as f_write_metrics:\n",
    "            # Use default serializer to handle potential numpy types if any remain\n",
    "             def default_serializer(obj):\n",
    "                if isinstance(obj, (np.bool_)): return bool(obj)\n",
    "                if isinstance(obj, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64)): return int(obj)\n",
    "                if isinstance(obj, (np.float_, np.float16, np.float32, np.float64)): return float(obj) if np.isfinite(obj) else None # Convert NaN/Inf to None\n",
    "                if isinstance(obj, (np.ndarray,)): return obj.tolist() # Convert arrays to lists\n",
    "                try: return str(obj)\n",
    "                except TypeError: return repr(obj) # Fallback if str fails\n",
    "\n",
    "             json.dump(key_metrics, f_write_metrics, indent=4, default=default_serializer)\n",
    "        print(f\"\\n  ‚úÖ Saved/Updated final WS critical parameters to: {key_metrics_path}\")\n",
    "    except TypeError as e_type_save:\n",
    "        print(f\"  ‚ö†Ô∏è Error saving final key metrics (TypeError - check data types): {e_type_save}\")\n",
    "        traceback.print_exc(limit=1)\n",
    "    except Exception as e_save:\n",
    "        print(f\"  ‚ö†Ô∏è Error saving final key metrics: {e_save}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Skipping final parameter reporting due to missing or failed analysis results.\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 10: Final critical parameter reporting completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7ebc945",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2091917542.py, line 318)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 318\u001b[0;36m\u001b[0m\n\u001b[0;31m    except KeyboardInterrupt: print(f\"\\nInterrupted ({model_name}).\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Universality Testing Sweeps (GPU - Final Implementation - Indentation Fix)\n",
    "# Description: Runs or loads sweeps for SBM and RGG models using the GPU-enabled\n",
    "#              run_single_instance function. Combines results. Corrects indentation error.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp # Ensure imported\n",
    "import torch # Ensure imported\n",
    "import traceback # Ensure imported\n",
    "\n",
    "print(\"\\n--- Cell 11: Universality Testing Sweeps (GPU - Final Implementation - Indentation Fix) ---\")\n",
    "\n",
    "# --- Configuration & Prerequisite Checks ---\n",
    "analysis_error_uni = False\n",
    "if 'config' not in globals():\n",
    "    print(\"‚ùå FATAL: Config dictionary missing. Run Cell 1.\"); analysis_error_uni = True\n",
    "if 'global_device' not in globals():\n",
    "    print(\"‚ùå FATAL: Global device not defined. Run Cell 0.\"); analysis_error_uni = True\n",
    "if 'get_sweep_parameters' not in globals() or 'generate_graph' not in globals():\n",
    "     print(\"‚ùå FATAL: Helper functions missing. Run Cell 2.\"); analysis_error_uni = True\n",
    "# Check worker function availability\n",
    "worker_func = None\n",
    "if 'run_single_instance' in globals():\n",
    "    worker_func = run_single_instance # Use Phase 1 worker if available\n",
    "    print(\"  Using Phase 1 worker 'run_single_instance'.\")\n",
    "elif 'run_single_instance_phase2' in globals():\n",
    "     worker_func = run_single_instance_phase2 # Fallback to Phase 2 worker\n",
    "     print(\"  Using Phase 2 worker 'run_single_instance_phase2'.\")\n",
    "else:\n",
    "     print(\"‚ùå FATAL: No suitable worker function found ('run_single_instance' or 'run_single_instance_phase2').\")\n",
    "     analysis_error_uni = True\n",
    "\n",
    "\n",
    "# --- Load Configuration Variables ---\n",
    "if not analysis_error_uni:\n",
    "     try:\n",
    "         device = global_device\n",
    "         output_dir = config['OUTPUT_DIR']; exp_name = config['EXPERIMENT_NAME']\n",
    "         system_sizes_uni = config['SYSTEM_SIZES']\n",
    "         graph_params_all = config['GRAPH_MODEL_PARAMS']\n",
    "         num_instances = config['NUM_INSTANCES_PER_PARAM']\n",
    "         num_trials = config['NUM_TRIALS_PER_INSTANCE']\n",
    "         workers = config['PARALLEL_WORKERS']\n",
    "         rule_params_base = config['RULE_PARAMS']\n",
    "         max_steps = config['MAX_SIMULATION_STEPS']\n",
    "         conv_thresh = config['CONVERGENCE_THRESHOLD']\n",
    "         state_dim = config['STATE_DIM']\n",
    "         calculate_energy = config.get('CALCULATE_ENERGY', False)\n",
    "         store_energy_history = config.get('STORE_ENERGY_HISTORY', False)\n",
    "         energy_type = config.get('ENERGY_FUNCTIONAL_TYPE', 'pairwise_dot')\n",
    "         all_metrics = config.get('ORDER_PARAMETERS_TO_ANALYZE', [])\n",
    "     except KeyError as e_key_uni:\n",
    "          print(f\"‚ùå FATAL: Missing key '{e_key_uni}' in config for universality sweeps.\")\n",
    "          analysis_error_uni = True\n",
    "     except Exception as e_conf_uni:\n",
    "          print(f\"‚ùå FATAL: Error loading config for universality sweeps: {e_conf_uni}.\")\n",
    "          analysis_error_uni = True\n",
    "\n",
    "# --- File Paths & Loading Existing Results ---\n",
    "combined_results_file = None\n",
    "combined_pickle_file = None\n",
    "all_universality_results_list = []\n",
    "models_available = []\n",
    "models_to_run = []\n",
    "\n",
    "if not analysis_error_uni:\n",
    "    combined_results_file = os.path.join(output_dir, f\"{exp_name}_universality_COMBINED_results.csv\")\n",
    "    combined_pickle_file = os.path.join(output_dir, f\"{exp_name}_universality_COMBINED_partial.pkl\")\n",
    "    models_available = list(graph_params_all.keys()) # Get all models defined in config\n",
    "    models_to_run = models_available[:] # Start assuming all models need to run\n",
    "\n",
    "    # Robust loading logic for combined_pickle_file/CSV\n",
    "    if os.path.exists(combined_pickle_file):\n",
    "        try:\n",
    "            with open(combined_pickle_file, 'rb') as f_load_pkl:\n",
    "                all_universality_results_list = pickle.load(f_load_pkl)\n",
    "            if isinstance(all_universality_results_list, list) and len(all_universality_results_list) > 0:\n",
    "                 # If pickle loaded successfully, determine which models are already present\n",
    "                 loaded_df = pd.DataFrame(all_universality_results_list)\n",
    "                 if 'model' in loaded_df.columns:\n",
    "                     models_completed = loaded_df['model'].unique()\n",
    "                     # Update models_to_run by removing completed models\n",
    "                     models_to_run = [m for m in models_available if m not in models_completed]\n",
    "                     print(f\"  Loaded {len(all_universality_results_list)} combined results from pickle. Models already completed: {list(models_completed)}\")\n",
    "                 else:\n",
    "                      warnings.warn(\"Loaded pickle file is missing 'model' column. Cannot determine completed models reliably.\", RuntimeWarning)\n",
    "                      # Assume need to run all if 'model' column is missing\n",
    "                      models_to_run = models_available[:]\n",
    "                      all_universality_results_list = [] # Reset list if format is unexpected\n",
    "            else:\n",
    "                  # Reset list if loaded object is not a non-empty list\n",
    "                  all_universality_results_list = []\n",
    "        except Exception as e_load_uni_pkl:\n",
    "            warnings.warn(f\"Could not load or parse universality pickle file '{combined_pickle_file}'. Error: {e_load_uni_pkl}\", RuntimeWarning)\n",
    "            all_universality_results_list = [] # Reset list on failure\n",
    "\n",
    "    # If pickle didn't exist or failed load, check for CSV (less ideal as vectors are lost)\n",
    "    elif os.path.exists(combined_results_file):\n",
    "         warnings.warn(f\"Pickle file not found, attempting to load from CSV '{combined_results_file}'. Note: State vectors will be missing.\", RuntimeWarning)\n",
    "         try:\n",
    "              # Load CSV, assuming it has the 'model' column\n",
    "              loaded_df_csv = pd.read_csv(combined_results_file)\n",
    "              if 'model' in loaded_df_csv.columns:\n",
    "                   # Convert CSV rows to dictionaries (approximates structure, loses vectors)\n",
    "                   all_universality_results_list = loaded_df_csv.to_dict('records')\n",
    "                   models_completed_csv = loaded_df_csv['model'].unique()\n",
    "                   models_to_run = [m for m in models_available if m not in models_completed_csv]\n",
    "                   print(f\"  Loaded {len(all_universality_results_list)} results from CSV. Models already completed: {list(models_completed_csv)}\")\n",
    "              else:\n",
    "                   warnings.warn(\"Loaded CSV file is missing 'model' column. Assuming no models completed.\", RuntimeWarning)\n",
    "                   models_to_run = models_available[:]\n",
    "                   all_universality_results_list = []\n",
    "         except Exception as e_load_uni_csv:\n",
    "              warnings.warn(f\"Could not load universality CSV file '{combined_results_file}'. Error: {e_load_uni_csv}\", RuntimeWarning)\n",
    "              models_to_run = models_available[:]\n",
    "              all_universality_results_list = []\n",
    "\n",
    "\n",
    "    print(f\"  Models remaining to run for Universality tests: {models_to_run}\")\n",
    "\n",
    "# --- Run Sweeps for Remaining Models ---\n",
    "if not analysis_error_uni and models_to_run:\n",
    "    print(\"\\n--- Running Individual Model Universality Sweeps ---\")\n",
    "    # Set spawn method if needed (should be done in Cell 0)\n",
    "    try:\n",
    "        if mp.get_start_method(allow_none=True) != 'spawn':\n",
    "             print(\"üö® WARNING: Multiprocessing start method not 'spawn'. Forcing again.\")\n",
    "             mp.set_start_method('spawn', force=True)\n",
    "    except Exception as e_set_spawn:\n",
    "         print(f\"‚ö†Ô∏è Warning: Could not force 'spawn' start method: {e_set_spawn}\")\n",
    "\n",
    "    # Iterate through models that need to be run\n",
    "    model_idx_outer = 0\n",
    "    while model_idx_outer < len(models_to_run):\n",
    "        model_name = models_to_run[model_idx_outer]\n",
    "        print(f\"\\n--- Running Universality Experiment for Model: {model_name} ---\")\n",
    "        # Get parameters for the current model\n",
    "        model_params = config['GRAPH_MODEL_PARAMS'].get(model_name, {})\n",
    "        if not model_params:\n",
    "             warnings.warn(f\"No parameters found for model '{model_name}' in config. Skipping.\", RuntimeWarning)\n",
    "             model_idx_outer += 1; continue\n",
    "\n",
    "        # Find the sweep parameter name for this model dynamically\n",
    "        param_name_uni = None\n",
    "        param_col_name_uni = None\n",
    "        model_param_iter = iter(model_params.items())\n",
    "        stop_model_param_iter = False\n",
    "        while not stop_model_param_iter:\n",
    "             try:\n",
    "                  key, value = next(model_param_iter)\n",
    "                  if isinstance(value, (list, np.ndarray)):\n",
    "                       param_name_uni = key.replace('_values', '')\n",
    "                       param_col_name_uni = param_name_uni + '_value'\n",
    "                       stop_model_param_iter = True\n",
    "             except StopIteration:\n",
    "                  stop_model_param_iter = True\n",
    "        # Fallback if no list/array found\n",
    "        if param_name_uni is None:\n",
    "             warnings.warn(f\"Could not find sweep parameter for model {model_name}. Assuming 'param'.\", RuntimeWarning)\n",
    "             param_name_uni = 'param'; param_col_name_uni = 'param_value'\n",
    "\n",
    "\n",
    "        # --- Setup per-model Logging & Partial Results ---\n",
    "        model_log_file = os.path.join(output_dir, f\"{exp_name}_universality_{model_name}.log\")\n",
    "        model_partial_results_file = os.path.join(output_dir, f\"{exp_name}_universality_{model_name}_partial.pkl\")\n",
    "        model_completed_tasks = set(); model_results_list = [] # Reset for each model\n",
    "\n",
    "        # --- Robust loading for per-model files (to handle restarts within a model run) ---\n",
    "        if os.path.exists(model_log_file):\n",
    "            try:\n",
    "                with open(model_log_file, 'r') as f_mlog:\n",
    "                    line_idx=0\n",
    "                    for line in f_mlog:\n",
    "                        sig = line.strip();\n",
    "                        if sig: model_completed_tasks.add(sig)\n",
    "                        line_idx+=1\n",
    "            except Exception as e_mlog: warnings.warn(f\"Could not load model log {model_log_file}: {e_mlog}\", RuntimeWarning)\n",
    "        if os.path.exists(model_partial_results_file):\n",
    "            try:\n",
    "                with open(model_partial_results_file, 'rb') as f_mpkl: model_results_list = pickle.load(f_mpkl)\n",
    "                # Rebuild signatures from loaded model results if list is valid\n",
    "                if isinstance(model_results_list, list) and len(model_results_list) > 0:\n",
    "                     temp_df_sig_model = pd.DataFrame(model_results_list)\n",
    "                     required_sig_cols_model = ['N', param_col_name_uni, 'instance', 'trial']\n",
    "                     cols_exist_model = all(k in temp_df_sig_model.columns for k in required_sig_cols_model)\n",
    "                     if cols_exist_model:\n",
    "                          rebuilt_model_sigs = set()\n",
    "                          row_idx_m = 0\n",
    "                          while row_idx_m < len(temp_df_sig_model):\n",
    "                               row_m = temp_df_sig_model.iloc[row_idx_m]\n",
    "                               try:\n",
    "                                   sig_m = f\"N={int(row_m['N'])}_{param_name_uni}={row_m[param_col_name_uni]:.5f}_inst={int(row_m['instance'])}_trial={int(row_m['trial'])}\"\n",
    "                                   rebuilt_model_sigs.add(sig_m)\n",
    "                               except Exception: pass # Ignore formatting errors\n",
    "                               row_idx_m += 1\n",
    "                          model_completed_tasks.update(rebuilt_model_sigs)\n",
    "                     else: warnings.warn(f\"Model partial results for {model_name} missing columns for signature rebuild.\", RuntimeWarning)\n",
    "                     del temp_df_sig_model\n",
    "                else: model_results_list = [] # Reset if not list or empty\n",
    "            except Exception as e_mpkl: warnings.warn(f\"Could not load model partial results {model_partial_results_file}: {e_mpkl}\", RuntimeWarning); model_results_list = []\n",
    "\n",
    "        # Generate & Filter tasks for the current model\n",
    "        uni_tasks_model = get_sweep_parameters(\n",
    "            graph_model_name=model_name, model_params=model_params,\n",
    "            system_sizes=system_sizes_uni, instances=num_instances, trials=num_trials\n",
    "        )\n",
    "        model_tasks_to_run = []; # List of tasks still needing execution\n",
    "        task_filter_idx = 0\n",
    "        while task_filter_idx < len(uni_tasks_model):\n",
    "            task_params = uni_tasks_model[task_filter_idx]\n",
    "            # Check if the parameter column exists\n",
    "            if param_col_name_uni not in task_params:\n",
    "                 warnings.warn(f\"Task {task_filter_idx} for {model_name} missing key '{param_col_name_uni}'. Skipping.\", RuntimeWarning)\n",
    "            else:\n",
    "                 # Create signature and check against completed set\n",
    "                 try:\n",
    "                     task_sig = f\"N={int(task_params['N'])}_{param_name_uni}={task_params[param_col_name_uni]:.5f}_inst={int(task_params['instance'])}_trial={int(task_params['trial'])}\"\n",
    "                     if task_sig not in model_completed_tasks:\n",
    "                         model_tasks_to_run.append(task_params)\n",
    "                 except Exception: pass # Ignore tasks that cause signature errors\n",
    "            task_filter_idx += 1\n",
    "\n",
    "        print(f\"Prepared {len(uni_tasks_model)} total tasks for {model_name}. Need to run {len(model_tasks_to_run)} new tasks.\")\n",
    "\n",
    "        # Execute if needed\n",
    "        if model_tasks_to_run:\n",
    "            model_start_time = time.time(); model_futures = {}; pool_broken_flag_model = False\n",
    "            executor_instance_model = ProcessPoolExecutor(max_workers=workers)\n",
    "            try:\n",
    "                # Submit tasks for the current model\n",
    "                submit_idx_m = 0\n",
    "                while submit_idx_m < len(model_tasks_to_run):\n",
    "                    task_params = model_tasks_to_run[submit_idx_m]\n",
    "                    # Double check key exists before graph generation\n",
    "                    if param_col_name_uni not in task_params: submit_idx_m += 1; continue\n",
    "\n",
    "                    graph_gen_params_m = task_params.get('fixed_params', {}).copy()\n",
    "                    graph_gen_params_m[param_name_uni] = task_params[param_col_name_uni] # Add sweep param with base name\n",
    "                    G = generate_graph( task_params['model'], graph_gen_params_m, task_params['N'], task_params['graph_seed'] )\n",
    "\n",
    "                    if G is None or G.number_of_nodes() == 0: submit_idx_m += 1; continue # Skip failed graph gen\n",
    "\n",
    "                    # Submit task using the selected worker function\n",
    "                    future = executor_instance_model.submit(\n",
    "                        worker_func, # The globally selected worker\n",
    "                        graph=G, N=task_params['N'], instance_params=task_params, trial_seed=task_params['sim_seed'],\n",
    "                        rule_params_in=rule_params_base, max_steps=max_steps, conv_thresh=conv_thresh, state_dim=state_dim,\n",
    "                        calculate_energy=calculate_energy, store_energy_history=store_energy_history,\n",
    "                        energy_type=energy_type, metrics_to_calc=all_metrics, device=str(device)\n",
    "                    )\n",
    "                    model_futures[future] = task_params\n",
    "                    submit_idx_m += 1\n",
    "\n",
    "                # Collect results for the current model\n",
    "                pbar_model = tqdm(total=len(model_futures), desc=f\"Sweep ({model_name})\", mininterval=2.0, unit=\"task\")\n",
    "                log_freq_m = max(1, len(model_futures)//50); save_freq_m = max(20, len(model_futures)//10)\n",
    "                tasks_done_m_since_save = 0\n",
    "                with open(model_log_file, 'a') as f_log_model:\n",
    "                    future_get_idx = 0\n",
    "                    futures_list_m = list(model_futures.keys()) # Get a list of futures to iterate through\n",
    "                    while future_get_idx < len(futures_list_m):\n",
    "                        future = futures_list_m[future_get_idx]\n",
    "                        task_params = model_futures[future] # Get original params\n",
    "\n",
    "                        if pool_broken_flag_model: pbar_model.update(1); future_get_idx+=1; continue # Skip if pool broke\n",
    "\n",
    "                        try:\n",
    "                            result_dict = future.result(timeout=1200) # Use timeout\n",
    "                            if result_dict is not None and isinstance(result_dict, dict):\n",
    "                                 # Combine task params and result dict\n",
    "                                 full_result = {**task_params, **result_dict}\n",
    "                                 model_results_list.append(full_result); tasks_done_m_since_save += 1\n",
    "                                 # Log completion periodically\n",
    "                                 param_val_key_l = param_col_name_uni\n",
    "                                 is_log_step_m = (future_get_idx % log_freq_m == 0)\n",
    "                                 success_m = result_dict.get('error_message') is None\n",
    "                                 key_exists_m = param_val_key_l in task_params\n",
    "                                 if is_log_step_m and success_m and key_exists_m:\n",
    "                                     try:\n",
    "                                         task_sig = f\"N={int(task_params['N'])}_{param_name_uni}={task_params[param_val_key_l]:.5f}_inst={int(task_params['instance'])}_trial={int(task_params['trial'])}\"\n",
    "                                         f_log_model.write(f\"{task_sig}\\n\"); f_log_model.flush()\n",
    "                                     except Exception: pass # Ignore logging errors\n",
    "                        except Exception as e_get_m:\n",
    "                             error_str_m = str(e_get_m)\n",
    "                             is_broken_m = False\n",
    "                             if \"Broken\" in error_str_m or \"abruptly\" in error_str_m or \"shutdown\" in error_str_m: is_broken_m = True\n",
    "                             elif isinstance(e_get_m, TypeError) or isinstance(e_get_m, AttributeError): is_broken_m = True\n",
    "                             if is_broken_m:\n",
    "                                  print(f\"\\n‚ùå ERROR: Pool broke during {model_name} run. Exception: {type(e_get_m).__name__}: {e_get_m}\"); pool_broken_flag_model = True\n",
    "                             else: # Log other errors (e.g., Timeout)\n",
    "                                  warnings.warn(f\"Error getting result for {model_name} task {task_params}: {type(e_get_m).__name__}\", RuntimeWarning)\n",
    "                                  error_res_m = {**task_params, 'error_message': f\"Future failed: {type(e_get_m).__name__}\"}\n",
    "                                  model_results_list.append(error_res_m); tasks_done_m_since_save += 1\n",
    "\n",
    "\n",
    "                        finally:\n",
    "                             pbar_model.update(1)\n",
    "                             # *** CORRECTED INDENTATION START ***\n",
    "                             # Save partial results for this model if frequency is met\n",
    "                             should_save_m = tasks_done_m_since_save >= save_freq_m\n",
    "                             if should_save_m:\n",
    "                                 try:\n",
    "                                     with open(model_partial_results_file, 'wb') as f_p: pickle.dump(model_results_list, f_p)\n",
    "                                     tasks_done_m_since_save = 0 # Reset counter after successful save\n",
    "                                 except Exception as e_sp_m: warnings.warn(f\"Could not save partial results for {model_name}: {e_sp_m}\", RuntimeWarning)\n",
    "                             # *** CORRECTED INDENTATION END ***\n",
    "                             future_get_idx += 1 # Increment get loop index\n",
    "\n",
    "\n",
    "                except KeyboardInterrupt: print(f\"\\nInterrupted ({model_name}).\")\n",
    "                finally: pbar_model.close();\n",
    "\n",
    "            except Exception as main_e_model: print(f\"\\n‚ùå ERROR during {model_name} setup: {main_e_model}\"); traceback.print_exc(limit=2)\n",
    "            finally:\n",
    "                print(f\"Shutting down executor ({model_name})...\"); executor_instance_model.shutdown(wait=True, cancel_futures=True); print(\"Executor shut down.\")\n",
    "                # Attempt final save for the current model\n",
    "                try:\n",
    "                    with open(model_partial_results_file, 'wb') as f_p_final: pickle.dump(model_results_list, f_p_final)\n",
    "                except Exception as e_spf: warnings.warn(f\"Could not save final partial results for {model_name}: {e_spf}\", RuntimeWarning)\n",
    "\n",
    "            model_end_time = time.time()\n",
    "            print(f\"  ‚úÖ Sweep for {model_name} completed ({model_end_time - model_start_time:.1f}s).\")\n",
    "\n",
    "        # Add model results to the main list, avoiding duplicates\n",
    "        added_count = 0\n",
    "        if isinstance(model_results_list, list) and len(model_results_list) > 0:\n",
    "             # Rebuild existing signatures from the main list to prevent duplicates\n",
    "             existing_signatures = set();\n",
    "             if isinstance(all_universality_results_list, list) and len(all_universality_results_list) > 0:\n",
    "                 try:\n",
    "                     # Define keys needed to build a unique signature\n",
    "                     param_keys = ['model', 'N', 'instance', 'trial', param_col_name_uni]\n",
    "                     # Check if keys exist in the first item (assume consistent structure)\n",
    "                     if all(k in all_universality_results_list[0] for k in param_keys):\n",
    "                          sig_idx = 0\n",
    "                          while sig_idx < len(all_universality_results_list):\n",
    "                               res = all_universality_results_list[sig_idx]\n",
    "                               try: existing_signatures.add(tuple(res.get(k) for k in param_keys))\n",
    "                               except Exception: pass # Ignore signature errors\n",
    "                               sig_idx += 1\n",
    "                     else: warnings.warn(\"Could not build existing signatures - key mismatch.\", RuntimeWarning)\n",
    "                 except Exception as e_sig_build: warnings.warn(f\"Error building existing signatures: {e_sig_build}\", RuntimeWarning)\n",
    "\n",
    "             # Iterate through results from the current model and add if signature is new\n",
    "             new_result_idx = 0\n",
    "             while new_result_idx < len(model_results_list):\n",
    "                 res = model_results_list[new_result_idx]\n",
    "                 try:\n",
    "                      param_keys_check = ['model', 'N', 'instance', 'trial', param_col_name_uni]\n",
    "                      # Check if keys exist in current result\n",
    "                      if all(k in res for k in param_keys_check):\n",
    "                           sig_tuple_check = tuple(res.get(k) for k in param_keys_check)\n",
    "                           if sig_tuple_check not in existing_signatures:\n",
    "                                all_universality_results_list.append(res); existing_signatures.add(sig_tuple_check); added_count += 1\n",
    "                      else: warnings.warn(\"Result missing keys for signature check.\", RuntimeWarning)\n",
    "                 except Exception as e_add_res: warnings.warn(f\"Error processing result for adding to main list: {e_add_res}\", RuntimeWarning)\n",
    "                 new_result_idx += 1\n",
    "\n",
    "        print(f\"  Added {added_count} new results from {model_name} to combined list (Total: {len(all_universality_results_list)}).\")\n",
    "\n",
    "        # Save combined list incrementally after each model finishes\n",
    "        try:\n",
    "            with open(combined_pickle_file, 'wb') as f_comb_partial: pickle.dump(all_universality_results_list, f_comb_partial)\n",
    "        except Exception as e_sc_inc: warnings.warn(f\"Incremental save of combined results failed: {e_sc_inc}\", RuntimeWarning)\n",
    "\n",
    "        # Check if pool broke during this model's run\n",
    "        if pool_broken_flag_model:\n",
    "             print(f\"‚ùå Aborting universality sweeps because process pool failed during {model_name} run.\")\n",
    "             analysis_error_uni = True # Mark analysis as errored\n",
    "             break # Exit the outer while loop over models\n",
    "\n",
    "        model_idx_outer += 1 # Move to the next model\n",
    "\n",
    "# --- Final Combine and Save ---\n",
    "if not all_universality_results_list:\n",
    "    print(\"\\n‚ö†Ô∏è No universality results collected or loaded.\")\n",
    "elif not analysis_error_uni: # Only save if no pool break occurred\n",
    "    print(\"\\n--- Combining Universality Results ---\")\n",
    "    combined_df = pd.DataFrame(all_universality_results_list)\n",
    "    # Check for errors reported by workers across all models\n",
    "    if 'error_message' in combined_df.columns:\n",
    "         failed_run_count_comb = combined_df['error_message'].notna().sum()\n",
    "         if failed_run_count_comb > 0:\n",
    "              warnings.warn(f\"{failed_run_count_comb} total universality runs reported errors.\", RuntimeWarning)\n",
    "\n",
    "    # Save final combined results\n",
    "    try:\n",
    "        # Save metadata (without large vectors) to CSV\n",
    "        cols_to_save_uni = [col for col in combined_df.columns if col not in ['final_state_vector', 'state_history', 'avg_change_history', 'baseline_state_for_spread']]\n",
    "        combined_df[cols_to_save_uni].to_csv(combined_results_file, index=False)\n",
    "        print(f\"\\n‚úÖ Combined universality metadata ({combined_df.shape[0]}) saved to CSV: {combined_results_file}\")\n",
    "        # Save the full list (including vectors if present) to pickle\n",
    "        with open(combined_pickle_file, 'wb') as f_comb_final:\n",
    "             pickle.dump(all_universality_results_list, f_comb_final)\n",
    "        print(f\"\\n‚úÖ Combined universality full data saved to Pickle: {combined_pickle_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving final combined universality results: {e}\")\n",
    "        traceback.print_exc(limit=1)\n",
    "\n",
    "    # Store in global variable only if successfully processed\n",
    "    global_universality_results = combined_df\n",
    "\n",
    "else: # Case where analysis_error_uni is True (e.g., pool broke)\n",
    "     print(\"\\n--- Skipped Final Saving of Universality Results due to Sweep Errors ---\")\n",
    "     # Store partial results if available\n",
    "     global_universality_results = pd.DataFrame(all_universality_results_list)\n",
    "\n",
    "\n",
    "# Add a final check for the global variable\n",
    "if 'global_universality_results' in globals() and isinstance(global_universality_results, pd.DataFrame) and not global_universality_results.empty:\n",
    "     print(f\"\\n‚úÖ `global_universality_results` DataFrame created (Shape: {global_universality_results.shape}).\")\n",
    "else:\n",
    "     print(\"\\n‚ö†Ô∏è `global_universality_results` DataFrame is empty or not created.\")\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Cell 11: Universality testing sweeps completed or loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.1: Critical Point & Exponent Analysis (SBM Model - FSS on Chi with Optuna)\n",
    "# Description: Analyzes SBM universality results. Calculates Susceptibility (Chi).\n",
    "#              Uses Optuna to find the best FSS parameters (pc, gamma/nu, 1/nu) for Chi.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit, minimize # Keep minimize available\n",
    "import warnings\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "import optuna  # Import Optuna\n",
    "\n",
    "# --- Suppress Optuna INFO messages ---\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"\\n--- Cell 11.1: Critical Point & Exponent Analysis (SBM Model - FSS on Chi with Optuna) ---\")\n",
    "\n",
    "# --- Prerequisites & Configuration ---\n",
    "analysis_error_sbm = False\n",
    "# Check for config dictionary\n",
    "if 'config' not in globals() or not isinstance(config, dict):\n",
    "    print(\"‚ùå FATAL: Config dictionary missing. Run Cell 1.\"); analysis_error_sbm = True\n",
    "# Check for combined universality results DataFrame\n",
    "if 'global_universality_results' not in globals():\n",
    "    print(\"‚ùå FATAL: Combined universality DataFrame 'global_universality_results' missing. Run Cell 11.\")\n",
    "    analysis_error_sbm = True\n",
    "elif not isinstance(global_universality_results, pd.DataFrame):\n",
    "     print(\"‚ùå FATAL: 'global_universality_results' is not a Pandas DataFrame.\")\n",
    "     analysis_error_sbm = True\n",
    "elif global_universality_results.empty:\n",
    "    print(\"‚ùå FATAL: Combined universality DataFrame 'global_universality_results' is empty.\")\n",
    "    analysis_error_sbm = True\n",
    "elif 'SBM' not in global_universality_results['model'].unique():\n",
    "    # Check if SBM model data specifically is present\n",
    "    print(\"‚ùå FATAL: No 'SBM' model results found in combined universality DataFrame.\")\n",
    "    analysis_error_sbm = True\n",
    "\n",
    "# Load necessary config parameters if no error yet\n",
    "output_dir = None\n",
    "exp_name = None\n",
    "primary_metric_sbm = None\n",
    "system_sizes_sbm = None\n",
    "param_name_sbm = None # Specific parameter name for SBM sweep\n",
    "if not analysis_error_sbm:\n",
    "    try:\n",
    "        output_dir = config['OUTPUT_DIR']\n",
    "        exp_name = config['EXPERIMENT_NAME']\n",
    "        primary_metric_sbm = config.get('PRIMARY_ORDER_PARAMETER', 'variance_norm')  # Need M for moments\n",
    "        system_sizes_sbm = config.get('SYSTEM_SIZES', [])\n",
    "        # Dynamically find SBM sweep parameter column name\n",
    "        sbm_params_cfg = config.get('GRAPH_MODEL_PARAMS', {}).get('SBM', {})\n",
    "        sbm_sweep_key = next((k for k in sbm_params_cfg if k.endswith('_values')), None)\n",
    "        if sbm_sweep_key:\n",
    "             param_name_sbm = sbm_sweep_key.replace('_values', '_value') # e.g., 'p_intra_value'\n",
    "        else:\n",
    "             warnings.warn(\"Could not determine SBM sweep parameter name from config. Assuming 'p_intra_value'.\", RuntimeWarning)\n",
    "             param_name_sbm = 'p_intra_value'\n",
    "\n",
    "        if not system_sizes_sbm:\n",
    "            print(\"‚ùå FATAL: SYSTEM_SIZES list is empty in config.\")\n",
    "            analysis_error_sbm = True\n",
    "\n",
    "    except KeyError as e_key_sbm:\n",
    "        print(f\"‚ùå FATAL: Missing key '{e_key_sbm}' in config for SBM analysis.\")\n",
    "        analysis_error_sbm = True\n",
    "    except Exception as e_conf_sbm:\n",
    "        print(f\"‚ùå FATAL: Error loading config for SBM analysis: {e_conf_sbm}.\")\n",
    "        analysis_error_sbm = True\n",
    "\n",
    "# --- Initialize results dictionary for this cell ---\n",
    "global_optuna_fss_chi_sbm_results = {}\n",
    "\n",
    "# --- Filter and Diagnose SBM Data ---\n",
    "sbm_results_df = pd.DataFrame() # Initialize empty DataFrame\n",
    "if not analysis_error_sbm:\n",
    "    print(f\"\\n--- Step 11.1.1: Filtering and Diagnosing SBM Input Data ---\")\n",
    "    # Filter the combined DataFrame for SBM model results\n",
    "    sbm_results_df = global_universality_results[global_universality_results['model'] == 'SBM'].copy()\n",
    "\n",
    "    if sbm_results_df.empty:\n",
    "        analysis_error_sbm = True\n",
    "        print(\"‚ùå FATAL: SBM results DataFrame is empty after filtering.\")\n",
    "    else:\n",
    "        print(f\"  Filtered SBM DataFrame Shape: {sbm_results_df.shape}\")\n",
    "        # Check required columns for SBM analysis\n",
    "        required_cols_sbm = ['N', param_name_sbm, primary_metric_sbm, 'instance', 'trial']\n",
    "        missing_cols_sbm = []\n",
    "        col_idx = 0\n",
    "        while col_idx < len(required_cols_sbm):\n",
    "             col = required_cols_sbm[col_idx]\n",
    "             if col not in sbm_results_df.columns:\n",
    "                  missing_cols_sbm.append(col)\n",
    "             col_idx += 1\n",
    "\n",
    "        if len(missing_cols_sbm) > 0:\n",
    "            analysis_error_sbm = True\n",
    "            print(f\"‚ùå FATAL: SBM data missing required columns: {missing_cols_sbm}.\")\n",
    "        else:\n",
    "            print(f\"  Required columns found: {required_cols_sbm}\")\n",
    "            # Check unique system sizes (N) for SBM\n",
    "            unique_N_sbm = sbm_results_df['N'].unique()\n",
    "            print(f\"  Unique 'N' for SBM: {sorted(unique_N_sbm)}\")\n",
    "            if len(unique_N_sbm) < 2:\n",
    "                analysis_error_sbm = True\n",
    "                print(f\"‚ùå FATAL: Need >= 2 unique 'N' for SBM FSS. Found {len(unique_N_sbm)}.\")\n",
    "            else:\n",
    "                print(\"  Sufficient unique 'N' values for SBM FSS.\")\n",
    "                # Diagnose primary metric column\n",
    "                metric_col_sbm = sbm_results_df[primary_metric_sbm]\n",
    "                total_sbm = len(metric_col_sbm)\n",
    "                non_nan_sbm = metric_col_sbm.notna().sum()\n",
    "                nan_sbm = metric_col_sbm.isna().sum()\n",
    "                print(f\"  SBM Diagnostics for '{primary_metric_sbm}': Total={total_sbm}, Non-NaN={non_nan_sbm}, NaN={nan_sbm}\")\n",
    "                if non_nan_sbm == 0:\n",
    "                    analysis_error_sbm = True\n",
    "                    print(f\"‚ùå FATAL: SBM Column '{primary_metric_sbm}' contains only NaNs.\")\n",
    "                else:\n",
    "                    print(\"‚úÖ SBM Data seems valid for moment calculation.\")\n",
    "\n",
    "# --- Aggregate Susceptibility for SBM ---\n",
    "fss_chi_df_sbm = pd.DataFrame() # Initialize empty dataframe\n",
    "if not analysis_error_sbm:\n",
    "    print(f\"\\n--- Step 11.1.2: Aggregating SBM Susceptibility (œá) ---\")\n",
    "    try:\n",
    "        # Calculate variance of the primary order parameter (M)\n",
    "        # Convert to numeric, coercing errors\n",
    "        M_numeric_sbm = pd.to_numeric(sbm_results_df[primary_metric_sbm], errors='coerce')\n",
    "        # Group by N and the SBM parameter, then calculate variance\n",
    "        var_M_sbm = sbm_results_df.assign(M_numeric_sbm=M_numeric_sbm).groupby(['N', param_name_sbm], observed=True)['M_numeric_sbm'].var()\n",
    "\n",
    "        if var_M_sbm.isna().any():\n",
    "            nan_groups_sbm = var_M_sbm[var_M_sbm.isna()].index.tolist()\n",
    "            warnings.warn(f\"NaNs found in SBM Var(M) calc for {len(nan_groups_sbm)} groups. Dropping.\", RuntimeWarning)\n",
    "\n",
    "        # Calculate Susceptibility: œá = N * Var(M)\n",
    "        susceptibility_chi_agg_sbm = var_M_sbm.index.get_level_values('N') * var_M_sbm\n",
    "        # Combine into DataFrame and drop rows with NaN susceptibility\n",
    "        fss_chi_df_sbm = pd.DataFrame({'susceptibility_chi': susceptibility_chi_agg_sbm}).reset_index().dropna()\n",
    "\n",
    "        # Check validity of the aggregated DataFrame\n",
    "        if fss_chi_df_sbm.empty:\n",
    "             raise ValueError(\"SBM Chi DataFrame empty after aggregation/dropna.\")\n",
    "        if fss_chi_df_sbm['N'].nunique() < 2:\n",
    "            raise ValueError(f\"SBM Chi DataFrame has < 2 unique sizes ({fss_chi_df_sbm['N'].unique()}) after aggregation/dropna.\")\n",
    "\n",
    "        print(f\"  Aggregated SBM Susceptibility ready for FSS (Entries: {len(fss_chi_df_sbm)}).\")\n",
    "        # print(\"  Sample SBM aggregated data:\\n\", fss_chi_df_sbm.head())\n",
    "\n",
    "    except KeyError as e_agg_key_sbm:\n",
    "        print(f\"‚ùå Error aggregating SBM Chi: Missing column {e_agg_key_sbm}\")\n",
    "        analysis_error_sbm = True\n",
    "    except Exception as agg_chi_e_sbm:\n",
    "        print(f\"‚ùå Error aggregating SBM Chi: {agg_chi_e_sbm}\")\n",
    "        traceback.print_exc(limit=1)\n",
    "        analysis_error_sbm = True\n",
    "\n",
    "# --- FSS on SBM Susceptibility using Optuna ---\n",
    "if not analysis_error_sbm:\n",
    "    print(f\"\\n--- Step 11.1.3: FSS on SBM Susceptibility using Optuna ---\")\n",
    "    # Prepare data for Optuna objective function\n",
    "    try:\n",
    "        Ls_chi_sbm = fss_chi_df_sbm['N'].values.astype(np.float64)\n",
    "        ps_chi_sbm = fss_chi_df_sbm[param_name_sbm].values.astype(np.float64)  # Use SBM parameter column\n",
    "        Ms_chi_sbm = fss_chi_df_sbm['susceptibility_chi'].values.astype(np.float64)\n",
    "    except KeyError as e_fss_prep_sbm:\n",
    "         print(f\"‚ùå Error preparing SBM FSS data: Missing column {e_fss_prep_sbm}.\")\n",
    "         analysis_error_sbm = True\n",
    "    except Exception as e_fss_prep_other_sbm:\n",
    "         print(f\"‚ùå Error preparing SBM FSS data: {e_fss_prep_other_sbm}.\")\n",
    "         analysis_error_sbm = True\n",
    "\n",
    "\n",
    "    # Define Optuna Objective Function (reusable structure, uses SBM data here)\n",
    "    # This function should ideally be defined once globally or imported if identical\n",
    "    def objective_fss_chi_sbm(trial):\n",
    "        # Suggest parameters for SBM (adjust ranges based on SBM behavior if known)\n",
    "        pc = trial.suggest_float(\"pc\", 0.01, 0.5)  # SBM p_c likely > 0.01, maybe even higher\n",
    "        gamma_nu = trial.suggest_float(\"gamma_over_nu\", 0.1, 3.0) # Ratio gamma/nu\n",
    "        one_nu = trial.suggest_float(\"one_over_nu\", 0.1, 5.0) # Ratio 1/nu\n",
    "\n",
    "        # --- Calculate scaled variables & collapse error using SBM data ---\n",
    "        scaled_x = (ps_chi_sbm - pc) * (Ls_chi_sbm ** one_nu)\n",
    "        scaled_y = Ms_chi_sbm * (Ls_chi_sbm ** (-gamma_nu))\n",
    "\n",
    "        # Sort by scaled_x for binning\n",
    "        sorted_indices = np.argsort(scaled_x)\n",
    "        scaled_x_sorted = scaled_x[sorted_indices]\n",
    "        scaled_y_sorted = scaled_y[sorted_indices]\n",
    "\n",
    "        # --- Calculate Collapse Error using Binning (same logic as Cell 9) ---\n",
    "        total_error = 0.0; num_bins = 20; # Use same binning approach\n",
    "        try:\n",
    "            valid_indices = np.isfinite(scaled_x_sorted) & np.isfinite(scaled_y_sorted)\n",
    "            if not np.any(valid_indices): return np.inf\n",
    "\n",
    "            scaled_x_finite = scaled_x_sorted[valid_indices]\n",
    "            scaled_y_finite = scaled_y_sorted[valid_indices]\n",
    "            num_valid_points = len(scaled_x_finite)\n",
    "            if num_valid_points < num_bins: num_bins = max(1, num_valid_points // 2)\n",
    "\n",
    "            min_x = np.min(scaled_x_finite); max_x = np.max(scaled_x_finite)\n",
    "            if abs(min_x - max_x) < 1e-9: return np.var(scaled_y_finite) if num_valid_points > 1 else 0.0\n",
    "\n",
    "            bins = np.linspace(min_x, max_x, num_bins + 1)\n",
    "            bin_indices = np.digitize(scaled_x_finite, bins)\n",
    "            non_empty_bin_count = 0\n",
    "            bin_idx = 1\n",
    "            while bin_idx <= num_bins:\n",
    "                y_in_bin = scaled_y_finite[bin_indices == bin_idx]\n",
    "                if len(y_in_bin) > 1:\n",
    "                    total_error += np.var(y_in_bin); non_empty_bin_count += 1\n",
    "                bin_idx += 1\n",
    "\n",
    "            if non_empty_bin_count > 0: return total_error / non_empty_bin_count\n",
    "            else: return np.inf\n",
    "        except Exception: return np.inf # Return high error on failure\n",
    "\n",
    "    # --- Run Optuna Study for SBM ---\n",
    "    n_optuna_trials_sbm = 100 # Number of trials for SBM optimization\n",
    "    print(f\"  Running Optuna study ({n_optuna_trials_sbm} trials) for SBM Chi...\")\n",
    "    study_chi_sbm = optuna.create_study(direction='minimize')\n",
    "    optimization_success_sbm = False\n",
    "    try:\n",
    "        study_chi_sbm.optimize(objective_fss_chi_sbm, n_trials=n_optuna_trials_sbm, show_progress_bar=True)\n",
    "        optimization_success_sbm = True\n",
    "    except Exception as optuna_err_sbm:\n",
    "        print(f\"‚ùå Error during Optuna SBM optimization: {optuna_err_sbm}\")\n",
    "        traceback.print_exc(limit=1)\n",
    "        global_optuna_fss_chi_sbm_results = {'success': False} # Store failure\n",
    "        analysis_error_sbm = True # Mark analysis as failed\n",
    "\n",
    "    # --- Process and Store Best SBM Results ---\n",
    "    if optimization_success_sbm:\n",
    "        if study_chi_sbm.best_trial:\n",
    "            bp_sbm = study_chi_sbm.best_params # Best parameters found\n",
    "            bv_sbm = study_chi_sbm.best_value   # Best objective value\n",
    "\n",
    "            pc_opt_sbm = bp_sbm['pc']\n",
    "            gamma_nu_opt_sbm = bp_sbm['gamma_over_nu']\n",
    "            one_nu_opt_sbm = bp_sbm['one_over_nu']\n",
    "\n",
    "            # Calculate original exponents\n",
    "            nu_opt_sbm = np.nan; gamma_opt_sbm = np.nan\n",
    "            if abs(one_nu_opt_sbm) > 1e-6:\n",
    "                nu_opt_sbm = 1.0 / one_nu_opt_sbm\n",
    "                gamma_opt_sbm = gamma_nu_opt_sbm * nu_opt_sbm\n",
    "            else: warnings.warn(\"SBM Optuna result 1/nu too close to zero.\", RuntimeWarning)\n",
    "\n",
    "            # Store results\n",
    "            global_optuna_fss_chi_sbm_results = {\n",
    "                'pc': pc_opt_sbm,\n",
    "                'gamma': gamma_opt_sbm,\n",
    "                'nu': nu_opt_sbm,\n",
    "                'gamma_over_nu': gamma_nu_opt_sbm,\n",
    "                'one_over_nu': one_nu_opt_sbm,\n",
    "                'success': True,\n",
    "                'objective': bv_sbm\n",
    "            }\n",
    "            print(\"\\n  ‚úÖ Optuna FSS Successful for SBM Chi:\")\n",
    "            print(f\"     Best Objective: {bv_sbm:.4e}\")\n",
    "            print(f\"     p_c(SBM) ‚âà {pc_opt_sbm:.6f}\")\n",
    "            print(f\"     Œ≥(SBM)   ‚âà {format_metric(gamma_opt_sbm, '%.4f')}\") # Use helper\n",
    "            print(f\"     ŒΩ(SBM)   ‚âà {format_metric(nu_opt_sbm, '%.4f')}\")   # Use helper\n",
    "        else:\n",
    "            print(\"  ‚ùå Optuna SBM study finished but reported no best trial.\")\n",
    "            global_optuna_fss_chi_sbm_results = {'success': False}\n",
    "\n",
    "    # --- Plot SBM FSS Collapse ---\n",
    "    if global_optuna_fss_chi_sbm_results.get('success', False):\n",
    "        print(\"  Generating FSS data collapse plot for SBM Chi...\")\n",
    "        # Retrieve optimal parameters for plotting\n",
    "        pc_sbm_plot = global_optuna_fss_chi_sbm_results['pc']\n",
    "        gamma_nu_sbm_plot = global_optuna_fss_chi_sbm_results['gamma_over_nu']\n",
    "        one_nu_sbm_plot = global_optuna_fss_chi_sbm_results['one_over_nu']\n",
    "        nu_val_sbm_plot = global_optuna_fss_chi_sbm_results['nu']\n",
    "\n",
    "        # Recalculate scaled variables\n",
    "        scaled_x_sbm = (ps_chi_sbm - pc_sbm_plot) * (Ls_chi_sbm ** one_nu_sbm_plot)\n",
    "        scaled_y_sbm = Ms_chi_sbm * (Ls_chi_sbm ** (-gamma_nu_sbm_plot))\n",
    "\n",
    "        # Create plot\n",
    "        fig_fss_sbm, ax_fss_sbm = plt.subplots(figsize=(8, 6))\n",
    "        unique_Ls_sbm_plot = sorted(np.unique(Ls_chi_sbm))\n",
    "        colors_sbm = plt.cm.viridis(np.linspace(0, 1, len(unique_Ls_sbm_plot)))\n",
    "\n",
    "        # Plot data for each system size\n",
    "        l_idx = 0\n",
    "        while l_idx < len(unique_Ls_sbm_plot):\n",
    "             L = unique_Ls_sbm_plot[l_idx]\n",
    "             mask = Ls_chi_sbm == L\n",
    "             ax_fss_sbm.scatter(scaled_x_sbm[mask], scaled_y_sbm[mask],\n",
    "                                label=f'N={int(L)}', color=colors_sbm[l_idx], alpha=0.7, s=20)\n",
    "             l_idx += 1\n",
    "\n",
    "        # Configure plot labels and title\n",
    "        # Use param_name_sbm (e.g., p_intra) in label\n",
    "        sbm_param_base_name = param_name_sbm.replace('_value', '')\n",
    "        xlabel_sbm = f'$({sbm_param_base_name} - p_c) N^{{1/\\\\nu}}$ (p$_c$‚âà{pc_sbm_plot:.4f}, ŒΩ‚âà{format_metric(nu_val_sbm_plot,\"%.3f\")})'\n",
    "        ylabel_sbm = f'$\\\\chi \\\\times N^{{-\\\\gamma/\\\\nu}}$ (Œ≥/ŒΩ‚âà{gamma_nu_sbm_plot:.3f})'\n",
    "        ax_fss_sbm.set_xlabel(xlabel_sbm)\n",
    "        ax_fss_sbm.set_ylabel(ylabel_sbm)\n",
    "        ax_fss_sbm.set_title(f'FSS Collapse for Susceptibility œá (SBM - Optuna)')\n",
    "        ax_fss_sbm.grid(True, linestyle=':')\n",
    "        ax_fss_sbm.legend(title='N')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save plot\n",
    "        fss_sbm_plot_path = os.path.join(output_dir, f\"{exp_name}_SBM_Susceptibility_FSS_collapse_OPTUNA.png\")\n",
    "        try:\n",
    "            plt.savefig(fss_sbm_plot_path, dpi=150)\n",
    "            print(f\"  ‚úÖ SBM FSS Chi Collapse plot saved to: {fss_sbm_plot_path}\")\n",
    "        except Exception as e_save_sbm:\n",
    "            print(f\"  ‚ùå Error saving SBM FSS plot: {e_save_sbm}\")\n",
    "        plt.close(fig_fss_sbm) # Close plot\n",
    "    else:\n",
    "        print(\"  Skipping SBM FSS Chi collapse plot due to optimization failure.\")\n",
    "\n",
    "# --- Optional: Estimate pc from peak (might be less accurate) ---\n",
    "# This block was present in the user input but seems to have a NameError.\n",
    "# Fixing the reference from df_plot to fss_chi_df_sbm\n",
    "if not analysis_error_sbm and not fss_chi_df_sbm.empty:\n",
    "    print(\"\\n--- Estimating p_c from SBM Susceptibility Peak (Largest N) ---\")\n",
    "    pc_chi_peak_sbm = np.nan\n",
    "    try:\n",
    "        largest_N_sbm = fss_chi_df_sbm['N'].max()\n",
    "        largest_N_data_chi_sbm = fss_chi_df_sbm[fss_chi_df_sbm['N'] == largest_N_sbm]\n",
    "        if not largest_N_data_chi_sbm.empty:\n",
    "            # Find index of the maximum susceptibility value for the largest N\n",
    "            peak_idx = largest_N_data_chi_sbm['susceptibility_chi'].idxmax()\n",
    "            # Check if index is valid before using .loc\n",
    "            if pd.notna(peak_idx) and peak_idx in largest_N_data_chi_sbm.index:\n",
    "                # Get the corresponding parameter value (p_intra_value)\n",
    "                pc_chi_peak_sbm = largest_N_data_chi_sbm.loc[peak_idx, param_name_sbm]\n",
    "                print(f\"    p_c(SBM) estimate from œá peak (N={largest_N_sbm}): {pc_chi_peak_sbm:.6f}\")\n",
    "            else:\n",
    "                print(f\"    Could not find valid Chi peak index for SBM (N={largest_N_sbm}).\")\n",
    "        else:\n",
    "            print(f\"    No SBM data found for N={largest_N_sbm} to estimate Chi peak.\")\n",
    "    except KeyError as e_peak_key:\n",
    "         print(f\"    Could not estimate from Chi peak: Missing column {e_peak_key}\")\n",
    "    except Exception as e_chi_sbm:\n",
    "        print(f\"    Could not estimate from Chi peak: {e_chi_sbm}\")\n",
    "\n",
    "\n",
    "# Final message if analysis was skipped\n",
    "elif analysis_error_sbm:\n",
    "     print(\"\\n‚ùå Skipping SBM Analysis Steps due to configuration or diagnostic errors.\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 11.1: SBM Analysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f35ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.2: Critical Point & Exponent Analysis (RGG Model - FSS on Chi with Optuna)\n",
    "# Description: Analyzes RGG universality results. Calculates Susceptibility (Chi).\n",
    "#              Uses Optuna to find the best FSS parameters (rc, gamma/nu, 1/nu) for Chi.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit, minimize # Keep minimize available\n",
    "import warnings\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "import optuna  # Import Optuna\n",
    "\n",
    "# --- Suppress Optuna INFO messages ---\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"\\n--- Cell 11.2: Critical Point & Exponent Analysis (RGG Model - FSS on Chi with Optuna) ---\")\n",
    "\n",
    "# --- Prerequisites & Configuration ---\n",
    "analysis_error_rgg = False\n",
    "# Check for config dictionary\n",
    "if 'config' not in globals() or not isinstance(config, dict):\n",
    "    print(\"‚ùå FATAL: Config dictionary missing. Run Cell 1.\"); analysis_error_rgg = True\n",
    "# Check for combined universality results DataFrame\n",
    "if 'global_universality_results' not in globals():\n",
    "    print(\"‚ùå FATAL: Combined universality DataFrame 'global_universality_results' missing. Run Cell 11.\")\n",
    "    analysis_error_rgg = True\n",
    "elif not isinstance(global_universality_results, pd.DataFrame):\n",
    "     print(\"‚ùå FATAL: 'global_universality_results' is not a Pandas DataFrame.\")\n",
    "     analysis_error_rgg = True\n",
    "elif global_universality_results.empty:\n",
    "    print(\"‚ùå FATAL: Combined universality DataFrame 'global_universality_results' is empty.\")\n",
    "    analysis_error_rgg = True\n",
    "elif 'RGG' not in global_universality_results['model'].unique():\n",
    "    # Check if RGG model data specifically is present\n",
    "    print(\"‚ùå FATAL: No 'RGG' model results found in combined universality DataFrame.\")\n",
    "    analysis_error_rgg = True\n",
    "\n",
    "# Load necessary config parameters if no error yet\n",
    "output_dir = None\n",
    "exp_name = None\n",
    "primary_metric_rgg = None\n",
    "system_sizes_rgg = None\n",
    "param_name_rgg = None # Specific parameter name for RGG sweep\n",
    "if not analysis_error_rgg:\n",
    "    try:\n",
    "        output_dir = config['OUTPUT_DIR']\n",
    "        exp_name = config['EXPERIMENT_NAME']\n",
    "        primary_metric_rgg = config.get('PRIMARY_ORDER_PARAMETER', 'variance_norm')\n",
    "        system_sizes_rgg = config.get('SYSTEM_SIZES', [])\n",
    "        # Dynamically find RGG sweep parameter column name\n",
    "        rgg_params_cfg = config.get('GRAPH_MODEL_PARAMS', {}).get('RGG', {})\n",
    "        rgg_sweep_key = next((k for k in rgg_params_cfg if k.endswith('_values')), None)\n",
    "        if rgg_sweep_key:\n",
    "             param_name_rgg = rgg_sweep_key.replace('_values', '_value') # e.g., 'radius_value'\n",
    "        else:\n",
    "             warnings.warn(\"Could not determine RGG sweep parameter name from config. Assuming 'radius_value'.\", RuntimeWarning)\n",
    "             param_name_rgg = 'radius_value'\n",
    "\n",
    "        if not system_sizes_rgg:\n",
    "            print(\"‚ùå FATAL: SYSTEM_SIZES list is empty in config.\")\n",
    "            analysis_error_rgg = True\n",
    "\n",
    "    except KeyError as e_key_rgg:\n",
    "        print(f\"‚ùå FATAL: Missing key '{e_key_rgg}' in config for RGG analysis.\")\n",
    "        analysis_error_rgg = True\n",
    "    except Exception as e_conf_rgg:\n",
    "        print(f\"‚ùå FATAL: Error loading config for RGG analysis: {e_conf_rgg}.\")\n",
    "        analysis_error_rgg = True\n",
    "\n",
    "# --- Initialize results dictionary for this cell ---\n",
    "global_optuna_fss_chi_rgg_results = {}\n",
    "\n",
    "# --- Filter and Diagnose RGG Data ---\n",
    "rgg_results_df = pd.DataFrame() # Initialize empty DataFrame\n",
    "if not analysis_error_rgg:\n",
    "    print(f\"\\n--- Step 11.2.1: Filtering and Diagnosing RGG Input Data ---\")\n",
    "    # Filter the combined DataFrame for RGG model results\n",
    "    rgg_results_df = global_universality_results[global_universality_results['model'] == 'RGG'].copy()\n",
    "\n",
    "    if rgg_results_df.empty:\n",
    "        analysis_error_rgg = True\n",
    "        print(\"‚ùå FATAL: RGG results DataFrame is empty after filtering.\")\n",
    "    else:\n",
    "        print(f\"  Filtered RGG DataFrame Shape: {rgg_results_df.shape}\")\n",
    "        # Check required columns for RGG analysis\n",
    "        required_cols_rgg = ['N', param_name_rgg, primary_metric_rgg, 'instance', 'trial']\n",
    "        missing_cols_rgg = []\n",
    "        col_idx = 0\n",
    "        while col_idx < len(required_cols_rgg):\n",
    "             col = required_cols_rgg[col_idx]\n",
    "             if col not in rgg_results_df.columns:\n",
    "                  missing_cols_rgg.append(col)\n",
    "             col_idx += 1\n",
    "\n",
    "        if len(missing_cols_rgg) > 0:\n",
    "            analysis_error_rgg = True\n",
    "            print(f\"‚ùå FATAL: RGG data missing required columns: {missing_cols_rgg}.\")\n",
    "        else:\n",
    "            print(f\"  Required columns found: {required_cols_rgg}\")\n",
    "            # Check unique system sizes (N) for RGG\n",
    "            unique_N_rgg = rgg_results_df['N'].unique()\n",
    "            print(f\"  Unique 'N' for RGG: {sorted(unique_N_rgg)}\")\n",
    "            if len(unique_N_rgg) < 2:\n",
    "                analysis_error_rgg = True\n",
    "                print(f\"‚ùå FATAL: Need >= 2 unique 'N' for RGG FSS. Found {len(unique_N_rgg)}.\")\n",
    "            else:\n",
    "                print(\"  Sufficient unique 'N' values for RGG FSS.\")\n",
    "                # Diagnose primary metric column\n",
    "                metric_col_rgg = rgg_results_df[primary_metric_rgg]\n",
    "                total_rgg = len(metric_col_rgg)\n",
    "                non_nan_rgg = metric_col_rgg.notna().sum()\n",
    "                nan_rgg = metric_col_rgg.isna().sum()\n",
    "                print(f\"  RGG Diagnostics for '{primary_metric_rgg}': Total={total_rgg}, Non-NaN={non_nan_rgg}, NaN={nan_rgg}\")\n",
    "                if non_nan_rgg == 0:\n",
    "                    analysis_error_rgg = True\n",
    "                    print(f\"‚ùå FATAL: RGG Column '{primary_metric_rgg}' contains only NaNs.\")\n",
    "                else:\n",
    "                    print(\"‚úÖ RGG Data seems valid for moment calculation.\")\n",
    "\n",
    "# --- Aggregate Susceptibility for RGG ---\n",
    "fss_chi_df_rgg = pd.DataFrame() # Initialize empty dataframe\n",
    "if not analysis_error_rgg:\n",
    "    print(f\"\\n--- Step 11.2.2: Aggregating RGG Susceptibility (œá) ---\")\n",
    "    try:\n",
    "        # Calculate variance of the primary order parameter (M)\n",
    "        M_numeric_rgg = pd.to_numeric(rgg_results_df[primary_metric_rgg], errors='coerce')\n",
    "        var_M_rgg = rgg_results_df.assign(M_numeric_rgg=M_numeric_rgg).groupby(['N', param_name_rgg], observed=True)['M_numeric_rgg'].var()\n",
    "\n",
    "        if var_M_rgg.isna().any():\n",
    "            nan_groups_rgg = var_M_rgg[var_M_rgg.isna()].index.tolist()\n",
    "            warnings.warn(f\"NaNs found in RGG Var(M) calc for {len(nan_groups_rgg)} groups. Dropping.\", RuntimeWarning)\n",
    "\n",
    "        # Calculate Susceptibility: œá = N * Var(M)\n",
    "        susceptibility_chi_agg_rgg = var_M_rgg.index.get_level_values('N') * var_M_rgg\n",
    "        # Combine into DataFrame and drop rows with NaN susceptibility\n",
    "        fss_chi_df_rgg = pd.DataFrame({'susceptibility_chi': susceptibility_chi_agg_rgg}).reset_index().dropna()\n",
    "\n",
    "        # Check validity of the aggregated DataFrame\n",
    "        if fss_chi_df_rgg.empty:\n",
    "             raise ValueError(\"RGG Chi DataFrame empty after aggregation/dropna.\")\n",
    "        if fss_chi_df_rgg['N'].nunique() < 2:\n",
    "            raise ValueError(f\"RGG Chi DataFrame has < 2 unique sizes ({fss_chi_df_rgg['N'].unique()}) after aggregation/dropna.\")\n",
    "\n",
    "        print(f\"  Aggregated RGG Susceptibility ready for FSS (Entries: {len(fss_chi_df_rgg)}).\")\n",
    "        # print(\"  Sample RGG aggregated data:\\n\", fss_chi_df_rgg.head())\n",
    "\n",
    "    except KeyError as e_agg_key_rgg:\n",
    "        print(f\"‚ùå Error aggregating RGG Chi: Missing column {e_agg_key_rgg}\")\n",
    "        analysis_error_rgg = True\n",
    "    except Exception as agg_chi_e_rgg:\n",
    "        print(f\"‚ùå Error aggregating RGG Chi: {agg_chi_e_rgg}\")\n",
    "        traceback.print_exc(limit=1)\n",
    "        analysis_error_rgg = True\n",
    "\n",
    "# --- FSS on RGG Susceptibility using Optuna ---\n",
    "if not analysis_error_rgg:\n",
    "    print(f\"\\n--- Step 11.2.3: FSS on RGG Susceptibility using Optuna ---\")\n",
    "    # Prepare data for Optuna objective function\n",
    "    try:\n",
    "        Ls_chi_rgg = fss_chi_df_rgg['N'].values.astype(np.float64)\n",
    "        # Use RGG parameter column (e.g., radius_value)\n",
    "        ps_chi_rgg = fss_chi_df_rgg[param_name_rgg].values.astype(np.float64)\n",
    "        Ms_chi_rgg = fss_chi_df_rgg['susceptibility_chi'].values.astype(np.float64)\n",
    "    except KeyError as e_fss_prep_rgg:\n",
    "         print(f\"‚ùå Error preparing RGG FSS data: Missing column {e_fss_prep_rgg}.\")\n",
    "         analysis_error_rgg = True\n",
    "    except Exception as e_fss_prep_other_rgg:\n",
    "         print(f\"‚ùå Error preparing RGG FSS data: {e_fss_prep_other_rgg}.\")\n",
    "         analysis_error_rgg = True\n",
    "\n",
    "    # Define Optuna Objective Function (reusable structure, uses RGG data here)\n",
    "    def objective_fss_chi_rgg(trial):\n",
    "        # Suggest parameters for RGG (critical radius 'rc')\n",
    "        # Adjust range based on sweep range and expected rc location\n",
    "        pc = trial.suggest_float(\"rc\", 0.05, 0.5) # 'rc' is the critical radius\n",
    "        gamma_nu = trial.suggest_float(\"gamma_over_nu\", 0.1, 3.0)\n",
    "        one_nu = trial.suggest_float(\"one_over_nu\", 0.1, 5.0)\n",
    "\n",
    "        # --- Calculate scaled variables & collapse error using RGG data ---\n",
    "        scaled_x = (ps_chi_rgg - pc) * (Ls_chi_rgg ** one_nu) # ps_chi_rgg holds radius values here\n",
    "        scaled_y = Ms_chi_rgg * (Ls_chi_rgg ** (-gamma_nu))\n",
    "\n",
    "        # Sort by scaled_x for binning\n",
    "        sorted_indices = np.argsort(scaled_x)\n",
    "        scaled_x_sorted = scaled_x[sorted_indices]\n",
    "        scaled_y_sorted = scaled_y[sorted_indices]\n",
    "\n",
    "        # --- Calculate Collapse Error using Binning (same logic as Cell 9) ---\n",
    "        total_error = 0.0; num_bins = 20;\n",
    "        try:\n",
    "            valid_indices = np.isfinite(scaled_x_sorted) & np.isfinite(scaled_y_sorted)\n",
    "            if not np.any(valid_indices): return np.inf\n",
    "\n",
    "            scaled_x_finite = scaled_x_sorted[valid_indices]\n",
    "            scaled_y_finite = scaled_y_sorted[valid_indices]\n",
    "            num_valid_points = len(scaled_x_finite)\n",
    "            if num_valid_points < num_bins: num_bins = max(1, num_valid_points // 2)\n",
    "\n",
    "            min_x = np.min(scaled_x_finite); max_x = np.max(scaled_x_finite)\n",
    "            if abs(min_x - max_x) < 1e-9: return np.var(scaled_y_finite) if num_valid_points > 1 else 0.0\n",
    "\n",
    "            bins = np.linspace(min_x, max_x, num_bins + 1)\n",
    "            bin_indices = np.digitize(scaled_x_finite, bins)\n",
    "            non_empty_bin_count = 0\n",
    "            bin_idx = 1\n",
    "            while bin_idx <= num_bins:\n",
    "                y_in_bin = scaled_y_finite[bin_indices == bin_idx]\n",
    "                if len(y_in_bin) > 1:\n",
    "                    total_error += np.var(y_in_bin); non_empty_bin_count += 1\n",
    "                bin_idx += 1\n",
    "\n",
    "            if non_empty_bin_count > 0: return total_error / non_empty_bin_count\n",
    "            else: return np.inf\n",
    "        except Exception: return np.inf\n",
    "\n",
    "    # --- Run Optuna Study for RGG ---\n",
    "    n_optuna_trials_rgg = 100 # Number of trials for RGG optimization\n",
    "    print(f\"  Running Optuna study ({n_optuna_trials_rgg} trials) for RGG Chi...\")\n",
    "    study_chi_rgg = optuna.create_study(direction='minimize')\n",
    "    optimization_success_rgg = False\n",
    "    try:\n",
    "        study_chi_rgg.optimize(objective_fss_chi_rgg, n_trials=n_optuna_trials_rgg, show_progress_bar=True)\n",
    "        optimization_success_rgg = True\n",
    "    except Exception as optuna_err_rgg:\n",
    "        print(f\"‚ùå Error during Optuna RGG optimization: {optuna_err_rgg}\")\n",
    "        traceback.print_exc(limit=1)\n",
    "        global_optuna_fss_chi_rgg_results = {'success': False} # Store failure\n",
    "        analysis_error_rgg = True # Mark analysis as failed\n",
    "\n",
    "    # --- Process and Store Best RGG Results ---\n",
    "    if optimization_success_rgg:\n",
    "        if study_chi_rgg.best_trial:\n",
    "            bp_rgg = study_chi_rgg.best_params # Best parameters found\n",
    "            bv_rgg = study_chi_rgg.best_value   # Best objective value\n",
    "\n",
    "            pc_opt_rgg = bp_rgg['rc'] # Critical radius\n",
    "            gamma_nu_opt_rgg = bp_rgg['gamma_over_nu']\n",
    "            one_nu_opt_rgg = bp_rgg['one_over_nu']\n",
    "\n",
    "            # Calculate original exponents\n",
    "            nu_opt_rgg = np.nan; gamma_opt_rgg = np.nan\n",
    "            if abs(one_nu_opt_rgg) > 1e-6:\n",
    "                nu_opt_rgg = 1.0 / one_nu_opt_rgg\n",
    "                gamma_opt_rgg = gamma_nu_opt_rgg * nu_opt_rgg\n",
    "            else: warnings.warn(\"RGG Optuna result 1/nu too close to zero.\", RuntimeWarning)\n",
    "\n",
    "            # Store results\n",
    "            global_optuna_fss_chi_rgg_results = {\n",
    "                'pc': pc_opt_rgg, # Storing critical radius under 'pc' key for consistency\n",
    "                'gamma': gamma_opt_rgg,\n",
    "                'nu': nu_opt_rgg,\n",
    "                'gamma_over_nu': gamma_nu_opt_rgg,\n",
    "                'one_over_nu': one_nu_opt_rgg,\n",
    "                'success': True,\n",
    "                'objective': bv_rgg\n",
    "            }\n",
    "            # Use helper from Cell 9 if available, otherwise simple format\n",
    "            if 'format_metric' not in globals(): format_metric = lambda v,f: f\"%.{f[-2]}f\" % v if pd.notna(v) else \"N/A\"\n",
    "\n",
    "            print(\"\\n  ‚úÖ Optuna FSS Successful for RGG Chi:\")\n",
    "            print(f\"     Best Objective: {bv_rgg:.4e}\")\n",
    "            print(f\"     r_c(RGG) ‚âà {pc_opt_rgg:.6f}\") # Use r_c in printout\n",
    "            print(f\"     Œ≥(RGG)   ‚âà {format_metric(gamma_opt_rgg, '%.4f')}\")\n",
    "            print(f\"     ŒΩ(RGG)   ‚âà {format_metric(nu_opt_rgg, '%.4f')}\")\n",
    "        else:\n",
    "            print(\"  ‚ùå Optuna RGG study finished but reported no best trial.\")\n",
    "            global_optuna_fss_chi_rgg_results = {'success': False}\n",
    "\n",
    "    # --- Plot RGG FSS Collapse ---\n",
    "    if global_optuna_fss_chi_rgg_results.get('success', False):\n",
    "        print(\"  Generating FSS data collapse plot for RGG Chi...\")\n",
    "        # Retrieve optimal parameters for plotting\n",
    "        pc_rgg_plot = global_optuna_fss_chi_rgg_results['pc'] # Critical radius\n",
    "        gamma_nu_rgg_plot = global_optuna_fss_chi_rgg_results['gamma_over_nu']\n",
    "        one_nu_rgg_plot = global_optuna_fss_chi_rgg_results['one_over_nu']\n",
    "        nu_val_rgg_plot = global_optuna_fss_chi_rgg_results['nu']\n",
    "\n",
    "        # Recalculate scaled variables using radius values (ps_chi_rgg)\n",
    "        scaled_x_rgg = (ps_chi_rgg - pc_rgg_plot) * (Ls_chi_rgg ** one_nu_rgg_plot)\n",
    "        scaled_y_rgg = Ms_chi_rgg * (Ls_chi_rgg ** (-gamma_nu_rgg_plot))\n",
    "\n",
    "        # Create plot\n",
    "        fig_fss_rgg, ax_fss_rgg = plt.subplots(figsize=(8, 6))\n",
    "        unique_Ls_rgg_plot = sorted(np.unique(Ls_chi_rgg))\n",
    "        colors_rgg = plt.cm.viridis(np.linspace(0, 1, len(unique_Ls_rgg_plot)))\n",
    "\n",
    "        # Plot data for each system size\n",
    "        l_idx = 0\n",
    "        while l_idx < len(unique_Ls_rgg_plot):\n",
    "             L = unique_Ls_rgg_plot[l_idx]\n",
    "             mask = Ls_chi_rgg == L\n",
    "             ax_fss_rgg.scatter(scaled_x_rgg[mask], scaled_y_rgg[mask],\n",
    "                                label=f'N={int(L)}', color=colors_rgg[l_idx], alpha=0.7, s=20)\n",
    "             l_idx += 1\n",
    "\n",
    "        # Configure plot labels and title using 'r' for radius\n",
    "        rgg_param_base_name = param_name_rgg.replace('_value', '') # Should be 'radius'\n",
    "        xlabel_rgg = f'$({rgg_param_base_name} - r_c) N^{{1/\\\\nu}}$ (r$_c$‚âà{pc_rgg_plot:.4f}, ŒΩ‚âà{format_metric(nu_val_rgg_plot,\"%.3f\")})'\n",
    "        ylabel_rgg = f'$\\\\chi \\\\times N^{{-\\\\gamma/\\\\nu}}$ (Œ≥/ŒΩ‚âà{gamma_nu_rgg_plot:.3f})'\n",
    "        ax_fss_rgg.set_xlabel(xlabel_rgg)\n",
    "        ax_fss_rgg.set_ylabel(ylabel_rgg)\n",
    "        ax_fss_rgg.set_title(f'FSS Collapse for Susceptibility œá (RGG - Optuna)')\n",
    "        ax_fss_rgg.grid(True, linestyle=':')\n",
    "        ax_fss_rgg.legend(title='N')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save plot\n",
    "        fss_rgg_plot_path = os.path.join(output_dir, f\"{exp_name}_RGG_Susceptibility_FSS_collapse_OPTUNA.png\")\n",
    "        try:\n",
    "            plt.savefig(fss_rgg_plot_path, dpi=150)\n",
    "            print(f\"  ‚úÖ RGG FSS Chi Collapse plot saved to: {fss_rgg_plot_path}\")\n",
    "        except Exception as e_save_rgg:\n",
    "            print(f\"  ‚ùå Error saving RGG FSS plot: {e_save_rgg}\")\n",
    "        plt.close(fig_fss_rgg) # Close plot\n",
    "    else:\n",
    "        print(\"  Skipping RGG FSS Chi collapse plot due to optimization failure.\")\n",
    "\n",
    "# --- Optional: Estimate rc from peak ---\n",
    "# Fixing NameError as in SBM cell\n",
    "if not analysis_error_rgg and not fss_chi_df_rgg.empty:\n",
    "    print(\"\\n--- Estimating r_c from RGG Susceptibility Peak (Largest N) ---\")\n",
    "    pc_chi_peak_rgg = np.nan # Use 'pc' prefix for consistency internally\n",
    "    try:\n",
    "        largest_N_rgg = fss_chi_df_rgg['N'].max()\n",
    "        largest_N_data_chi_rgg = fss_chi_df_rgg[fss_chi_df_rgg['N'] == largest_N_rgg]\n",
    "        if not largest_N_data_chi_rgg.empty:\n",
    "            peak_idx_rgg = largest_N_data_chi_rgg['susceptibility_chi'].idxmax()\n",
    "            if pd.notna(peak_idx_rgg) and peak_idx_rgg in largest_N_data_chi_rgg.index:\n",
    "                # Get the corresponding radius value\n",
    "                pc_chi_peak_rgg = largest_N_data_chi_rgg.loc[peak_idx_rgg, param_name_rgg]\n",
    "                print(f\"    r_c(RGG) estimate from œá peak (N={largest_N_rgg}): {pc_chi_peak_rgg:.6f}\")\n",
    "            else:\n",
    "                print(f\"    Could not find valid Chi peak index for RGG (N={largest_N_rgg}).\")\n",
    "        else:\n",
    "            print(f\"    No RGG data found for N={largest_N_rgg} to estimate Chi peak.\")\n",
    "    except KeyError as e_peak_key_rgg:\n",
    "         print(f\"    Could not estimate from Chi peak: Missing column {e_peak_key_rgg}\")\n",
    "    except Exception as e_chi_rgg:\n",
    "        print(f\"    Could not estimate from Chi peak: {e_chi_rgg}\")\n",
    "\n",
    "\n",
    "# Final message if analysis was skipped\n",
    "elif analysis_error_rgg:\n",
    "     print(\"\\n‚ùå Skipping RGG Analysis Steps due to configuration or diagnostic errors.\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 11.2: RGG Analysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477489a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.3: Universality Class Comparison (Using Optuna Chi FSS Results)\n",
    "# Description: Compares the critical exponents (gamma, nu) estimated via Optuna FSS\n",
    "#              on Susceptibility (Chi) for WS, SBM, and RGG models to assess universality.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import warnings # Import warnings\n",
    "\n",
    "print(\"\\n--- Cell 11.3: Universality Class Comparison (Using Optuna Chi FSS Results) ---\")\n",
    "\n",
    "# --- Helper Function (redefine or ensure available) ---\n",
    "def format_metric(value, fmt):\n",
    "    \"\"\"Safely formats a numerical value using a format string.\"\"\"\n",
    "    is_valid_number = False\n",
    "    if value is not None:\n",
    "        # Check if it's an int or float, and finite\n",
    "        if isinstance(value, (int, float)) and np.isfinite(value):\n",
    "            is_valid_number = True\n",
    "    if is_valid_number:\n",
    "        try:\n",
    "            # Attempt to format the number\n",
    "            return fmt % value\n",
    "        except (TypeError, ValueError):\n",
    "            # Return error string if formatting fails\n",
    "            return \"Format Error\"\n",
    "    else:\n",
    "        # Return N/A string for non-numbers or non-finite numbers\n",
    "        return \"N/A\"\n",
    "\n",
    "# --- Prerequisites ---\n",
    "comparison_error = False\n",
    "results_store_chi = {} # Store results specifically from Chi FSS\n",
    "\n",
    "# Check WS Results from Cell 9\n",
    "ws_results_valid = False\n",
    "# Check if the global variable exists and is a dictionary\n",
    "if 'global_optuna_fss_chi_results' in globals() and isinstance(global_optuna_fss_chi_results, dict):\n",
    "     # Check if the 'success' key is present and True\n",
    "     if global_optuna_fss_chi_results.get('success', False):\n",
    "          results_store_chi['WS'] = global_optuna_fss_chi_results\n",
    "          ws_results_valid = True\n",
    "     else:\n",
    "          # Print warning if success flag is False\n",
    "          print(\"‚ö†Ô∏è WS Optuna Chi FSS results indicate failure (success=False).\")\n",
    "else:\n",
    "     # Print warning if variable is missing or wrong type\n",
    "     print(\"‚ö†Ô∏è WS Optuna Chi FSS results ('global_optuna_fss_chi_results') missing or invalid type.\")\n",
    "\n",
    "# Check SBM Results from Cell 11.1\n",
    "sbm_results_valid = False\n",
    "if 'global_optuna_fss_chi_sbm_results' in globals() and isinstance(global_optuna_fss_chi_sbm_results, dict):\n",
    "     if global_optuna_fss_chi_sbm_results.get('success', False):\n",
    "          results_store_chi['SBM'] = global_optuna_fss_chi_sbm_results\n",
    "          sbm_results_valid = True\n",
    "     else: print(\"‚ö†Ô∏è SBM Optuna Chi FSS results indicate failure (success=False).\")\n",
    "else: print(\"‚ö†Ô∏è SBM Optuna Chi FSS results ('global_optuna_fss_chi_sbm_results') missing or invalid type.\")\n",
    "\n",
    "# Check RGG Results from Cell 11.2\n",
    "rgg_results_valid = False\n",
    "if 'global_optuna_fss_chi_rgg_results' in globals() and isinstance(global_optuna_fss_chi_rgg_results, dict):\n",
    "     if global_optuna_fss_chi_rgg_results.get('success', False):\n",
    "          results_store_chi['RGG'] = global_optuna_fss_chi_rgg_results\n",
    "          rgg_results_valid = True\n",
    "     else: print(\"‚ö†Ô∏è RGG Optuna Chi FSS results indicate failure (success=False).\")\n",
    "else: print(\"‚ö†Ô∏è RGG Optuna Chi FSS results ('global_optuna_fss_chi_rgg_results') missing or invalid type.\")\n",
    "\n",
    "# Check if enough results are available for comparison\n",
    "successful_models_count = len(results_store_chi)\n",
    "if successful_models_count < 2:\n",
    "     print(f\"‚ùå Need successful Optuna Chi FSS results from at least two models for comparison (Found {successful_models_count}).\")\n",
    "     comparison_error = True\n",
    "\n",
    "# Load config if needed for output paths\n",
    "output_dir = None; exp_name = None\n",
    "if 'config' in globals() and isinstance(config, dict):\n",
    "     try:\n",
    "          output_dir = config['OUTPUT_DIR']; exp_name = config['EXPERIMENT_NAME']\n",
    "     except KeyError:\n",
    "          comparison_error = True; print(\"‚ùå Config missing OUTPUT_DIR or EXPERIMENT_NAME.\")\n",
    "else:\n",
    "     comparison_error = True; print(\"‚ùå Config dictionary missing.\")\n",
    "\n",
    "# --- Compare Exponents ---\n",
    "if not comparison_error:\n",
    "    print(\"\\n--- Comparing Critical Exponents (Œ≥, ŒΩ) Across Models (from Chi FSS) ---\")\n",
    "    comparison_data = [] # List to store dictionaries for DataFrame\n",
    "    gamma_values_comp = [] # List to store valid gamma values for stats\n",
    "    nu_values_comp = []    # List to store valid nu values for stats\n",
    "    models_compared = list(results_store_chi.keys()) # Get list of models with results\n",
    "\n",
    "    # Iterate through the stored results for each successful model\n",
    "    model_idx = 0\n",
    "    while model_idx < len(models_compared):\n",
    "        model = models_compared[model_idx]\n",
    "        results = results_store_chi[model] # Get the results dict for the model\n",
    "\n",
    "        # Extract exponents and critical point safely using .get()\n",
    "        gamma = results.get('gamma', np.nan)\n",
    "        nu = results.get('nu', np.nan)\n",
    "        pc = results.get('pc', np.nan) # Critical point (p_c, p_c(SBM), r_c)\n",
    "        obj = results.get('objective', np.nan) # Optuna objective value\n",
    "\n",
    "        # Append formatted data for the comparison table\n",
    "        comparison_data.append({\n",
    "            'Model': model,\n",
    "            # Use param name consistent with model if possible (p_c, p_c(SBM), r_c)\n",
    "            'Critical Point': format_metric(pc, '%.5f'),\n",
    "            'Gamma (Œ≥)': format_metric(gamma, '%.3f'),   # Format gamma\n",
    "            'Nu (ŒΩ)': format_metric(nu, '%.3f'),        # Format nu\n",
    "            'Optuna Objective': format_metric(obj, '%.2e') # Format objective value\n",
    "        })\n",
    "\n",
    "        # Add valid exponents to lists for statistical comparison\n",
    "        # Check using pd.notna which handles None and np.nan\n",
    "        if pd.notna(gamma):\n",
    "            gamma_values_comp.append(gamma)\n",
    "        if pd.notna(nu):\n",
    "            nu_values_comp.append(nu)\n",
    "\n",
    "        model_idx += 1 # Increment loop counter\n",
    "\n",
    "    # Create and print the comparison DataFrame\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    # Use to_string() for better console formatting without truncation\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "    # --- Quantitative Comparison (Relative Standard Deviation - RSD) ---\n",
    "    print(\"\\n  Quantitative Assessment (RSD):\")\n",
    "    # Calculate Gamma stats if enough values (at least 2)\n",
    "    gamma_mean = np.nan; gamma_std = np.nan; gamma_rsd = np.inf # Initialize stats\n",
    "    if len(gamma_values_comp) >= 2:\n",
    "        gamma_mean = np.mean(gamma_values_comp); gamma_std = np.std(gamma_values_comp)\n",
    "        # Calculate RSD = (StdDev / |Mean|) * 100%, handle potential zero mean\n",
    "        if gamma_mean != 0 and pd.notna(gamma_mean) and pd.notna(gamma_std):\n",
    "             # Ensure calculation is done using floats\n",
    "             gamma_rsd = (float(gamma_std) / abs(float(gamma_mean))) * 100.0\n",
    "        print(f\"  Gamma (Œ≥): Mean={format_metric(gamma_mean, '%.3f')}, StdDev={format_metric(gamma_std, '%.3f')}, RSD={format_metric(gamma_rsd, '%.1f')}%\")\n",
    "        # Interpretation based on RSD threshold (e.g., 15-25%)\n",
    "        if gamma_rsd < 15.0:\n",
    "            print(\"    Suggests reasonable consistency for Gamma.\")\n",
    "        elif gamma_rsd < 25.0:\n",
    "            print(\"    Suggests potential moderate differences for Gamma.\")\n",
    "        else: # High RSD indicates likely distinct classes\n",
    "            print(\"    Suggests significant differences for Gamma (distinct classes).\")\n",
    "    else:\n",
    "        print(\"  Gamma (Œ≥): Cannot perform quantitative comparison (need ‚â• 2 valid estimates).\")\n",
    "\n",
    "    # Calculate Nu stats if enough values\n",
    "    nu_mean = np.nan; nu_std = np.nan; nu_rsd = np.inf # Initialize stats\n",
    "    if len(nu_values_comp) >= 2:\n",
    "        nu_mean = np.mean(nu_values_comp); nu_std = np.std(nu_values_comp)\n",
    "        # Calculate RSD for Nu\n",
    "        if nu_mean != 0 and pd.notna(nu_mean) and pd.notna(nu_std):\n",
    "             nu_rsd = (float(nu_std) / abs(float(nu_mean))) * 100.0\n",
    "        print(f\"  Nu (ŒΩ):    Mean={format_metric(nu_mean, '%.3f')}, StdDev={format_metric(nu_std, '%.3f')}, RSD={format_metric(nu_rsd, '%.1f')}%\")\n",
    "        # Interpretation based on RSD threshold\n",
    "        if nu_rsd < 15.0:\n",
    "            print(\"    Suggests reasonable consistency for Nu.\")\n",
    "        elif nu_rsd < 25.0:\n",
    "            print(\"    Suggests potential moderate differences for Nu.\")\n",
    "        else: # High RSD\n",
    "            print(\"    Suggests significant differences for Nu (distinct classes).\")\n",
    "    else:\n",
    "        print(\"  Nu (ŒΩ):    Cannot perform quantitative comparison (need ‚â• 2 valid estimates).\")\n",
    "\n",
    "    # --- Conclusion based on combined RSD analysis ---\n",
    "    print(\"\\n  Preliminary Universality Conclusion (based on Chi FSS):\")\n",
    "    # Determine overall consistency based on thresholds for BOTH exponents\n",
    "    # Using a stricter threshold (e.g., 20-25%) to declare distinct classes based on Phase 1 findings\n",
    "    gamma_likely_distinct = pd.isna(gamma_rsd) or gamma_rsd > 25.0\n",
    "    nu_likely_distinct = pd.isna(nu_rsd) or nu_rsd > 25.0\n",
    "\n",
    "    # Check if *either* exponent suggests distinct classes\n",
    "    if gamma_likely_distinct or nu_likely_distinct:\n",
    "         print(\"    ‚ùå Significant variation observed in at least one critical exponent.\")\n",
    "         print(f\"       (RSDs: Gamma={format_metric(gamma_rsd, '%.1f')}%, Nu={format_metric(nu_rsd, '%.1f')}%)\")\n",
    "         print(\"       Evidence strongly suggests models belong to DISTINCT universality classes.\")\n",
    "    # Check if both are reasonably consistent (low RSD)\n",
    "    elif gamma_rsd < 15.0 and nu_rsd < 15.0: # Need low RSD for both to suggest same class\n",
    "         print(\"    ‚úÖ Low variation in exponents observed.\")\n",
    "         print(\"       Evidence supports a single universality class across tested models,\")\n",
    "         print(f\"       characterized by Œ≥ ‚âà {format_metric(gamma_mean, '%.3f')} and ŒΩ ‚âà {format_metric(nu_mean, '%.3f')}.\")\n",
    "    else: # Intermediate case - results are borderline or ambiguous\n",
    "         print(\"    üü° Moderate or ambiguous variation in exponents.\")\n",
    "         print(f\"       (RSDs: Gamma={format_metric(gamma_rsd, '%.1f')}%, Nu={format_metric(nu_rsd, '%.1f')}%)\")\n",
    "         print(\"       Universality is questionable; distinct classes remain likely.\")\n",
    "\n",
    "\n",
    "    # Save comparison table to CSV if path is valid\n",
    "    if output_dir is not None and exp_name is not None:\n",
    "        comp_table_path = os.path.join(output_dir, f\"{exp_name}_universality_exponent_comparison_CHI.csv\")\n",
    "        try:\n",
    "            comparison_df.to_csv(comp_table_path, index=False)\n",
    "            print(f\"\\n‚úÖ Chi exponent comparison table saved to: {comp_table_path}\")\n",
    "        except Exception as e_save_comp:\n",
    "            print(f\"‚ùå Error saving comparison table: {e_save_comp}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Could not save comparison table (output path or experiment name missing).\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Skipping universality comparison due to missing results or configuration errors.\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 11.3: Universality Class Comparison completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.4: Energy Functional Analysis (Lyapunov Check - Final)\n",
    "# Description: Analyzes simulation results (combined if available) to check if the\n",
    "#              energy functional behaves like a Lyapunov function. Requires energy\n",
    "#              history to be stored during simulation for monotonicity check.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "print(\"\\n--- Cell 11.4: Energy Functional Analysis (Lyapunov Check - Final) ---\")\n",
    "\n",
    "# --- Prerequisites ---\n",
    "analysis_error_energy = False\n",
    "# Check for config dictionary\n",
    "if 'config' not in globals() or not isinstance(config, dict):\n",
    "    print(\"‚ùå FATAL: Config dictionary missing. Run Cell 1.\"); analysis_error_energy = True\n",
    "else:\n",
    "    # Load flags from config\n",
    "    config = globals()['config']\n",
    "    calculate_energy_flag = config.get('CALCULATE_ENERGY', False)\n",
    "    # Check if energy history was intended to be stored\n",
    "    store_history_flag = config.get('STORE_ENERGY_HISTORY', False)\n",
    "    energy_functional_type = config.get('ENERGY_FUNCTIONAL_TYPE', 'N/A')\n",
    "\n",
    "    # Skip analysis if energy calculation was disabled during the runs\n",
    "    if not calculate_energy_flag:\n",
    "        print(\"‚ÑπÔ∏è Skipping Energy Analysis: CALCULATE_ENERGY was False during sweeps.\")\n",
    "        analysis_error_energy = True\n",
    "\n",
    "# Find the relevant results DataFrame\n",
    "results_df_energy = pd.DataFrame() # Initialize empty DataFrame\n",
    "source_data_name = \"No Data\"\n",
    "if not analysis_error_energy:\n",
    "    # Prioritize combined universality results if available (contains all models)\n",
    "    if 'global_universality_results' in globals() and isinstance(global_universality_results, pd.DataFrame) and not global_universality_results.empty:\n",
    "        results_df_energy = global_universality_results\n",
    "        source_data_name = \"Combined Universality Results (All Models)\"\n",
    "    # Fallback to primary WS sweep results if universality results are missing\n",
    "    elif 'global_sweep_results' in globals() and isinstance(global_sweep_results, pd.DataFrame) and not global_sweep_results.empty:\n",
    "        results_df_energy = global_sweep_results\n",
    "        source_data_name = \"Primary WS Sweep Results\"\n",
    "    else:\n",
    "        # If no suitable DataFrame is found\n",
    "        print(\"‚ùå Cannot analyze energy: No suitable results DataFrame found ('global_universality_results' or 'global_sweep_results'). Run Cell 8 or 11.\")\n",
    "        analysis_error_energy = True\n",
    "\n",
    "\n",
    "# --- Analyze Energy Data ---\n",
    "if not analysis_error_energy:\n",
    "    print(f\"  Using data source: {source_data_name}\")\n",
    "    print(f\"  Analyzing energy functional type: {energy_functional_type}\")\n",
    "\n",
    "    # Define expected column names\n",
    "    energy_col = 'final_energy'\n",
    "    monotonic_col = 'energy_monotonic'\n",
    "\n",
    "    # --- Final Energy Statistics ---\n",
    "    # Check if the final energy column exists first\n",
    "    if energy_col not in results_df_energy.columns:\n",
    "        print(f\"‚ùå Cannot analyze final energy: Column ('{energy_col}') not found in DataFrame.\")\n",
    "        # Continue to check monotonicity if flag was set, but cannot analyze final energy values\n",
    "    else:\n",
    "         print(f\"\\n  Final Energy Statistics:\")\n",
    "         num_total_runs = len(results_df_energy)\n",
    "         # Count runs where final energy is a valid number (not NaN)\n",
    "         valid_energy_runs = results_df_energy[energy_col].notna().sum()\n",
    "         print(f\"    Total Simulation Runs in DataFrame: {num_total_runs}\")\n",
    "         print(f\"    Runs with Valid Final Energy: {valid_energy_runs}\")\n",
    "\n",
    "         if valid_energy_runs > 0:\n",
    "             # Calculate mean and standard deviation of valid final energies\n",
    "             mean_final_energy = results_df_energy[energy_col].mean()\n",
    "             std_final_energy = results_df_energy[energy_col].std()\n",
    "             min_final_energy = results_df_energy[energy_col].min()\n",
    "             max_final_energy = results_df_energy[energy_col].max()\n",
    "             print(f\"    Mean Final Energy: {mean_final_energy:.4f}\")\n",
    "             print(f\"    Std Dev Final Energy: {std_final_energy:.4f}\")\n",
    "             print(f\"    Min Final Energy: {min_final_energy:.4f}\")\n",
    "             print(f\"    Max Final Energy: {max_final_energy:.4f}\")\n",
    "         else:\n",
    "              print(\"    No valid final energy values found to calculate statistics.\")\n",
    "\n",
    "    # --- Analyze Monotonicity (Lyapunov Check) ---\n",
    "    print(\"\\n  Lyapunov Behavior Statistics (Monotonicity Check):\")\n",
    "    # Check if energy history was supposed to be stored\n",
    "    if not store_history_flag:\n",
    "        print(\"    Monotonicity check skipped: STORE_ENERGY_HISTORY was False during sweeps.\")\n",
    "    # Check if the monotonicity result column exists\n",
    "    elif monotonic_col not in results_df_energy.columns:\n",
    "        print(f\"    ‚ö†Ô∏è Cannot analyze energy monotonicity: Column ('{monotonic_col}') not found.\")\n",
    "        print(\"       (Check if `run_single_instance` correctly calculates and returns this column when store_energy_history=True).\")\n",
    "    else:\n",
    "        # Proceed with analysis if column exists\n",
    "        # Count runs where monotonicity check result is valid (not NaN)\n",
    "        valid_monotonic_checks = results_df_energy[monotonic_col].notna().sum()\n",
    "        if valid_monotonic_checks > 0:\n",
    "             # Count runs where energy was monotonic (True values)\n",
    "             # Assuming boolean True/False stored, sum() treats True as 1, False as 0\n",
    "             # Handle potential non-boolean types gracefully\n",
    "             try:\n",
    "                  # Attempt boolean conversion and sum, filter out NaNs first\n",
    "                  num_monotonic = results_df_energy[monotonic_col].dropna().astype(bool).sum()\n",
    "             except (TypeError, ValueError):\n",
    "                  # Fallback if conversion fails (e.g., unexpected strings)\n",
    "                  warnings.warn(\"Could not reliably convert 'energy_monotonic' column to boolean for counting.\", RuntimeWarning)\n",
    "                  num_monotonic = \"Error\" # Indicate counting failure\n",
    "\n",
    "             # Calculate fraction if counting was successful\n",
    "             monotonic_fraction = \"N/A\"\n",
    "             if isinstance(num_monotonic, (int, float)): # Check if count is a number\n",
    "                   monotonic_fraction = num_monotonic / valid_monotonic_checks\n",
    "                   print(f\"    Total Runs with Valid Monotonicity Check: {valid_monotonic_checks}\")\n",
    "                   print(f\"    Runs with Monotonic/Stable Energy: {num_monotonic}\")\n",
    "                   print(f\"    Fraction Monotonic/Stable: {monotonic_fraction:.4f}\")\n",
    "                   # Interpretation based on the fraction\n",
    "                   if monotonic_fraction > 0.95:\n",
    "                       print(\"    ‚úÖ High fraction strongly supports Lyapunov-like behavior for the calculated energy.\")\n",
    "                   elif monotonic_fraction > 0.8:\n",
    "                       print(\"    üü° Moderate fraction suggests generally Lyapunov-like behavior, with some exceptions or noise.\")\n",
    "                   else:\n",
    "                       print(\"    ‚ùå Low fraction suggests the calculated energy functional ('{energy_functional_type}') is not consistently Lyapunov-like for these dynamics/parameters.\")\n",
    "             else:\n",
    "                   # If num_monotonic is \"Error\"\n",
    "                   print(f\"    Could not calculate monotonic fraction due to data type issues in '{monotonic_col}'.\")\n",
    "\n",
    "        else:\n",
    "             # If no valid monotonicity checks were found (all NaN)\n",
    "             print(\"    No valid monotonicity checks found (all values might be NaN).\")\n",
    "\n",
    "\n",
    "    # --- Mathematical Argument (Placeholder) ---\n",
    "    # Keep conceptual explanation from Phase 1 summary\n",
    "    print(\"\\n  Mathematical Argument (Conceptual):\")\n",
    "    print(\"    A formal proof that the energy functional is a strict Lyapunov function for\")\n",
    "    print(\"    these complex, stochastic dynamics remains challenging.\")\n",
    "    print(\"    The empirical monotonicity check provides evidence but is not definitive proof.\")\n",
    "    print(\"    Factors like noise, discrete updates, and boundary effects can influence behavior.\")\n",
    "\n",
    "else:\n",
    "    # This block executes if analysis_error_energy was True initially\n",
    "    print(\"‚ùå Skipping energy functional analysis due to configuration errors or lack of data.\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 11.4: Energy Functional Analysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d071c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.5: Rule Parameter Sensitivity Analysis (GPU - Final Attempt, Simplified Imports)\n",
    "# Description: Explicitly loads config, uses correct worker count, runs sweeps using\n",
    "#              imported worker function. Ensures all local helper functions are defined.\n",
    "#              All logic fully expanded.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "# *** Import ONLY the worker function from the external file ***\n",
    "# Try importing Phase 1 worker first as that was used in the provided output\n",
    "worker_func_sens = None\n",
    "try:\n",
    "    from worker_utils import run_single_instance\n",
    "    worker_func_sens = run_single_instance\n",
    "    print(\"‚úÖ Imported Phase 1 worker: run_single_instance for sensitivity analysis.\")\n",
    "except ImportError:\n",
    "    # Fallback to Phase 2 worker if Phase 1 worker isn't available\n",
    "    if 'run_single_instance_phase2' in globals():\n",
    "         print(\"‚ö†Ô∏è WARNING: Phase 1 worker 'run_single_instance' not found. Using Phase 2 worker 'run_single_instance_phase2' for sensitivity.\")\n",
    "         worker_func_sens = run_single_instance_phase2\n",
    "    else:\n",
    "         raise ImportError(\"‚ùå ERROR: Could not import 'run_single_instance' from worker_utils.py or find 'run_single_instance_phase2' in scope.\")\n",
    "\n",
    "# --- Define Helper functions NEEDED LOCALLY in THIS cell ---\n",
    "# (Copied from Cell 2 definitions to ensure they exist in this scope)\n",
    "# Note: Ensure these match the versions used successfully in Phase 1/Cell 2\n",
    "\n",
    "def get_sweep_parameters(graph_model_name, model_params, system_sizes, instances, trials, sensitivity_param=None, sensitivity_values=None):\n",
    "    \"\"\"Generates parameter dictionaries for simulation tasks, ensuring primary sweep param is always included.\"\"\"\n",
    "    all_task_params = []; base_seed = int(time.time() * 1000) % 100000; param_counter = 0\n",
    "    primary_param_key = None; primary_param_name = None; primary_param_values = None; fixed_params = {}\n",
    "    for key, values in model_params.items():\n",
    "        is_sweep_candidate = False\n",
    "        if isinstance(values, (list, np.ndarray)):\n",
    "             if len(values) > 0: is_sweep_candidate = True\n",
    "        if is_sweep_candidate:\n",
    "            primary_param_key = key; primary_param_name = key.replace('_values', ''); primary_param_values = values\n",
    "        else: fixed_params[key] = values\n",
    "    if primary_param_key is None:\n",
    "        if graph_model_name == 'RGG' and 'radius_values' in model_params: primary_param_key = 'radius_values'; primary_param_name = 'radius'; primary_param_values = model_params['radius_values']\n",
    "        elif graph_model_name == 'SBM' and 'p_intra_values' in model_params: primary_param_key = 'p_intra_values'; primary_param_name = 'p_intra'; primary_param_values = model_params['p_intra_values']\n",
    "        elif graph_model_name == 'WS' and 'p_values' in model_params: primary_param_key = 'p_values'; primary_param_name = 'p'; primary_param_values = model_params['p_values']\n",
    "        else: primary_param_name = 'param'; primary_param_key = 'param_values'; primary_param_values = [0]; warnings.warn(f\"Sweep param not found for {graph_model_name}. Using dummy 'param'.\", RuntimeWarning)\n",
    "    if not isinstance(primary_param_values, (list, np.ndarray)): primary_param_values = [primary_param_values]\n",
    "    primary_param_col_name = primary_param_name + '_value'\n",
    "    sens_loop_values = []\n",
    "    if sensitivity_param is not None and sensitivity_values is not None:\n",
    "        if isinstance(sensitivity_values, (list, np.ndarray)): sens_loop_values = sensitivity_values if len(sensitivity_values) > 0 else [None]\n",
    "        else: sens_loop_values = [sensitivity_values]\n",
    "    else: sens_loop_values = [None]\n",
    "    n_idx = 0\n",
    "    while n_idx < len(system_sizes):\n",
    "        N = system_sizes[n_idx]; p_val_idx = 0\n",
    "        while p_val_idx < len(primary_param_values):\n",
    "             p_val = primary_param_values[p_val_idx]; sens_val_idx = 0\n",
    "             while sens_val_idx < len(sens_loop_values):\n",
    "                 sens_val = sens_loop_values[sens_val_idx]; inst_idx = 0\n",
    "                 while inst_idx < instances:\n",
    "                     graph_seed = base_seed + param_counter + inst_idx * 13 + n_idx * 100 + p_val_idx * 10 + sens_val_idx\n",
    "                     trial_idx = 0\n",
    "                     while trial_idx < trials:\n",
    "                         sim_seed = base_seed + param_counter + inst_idx * 101 + trial_idx * 7 + n_idx * 1000 + p_val_idx * 100 + sens_val_idx\n",
    "                         task = {'model': graph_model_name, 'N': N, 'fixed_params': fixed_params.copy(),\n",
    "                                 primary_param_col_name: p_val, 'instance': inst_idx, 'trial': trial_idx,\n",
    "                                 'graph_seed': graph_seed, 'sim_seed': sim_seed,\n",
    "                                 'rule_param_name': sensitivity_param, 'rule_param_value': sens_val }\n",
    "                         all_task_params.append(task); param_counter += 1; trial_idx += 1\n",
    "                     inst_idx += 1\n",
    "                 sens_val_idx += 1\n",
    "             p_val_idx += 1\n",
    "        n_idx += 1\n",
    "    return all_task_params\n",
    "\n",
    "def generate_graph(model_name, params, N, seed):\n",
    "    \"\"\"Generates a graph using NetworkX.\"\"\"\n",
    "    np.random.seed(seed); G = nx.Graph()\n",
    "    try:\n",
    "        gen_params = params.copy(); base_param_name = next((k.replace('_value','') for k in gen_params if k.endswith('_value')), None)\n",
    "        if base_param_name and base_param_name+'_value' in gen_params: gen_params[base_param_name] = gen_params.pop(base_param_name+'_value')\n",
    "        if model_name == 'WS':\n",
    "            k = gen_params.get('k_neighbors', 4); p_rewire = gen_params.get('p', 0.1); k = int(k); k = max(2, k if k % 2 == 0 else k - 1); k = min(k, N - 1)\n",
    "            if N > k: G = nx.watts_strogatz_graph(n=N, k=k, p=p_rewire, seed=seed)\n",
    "            else: G = nx.complete_graph(N); warnings.warn(f\"WS N<=k ({N}<={k}), generating complete graph.\", RuntimeWarning)\n",
    "        elif model_name == 'SBM':\n",
    "            n_communities = gen_params.get('n_communities', 2); p_intra = gen_params.get('p_intra', 0.2); p_inter = gen_params.get('p_inter', 0.01)\n",
    "            if N < n_communities: n_communities = N; warnings.warn(f\"SBM N<communities\", RuntimeWarning)\n",
    "            if n_communities <= 0: raise ValueError(\"n_communities must be positive.\")\n",
    "            sizes = []; base_size = N // n_communities; remainder = N % n_communities; i = 0\n",
    "            while i < n_communities: sizes.append(base_size + (1 if i < remainder else 0)); i += 1\n",
    "            if 0 in sizes: raise ValueError(f\"SBM zero-sized community for N={N}, C={n_communities}\")\n",
    "            probs = []; row_idx = 0\n",
    "            while row_idx < n_communities:\n",
    "                 row = []; col_idx = 0\n",
    "                 while col_idx < n_communities: row.append(p_intra if row_idx == col_idx else p_inter); col_idx += 1\n",
    "                 probs.append(row); row_idx += 1\n",
    "            G = nx.stochastic_block_model(sizes=sizes, p=probs, seed=seed)\n",
    "        elif model_name == 'RGG':\n",
    "            radius = gen_params.get('radius', 0.1); G = nx.random_geometric_graph(n=N, radius=radius, seed=seed)\n",
    "        else: raise ValueError(f\"Unknown graph model: {model_name}\")\n",
    "    except Exception as e: G = nx.Graph(); warnings.warn(f\"Graph gen failed for {model_name} N={N} params={params}: {e}\", RuntimeWarning)\n",
    "    # Relabeling logic (expanded)\n",
    "    if G.number_of_nodes() > 0:\n",
    "         needs_relabel = False; node_iterator = iter(G.nodes()); stop_iter_relabel = False\n",
    "         while not stop_iter_relabel:\n",
    "             try:\n",
    "                 node = next(node_iterator)\n",
    "                 if not isinstance(node, str): needs_relabel = True; stop_iter_relabel = True\n",
    "             except StopIteration: stop_iter_relabel = True\n",
    "         if needs_relabel:\n",
    "             node_mapping = {}; original_nodes_relabel = list(G.nodes()); node_idx_relabel = 0\n",
    "             while node_idx_relabel < len(original_nodes_relabel): node_mapping[original_nodes_relabel[node_idx_relabel]] = str(original_nodes_relabel[node_idx_relabel]); node_idx_relabel += 1\n",
    "             G = nx.relabel_nodes(G, node_mapping, copy=False)\n",
    "    return G\n",
    "\n",
    "def reversed_sigmoid_func(x, A, x0, k, C):\n",
    "    \"\"\" Reversed sigmoid function (decreasing S-shape). \"\"\"\n",
    "    try:\n",
    "        x_array = np.asarray(x, dtype=float)\n",
    "        if not all(isinstance(p, (int, float)) and np.isfinite(p) for p in [A, x0, k, C]): return np.full_like(x_array, np.nan, dtype=float)\n",
    "        exponent_term = k * (x_array - x0); exponent_term_clipped = np.clip(exponent_term, -700, 700)\n",
    "        denominator = 1.0 + np.exp(exponent_term_clipped); denominator_safe = np.where(denominator == 0, 1e-300, denominator)\n",
    "        result = A / denominator_safe + C; result_final = np.nan_to_num(result, nan=np.nan, posinf=np.nan, neginf=np.nan); return result_final\n",
    "    except Exception: return np.full_like(np.asarray(x), np.nan, dtype=float) # Ensure return shape matches x\n",
    "\n",
    "\n",
    "print(\"\\n--- Cell 11.5: Rule Parameter Sensitivity Analysis (GPU) ---\")\n",
    "print(\"  Defined local helper functions.\")\n",
    "\n",
    "# --- Configuration Loading ---\n",
    "config = {}; analysis_error_sensitivity = False\n",
    "output_dir_sens = None; exp_name_sens = None; sensitivity_param_name = None; sensitivity_values = None\n",
    "TARGET_MODEL_SENS = 'WS' # Sensitivity usually tested on one baseline model\n",
    "param_col_name_sens = None; param_base_name_sens = None; system_sizes_sens = []\n",
    "num_instances_sens = 10; num_trials_sens = 3; rule_params_base_sens = {}; max_steps_sens = 200; conv_thresh_sens = 1e-4\n",
    "state_dim_sens = 5; workers_sens = 32; primary_metric_sens = 'variance_norm'; all_metrics_sens = []\n",
    "calculate_energy_sens = False; store_energy_history_sens = False; energy_type_sens = 'pairwise_dot'\n",
    "\n",
    "# Check if config exists\n",
    "if 'config' not in globals() or not isinstance(globals()['config'], dict):\n",
    "     print(\"‚ùå FATAL: Config dictionary missing. Run Cell 1.\"); analysis_error_sensitivity = True\n",
    "else:\n",
    "    try:\n",
    "        config = globals()['config'] # Use existing config\n",
    "        output_dir_sens = config['OUTPUT_DIR']; exp_name_sens = config['EXPERIMENT_NAME']\n",
    "        sensitivity_param_name = config.get('SENSITIVITY_RULE_PARAM')\n",
    "        sensitivity_values = config.get('SENSITIVITY_VALUES')\n",
    "        # Check if sensitivity analysis is configured\n",
    "        if sensitivity_param_name is None or sensitivity_values is None or len(sensitivity_values) == 0:\n",
    "             print(\"‚ÑπÔ∏è Skipping Sensitivity Analysis: SENSITIVITY_RULE_PARAM or SENSITIVITY_VALUES missing/empty in config.\")\n",
    "             analysis_error_sensitivity = True\n",
    "\n",
    "        if not analysis_error_sensitivity:\n",
    "             graph_params_sens = config['GRAPH_MODEL_PARAMS'].get(TARGET_MODEL_SENS,{})\n",
    "             # Find sweep parameter for the target model (WS)\n",
    "             ws_sweep_key = next((k for k in graph_params_sens if k.endswith('_values')), None)\n",
    "             if ws_sweep_key:\n",
    "                 param_base_name_sens = ws_sweep_key.replace('_values', '')\n",
    "                 param_col_name_sens = param_base_name_sens + '_value'\n",
    "             else:\n",
    "                 param_base_name_sens = 'p'; param_col_name_sens = 'p_value' # Default for WS\n",
    "                 warnings.warn(f\"Could not determine WS sweep parameter name for sensitivity. Assuming '{param_col_name_sens}'.\")\n",
    "\n",
    "             print(f\"  Sensitivity analysis target: Model={TARGET_MODEL_SENS}, Param='{sensitivity_param_name}', Values={sensitivity_values}\")\n",
    "             print(f\"  Sweep parameter for {TARGET_MODEL_SENS}: '{param_col_name_sens}'\")\n",
    "\n",
    "             # Use largest system size for sensitivity analysis for clearer signal\n",
    "             all_system_sizes = config.get('SYSTEM_SIZES', [])\n",
    "             if all_system_sizes:\n",
    "                 system_sizes_sens = [all_system_sizes[-1]] # Get the largest N\n",
    "             else:\n",
    "                 system_sizes_sens = [700] # Fallback N\n",
    "             N_sens = system_sizes_sens[0]\n",
    "             print(f\"  Using system size N = {N_sens} for sensitivity.\")\n",
    "\n",
    "             # Load other necessary params\n",
    "             num_instances_sens = config['NUM_INSTANCES_PER_PARAM']\n",
    "             num_trials_sens = config['NUM_TRIALS_PER_INSTANCE']\n",
    "             rule_params_base_sens = config['RULE_PARAMS']\n",
    "             max_steps_sens = config['MAX_SIMULATION_STEPS']\n",
    "             conv_thresh_sens = config['CONVERGENCE_THRESHOLD']\n",
    "             state_dim_sens = config['STATE_DIM']\n",
    "             workers_sens = config.get('PARALLEL_WORKERS', 32)\n",
    "             primary_metric_sens = config.get('PRIMARY_ORDER_PARAMETER', 'variance_norm')\n",
    "             all_metrics_sens = config.get('ORDER_PARAMETERS_TO_ANALYZE', [])\n",
    "             calculate_energy_sens = config.get('CALCULATE_ENERGY', False)\n",
    "             store_energy_history_sens = config.get('STORE_ENERGY_HISTORY', False)\n",
    "             energy_type_sens = config.get('ENERGY_FUNCTIONAL_TYPE', 'pairwise_dot')\n",
    "\n",
    "    except KeyError as e_key_sens:\n",
    "         print(f\"‚ùå FATAL: Missing key '{e_key_sens}' in config for sensitivity analysis.\")\n",
    "         analysis_error_sensitivity = True\n",
    "    except Exception as config_e:\n",
    "         print(f\"‚ùå FATAL: Error loading config for sensitivity: {config_e}\"); analysis_error_sensitivity = True\n",
    "\n",
    "# --- Device Check ---\n",
    "device_sens = torch.device('cpu') # Default device\n",
    "if not analysis_error_sensitivity:\n",
    "    if 'global_device' in globals():\n",
    "        device_sens = global_device # Use globally set device\n",
    "    else:\n",
    "         if torch.cuda.is_available(): device_sens = torch.device('cuda:0')\n",
    "    print(f\"  Using device for sensitivity runs: {device_sens}\")\n",
    "\n",
    "\n",
    "# --- File Paths & Loading ---\n",
    "all_sensitivity_results_list = []\n",
    "values_to_run = []\n",
    "combined_sensitivity_results_file = None\n",
    "combined_sensitivity_pickle_file = None\n",
    "\n",
    "if not analysis_error_sensitivity:\n",
    "    # Define file paths based on the sensitivity parameter name\n",
    "    combined_sensitivity_results_file = os.path.join(output_dir_sens, f\"{exp_name_sens}_sensitivity_{sensitivity_param_name}_COMBINED_results.csv\")\n",
    "    combined_sensitivity_pickle_file = os.path.join(output_dir_sens, f\"{exp_name_sens}_sensitivity_{sensitivity_param_name}_COMBINED_partial.pkl\")\n",
    "    values_to_run = list(sensitivity_values) # Copy the list\n",
    "\n",
    "    # Load existing partial results if available\n",
    "    if os.path.exists(combined_sensitivity_pickle_file):\n",
    "        try:\n",
    "            with open(combined_sensitivity_pickle_file, 'rb') as f_load_sens:\n",
    "                all_sensitivity_results_list = pickle.load(f_load_sens)\n",
    "            if isinstance(all_sensitivity_results_list, list) and len(all_sensitivity_results_list) > 0:\n",
    "                 loaded_sens_df = pd.DataFrame(all_sensitivity_results_list)\n",
    "                 # Check which sensitivity values are already present\n",
    "                 if 'sensitivity_param_value' in loaded_sens_df.columns:\n",
    "                      completed_values = loaded_sens_df['sensitivity_param_value'].unique()\n",
    "                      # Update values_to_run to only include missing ones\n",
    "                      values_to_run = [v for v in sensitivity_values if v not in completed_values]\n",
    "                      print(f\"  Loaded {len(all_sensitivity_results_list)} sensitivity results. Values completed: {completed_values}\")\n",
    "                 else:\n",
    "                      warnings.warn(\"Loaded sensitivity pickle missing 'sensitivity_param_value' column. Assuming all values need rerunning.\", RuntimeWarning)\n",
    "                      all_sensitivity_results_list = [] # Reset if column missing\n",
    "            else:\n",
    "                 all_sensitivity_results_list = [] # Reset if loaded data invalid\n",
    "        except Exception as e_load_pkl:\n",
    "            warnings.warn(f\"Could not load sensitivity pickle ({e_load_pkl}). Assuming all values need running.\", RuntimeWarning)\n",
    "            all_sensitivity_results_list = []\n",
    "    print(f\"  Sensitivity values remaining to run for '{sensitivity_param_name}': {values_to_run}\")\n",
    "\n",
    "\n",
    "# --- Run Sensitivity Sweeps ---\n",
    "if not analysis_error_sensitivity and len(values_to_run) > 0:\n",
    "    print(f\"\\n--- Running Sensitivity Sweeps for Param: '{sensitivity_param_name}' ---\")\n",
    "    # Ensure spawn method is set (should be done in Cell 0)\n",
    "    try:\n",
    "        if mp.get_start_method(allow_none=True) != 'spawn':\n",
    "             print(\"üö® WARNING: Forcing multiprocessing start method to 'spawn' for sensitivity runs.\")\n",
    "             mp.set_start_method('spawn', force=True)\n",
    "    except Exception as e_set_spawn_sens:\n",
    "         print(f\"‚ö†Ô∏è Warning: Could not set 'spawn' start method: {e_set_spawn_sens}. GPU workers might fail.\")\n",
    "\n",
    "    # Define keys needed to reconstruct results accurately if needed\n",
    "    essential_param_keys = ['model', 'N', 'instance', 'trial', 'graph_seed', 'sim_seed', 'rule_param_name', 'rule_param_value', param_col_name_sens]\n",
    "\n",
    "    # Iterate through each sensitivity value that needs to be run\n",
    "    sens_value_index = 0\n",
    "    while sens_value_index < len(values_to_run): # Use while loop\n",
    "         sens_value = values_to_run[sens_value_index]\n",
    "         print(f\"\\n-- Running for {sensitivity_param_name} = {sens_value:.4f} --\")\n",
    "\n",
    "         # Define rule parameters for this specific sensitivity value\n",
    "         current_rule_params = rule_params_base_sens.copy()\n",
    "         current_rule_params[sensitivity_param_name] = sens_value\n",
    "\n",
    "         # Generate tasks for this specific sensitivity value\n",
    "         sens_tasks = get_sweep_parameters(\n",
    "             graph_model_name=TARGET_MODEL_SENS,\n",
    "             model_params=config['GRAPH_MODEL_PARAMS'].get(TARGET_MODEL_SENS, {}), # Use original P1 ranges for WS sweep\n",
    "             system_sizes=system_sizes_sens, # Use specific N for sensitivity\n",
    "             instances=num_instances_sens,\n",
    "             trials=num_trials_sens,\n",
    "             sensitivity_param=sensitivity_param_name, # Pass the sensitivity param name\n",
    "             sensitivity_values=[sens_value] # Pass ONLY the current value\n",
    "         )\n",
    "         print(f\"  Generated {len(sens_tasks)} tasks for value {sens_value:.4f}...\")\n",
    "\n",
    "         # Check if tasks were generated and have the sweep parameter column\n",
    "         tasks_are_valid = False\n",
    "         if len(sens_tasks) > 0:\n",
    "             if param_col_name_sens in sens_tasks[0]:\n",
    "                 tasks_are_valid = True\n",
    "             else:\n",
    "                  warnings.warn(f\"Generated tasks missing sweep key '{param_col_name_sens}'! Check get_sweep_parameters.\", RuntimeWarning)\n",
    "         else:\n",
    "              print(\"  No tasks generated for this sensitivity value.\")\n",
    "\n",
    "         if not tasks_are_valid:\n",
    "              sens_value_index += 1; continue # Skip to next value if tasks invalid\n",
    "\n",
    "         # --- Execute Sweep for this Sensitivity Value ---\n",
    "         sens_start_time = time.time(); futures_map = {}; pool_broken_flag_sens = False\n",
    "         executor_instance_sens = ProcessPoolExecutor(max_workers=workers_sens)\n",
    "         try: # Process pool execution\n",
    "             # Submit tasks to the pool\n",
    "             task_idx_sens = 0\n",
    "             while task_idx_sens < len(sens_tasks):\n",
    "                 task_params = sens_tasks[task_idx_sens]\n",
    "                 # Ensure sweep key exists before graph gen (redundant check)\n",
    "                 if param_col_name_sens not in task_params: task_idx_sens += 1; continue\n",
    "\n",
    "                 graph_gen_params_sens = task_params.get('fixed_params', {}).copy()\n",
    "                 graph_gen_params_sens[param_base_name_sens] = task_params[param_col_name_sens]\n",
    "                 G = generate_graph( task_params['model'], graph_gen_params_sens, task_params['N'], task_params['graph_seed'] )\n",
    "\n",
    "                 if G is None or G.number_of_nodes() == 0: task_idx_sens += 1; continue # Skip failed graph gen\n",
    "\n",
    "                 # Use the selected worker function (imported earlier)\n",
    "                 future = executor_instance_sens.submit(\n",
    "                     worker_func_sens, # Use the imported function\n",
    "                     G, task_params['N'], task_params, task_params['sim_seed'],\n",
    "                     current_rule_params, # Pass rules specific to this sensitivity value\n",
    "                     max_steps_sens, conv_thresh_sens, state_dim_sens,\n",
    "                     calculate_energy_sens, store_energy_history_sens, energy_type_sens,\n",
    "                     all_metrics_sens, str(device_sens) # Pass device name as string\n",
    "                 )\n",
    "                 futures_map[future] = task_params; task_idx_sens += 1 # Store future and params\n",
    "\n",
    "             # Collect results using as_completed for better progress update\n",
    "             pbar_sens = tqdm(total=len(futures_map), desc=f\"Sens. ({sens_value:.3f})\", mininterval=2.0, unit=\"task\")\n",
    "             results_this_value = [] # Store results specific to this sens_value run\n",
    "             try:\n",
    "                 futures_iterator = as_completed(futures_map)\n",
    "                 for future in futures_iterator: # Expanded loop for clarity\n",
    "                     original_task_params = futures_map[future] # Get back original task info\n",
    "                     if pool_broken_flag_sens: pbar_sens.update(1); continue # Update progress but skip if pool broke\n",
    "\n",
    "                     try:\n",
    "                         result_dict = future.result(timeout=1200) # Get result with timeout\n",
    "                         if result_dict is not None and isinstance(result_dict, dict):\n",
    "                             # Explicitly reconstruct the full result dictionary from task + result\n",
    "                             full_result = {}\n",
    "                             # Copy essential keys from original task parameters\n",
    "                             essential_idx = 0\n",
    "                             while essential_idx < len(essential_param_keys):\n",
    "                                 key = essential_param_keys[essential_idx]\n",
    "                                 if key in original_task_params:\n",
    "                                     full_result[key] = original_task_params[key]\n",
    "                                 essential_idx += 1\n",
    "                             # Update with the results returned by the worker\n",
    "                             full_result.update(result_dict)\n",
    "                             # Final safety check for sweep parameter key (should exist)\n",
    "                             if param_col_name_sens not in full_result:\n",
    "                                 if param_col_name_sens in original_task_params:\n",
    "                                     full_result[param_col_name_sens] = original_task_params[param_col_name_sens]\n",
    "                                 else:\n",
    "                                     warnings.warn(f\"Essential key '{param_col_name_sens}' missing after merge!\", RuntimeWarning)\n",
    "                             results_this_value.append(full_result) # Add to list for this value\n",
    "                     except Exception as e_get_sens:\n",
    "                          error_str_sens = str(e_get_sens)\n",
    "                          is_broken_sens = False\n",
    "                          if \"Broken\" in error_str_sens or \"abruptly\" in error_str_sens or \"shutdown\" in error_str_sens: is_broken_sens = True\n",
    "                          elif isinstance(e_get_sens, TypeError) or isinstance(e_get_sens, AttributeError): is_broken_sens = True\n",
    "                          if is_broken_sens:\n",
    "                               print(f\"\\n‚ùå Pool broke during sensitivity run ({sens_value:.3f})\"); pool_broken_flag_sens = True; break # Exit inner loop\n",
    "                          else: # Handle other errors like timeout\n",
    "                               warnings.warn(f\"Error getting result for sensitivity task {original_task_params}: {type(e_get_sens).__name__}\", RuntimeWarning)\n",
    "                               error_res_sens = {**original_task_params, 'error_message': f\"Future failed: {type(e_get_sens).__name__}\"}\n",
    "                               results_this_value.append(error_res_sens)\n",
    "                     finally:\n",
    "                          pbar_sens.update(1) # Update progress bar\n",
    "\n",
    "             except KeyboardInterrupt: print(f\"\\nInterrupted sensitivity run ({sens_value:.3f}).\")\n",
    "             finally: pbar_sens.close(); # Ensure progress bar is closed\n",
    "\n",
    "         except Exception as main_e_sens:\n",
    "              print(f\"\\n‚ùå ERROR during Sensitivity setup/execution for {sens_value:.3f}: {main_e_sens}\")\n",
    "              traceback.print_exc(limit=1)\n",
    "         finally:\n",
    "              print(f\"Shutting down executor ({sens_value:.3f})...\"); executor_instance_sens.shutdown(wait=True, cancel_futures=True); print(\"Executor shut down.\")\n",
    "\n",
    "         # Process results for this sensitivity value\n",
    "         sens_end_time = time.time(); print(f\"  ‚úÖ Sweep for {sens_value:.3f} completed ({sens_end_time-sens_start_time:.1f}s).\")\n",
    "         # Filter out potential None results before adding\n",
    "         valid_results_this_value = [r for r in results_this_value if r is not None and isinstance(r, dict)]\n",
    "         added_now = 0\n",
    "         if len(valid_results_this_value) > 0:\n",
    "              # Extend the main list with results from this value\n",
    "              all_sensitivity_results_list.extend(valid_results_this_value)\n",
    "              added_now = len(valid_results_this_value)\n",
    "              print(f\"  Added {added_now} valid results to main list.\")\n",
    "              # Save incrementally after each sensitivity value completes\n",
    "              try:\n",
    "                  with open(combined_sensitivity_pickle_file, 'wb') as f_comb_sens_inc:\n",
    "                       pickle.dump(all_sensitivity_results_list, f_comb_sens_inc)\n",
    "              except Exception as e_save_inc_sens:\n",
    "                   warnings.warn(f\"Incremental save of sensitivity results failed: {e_save_inc_sens}\", RuntimeWarning)\n",
    "         else:\n",
    "              print(\"  ‚ö†Ô∏è No valid results obtained for this sensitivity value.\")\n",
    "\n",
    "         # Check if the pool broke during this run\n",
    "         if pool_broken_flag_sens:\n",
    "              print(\"‚ùå Aborting sensitivity sweep because process pool failed.\")\n",
    "              analysis_error_sensitivity = True # Mark analysis as failed\n",
    "              break # Exit the outer while loop over sensitivity values\n",
    "\n",
    "         sens_value_index += 1 # Increment outer loop counter\n",
    "\n",
    "    # Final message if errors occurred during the sweep\n",
    "    if analysis_error_sensitivity: print(\"\\n‚ùå Errors occurred during sensitivity sweep execution.\")\n",
    "\n",
    "\n",
    "# --- Save Combined Sensitivity Results ---\n",
    "global_sensitivity_results = pd.DataFrame() # Initialize global variable\n",
    "if not analysis_error_sensitivity and len(all_sensitivity_results_list) > 0:\n",
    "    print(\"\\nSaving combined sensitivity results...\")\n",
    "    try:\n",
    "        # Create DataFrame from the combined list\n",
    "        combined_sens_df = pd.DataFrame(all_sensitivity_results_list)\n",
    "\n",
    "        # --- Critical Check: Ensure sweep parameter column exists ---\n",
    "        if param_col_name_sens not in combined_sens_df.columns:\n",
    "             # This should not happen if task generation and result merging worked correctly\n",
    "             missing_col_msg = f\"CRITICAL ERROR: Sweep parameter column '{param_col_name_sens}' is missing from the final sensitivity DataFrame! Check worker result merging.\"\n",
    "             warnings.warn(missing_col_msg, RuntimeWarning)\n",
    "             raise KeyError(missing_col_msg) # Raise error to prevent saving bad data\n",
    "        else:\n",
    "             print(f\"  Column '{param_col_name_sens}' confirmed present in sensitivity DataFrame.\")\n",
    "\n",
    "        # Save metadata to CSV (excluding large vector columns)\n",
    "        cols_to_save_sens = [col for col in combined_sens_df.columns if col not in ['final_state_vector', 'state_history', 'avg_change_history', 'baseline_state_for_spread']]\n",
    "        combined_sens_df[cols_to_save_sens].to_csv(combined_sensitivity_results_file, index=False)\n",
    "        # Save the full data (including vectors if present) to Pickle\n",
    "        with open(combined_sensitivity_pickle_file, 'wb') as f_comb_sens_final:\n",
    "             pickle.dump(all_sensitivity_results_list, f_comb_sens_final)\n",
    "        print(f\"  ‚úÖ Combined sensitivity results saved ({combined_sens_df.shape[0]} entries). CSV: '{combined_sensitivity_results_file}', Pickle: '{combined_sensitivity_pickle_file}'\")\n",
    "        # Assign the full DataFrame to the global variable\n",
    "        global_sensitivity_results = combined_sens_df\n",
    "\n",
    "    except KeyError as e_key_save: # Catch the specific KeyError raised above\n",
    "         print(f\"‚ùå Error saving sensitivity DataFrame due to missing key: {e_key_save}\")\n",
    "         global_sensitivity_results = pd.DataFrame() # Ensure global var is empty DF\n",
    "    except Exception as e_save_comb_sens:\n",
    "         print(f\"‚ùå Error creating/saving combined sensitivity DataFrame: {e_save_comb_sens}\")\n",
    "         traceback.print_exc(limit=1)\n",
    "         global_sensitivity_results = pd.DataFrame() # Ensure global var is empty DF\n",
    "\n",
    "\n",
    "# --- Inspect DataFrame ---\n",
    "print(\"\\n--- Inspecting `global_sensitivity_results` DataFrame ---\")\n",
    "if 'global_sensitivity_results' in globals() and isinstance(global_sensitivity_results, pd.DataFrame) and not global_sensitivity_results.empty:\n",
    "    print(f\"  Shape: {global_sensitivity_results.shape}\")\n",
    "    print(f\"  Columns: {list(global_sensitivity_results.columns)}\")\n",
    "    # Check specifically for the sweep parameter and sensitivity value columns\n",
    "    sweep_col_present = param_col_name_sens in global_sensitivity_results.columns\n",
    "    sens_val_col_present = 'sensitivity_param_value' in global_sensitivity_results.columns\n",
    "    print(f\"  Sweep Param Column ('{param_col_name_sens}') Present: {'‚úÖ Yes' if sweep_col_present else '‚ùå NO'}\")\n",
    "    print(f\"  Sensitivity Value Column ('sensitivity_param_value') Present: {'‚úÖ Yes' if sens_val_col_present else '‚ùå NO'}\")\n",
    "    print(\"  Head:\\n\", global_sensitivity_results.head().to_string())\n",
    "else:\n",
    "    print(\"  DataFrame `global_sensitivity_results` is missing or empty.\")\n",
    "\n",
    "\n",
    "# --- Analyze Sensitivity Impact (Simple Fit) ---\n",
    "if not analysis_error_sensitivity and 'global_sensitivity_results' in globals() and isinstance(global_sensitivity_results, pd.DataFrame) and not global_sensitivity_results.empty:\n",
    "    # Check required columns for analysis\n",
    "    required_analysis_cols = [param_col_name_sens, 'sensitivity_param_value', primary_metric_sens]\n",
    "    cols_missing_analysis = False\n",
    "    col_check_idx = 0\n",
    "    while col_check_idx < len(required_analysis_cols):\n",
    "        if required_analysis_cols[col_check_idx] not in global_sensitivity_results.columns:\n",
    "             print(f\"‚ùå Cannot analyze sensitivity impact: Column '{required_analysis_cols[col_check_idx]}' missing.\")\n",
    "             cols_missing_analysis = True\n",
    "             break\n",
    "        col_check_idx += 1\n",
    "\n",
    "    if not cols_missing_analysis:\n",
    "         print(f\"\\n--- Analyzing Impact of '{sensitivity_param_name}' on Critical Point (Simple Sigmoid Fit) ---\")\n",
    "         sensitivity_analysis_results = [] # Store fit results {sens_value: pc_estimate}\n",
    "         # Get unique sensitivity values present in the results\n",
    "         valid_sens_values_present = sorted(global_sensitivity_results['sensitivity_param_value'].unique())\n",
    "\n",
    "         if len(valid_sens_values_present) == 0:\n",
    "              print(\"  No sensitivity values found in the results DataFrame.\")\n",
    "         else:\n",
    "              # Iterate through each sensitivity value found in the results\n",
    "              sens_idx_analyze = 0\n",
    "              while sens_idx_analyze < len(valid_sens_values_present):\n",
    "                  sens_value = valid_sens_values_present[sens_idx_analyze]\n",
    "                  print(f\"  Analyzing results for {sensitivity_param_name} = {sens_value:.4f}\")\n",
    "                  # Filter DataFrame for the current sensitivity value\n",
    "                  sens_value_df = global_sensitivity_results[global_sensitivity_results['sensitivity_param_value'] == sens_value]\n",
    "\n",
    "                  # Aggregate results for fitting: group by sweep param, calc mean/std of order param\n",
    "                  pc_est = np.nan # Default estimate\n",
    "                  try:\n",
    "                      # Group by the primary sweep parameter (e.g., p_value)\n",
    "                      # Calculate mean and std dev of the primary order parameter\n",
    "                      agg_sens_df = sens_value_df.groupby(param_col_name_sens)[primary_metric_sens].agg(['mean', 'std']).reset_index()\n",
    "                      # Drop rows where mean could not be calculated (e.g., all NaNs in group)\n",
    "                      agg_sens_df = agg_sens_df.dropna(subset=['mean'])\n",
    "\n",
    "                      # Check if enough data points remain for fitting (e.g., need at least 4)\n",
    "                      min_points_for_fit = 4\n",
    "                      if agg_sens_df.empty or len(agg_sens_df) < min_points_for_fit:\n",
    "                          print(f\"    Not enough aggregated data points ({len(agg_sens_df)}) to perform sigmoid fit. Need >= {min_points_for_fit}.\")\n",
    "                          sensitivity_analysis_results.append({'sens_value': sens_value, 'pc': np.nan})\n",
    "                      else:\n",
    "                          # --- Perform Sigmoid Fit ---\n",
    "                          p_vals_sens = agg_sens_df[param_col_name_sens].values # X values for fit\n",
    "                          metric_vals_sens = agg_sens_df['mean'].values # Y values for fit\n",
    "\n",
    "                          # Provide initial guesses for parameters [A, x0, k, C]\n",
    "                          min_met=np.min(metric_vals_sens); max_met=np.max(metric_vals_sens)\n",
    "                          amp_guess=max_met-min_met # Amplitude guess\n",
    "                          pc_guess=np.median(p_vals_sens) # Guess pc is near median of p values\n",
    "                          p_range=max(p_vals_sens)-min(p_vals_sens)\n",
    "                          k_guess=abs(amp_guess)/(p_range+1e-6)*4 # Steepness guess (heuristic)\n",
    "                          offset_guess=min_met # Offset guess\n",
    "                          initial_params = [amp_guess, pc_guess, k_guess, offset_guess]\n",
    "\n",
    "                          # Define bounds for parameters if needed (optional but recommended)\n",
    "                          # Bounds: ([A_min, x0_min, k_min, C_min], [A_max, x0_max, k_max, C_max])\n",
    "                          fit_bounds = ([-np.inf, min(p_vals_sens), 1e-3, -np.inf], [np.inf, max(p_vals_sens), 1e3, np.inf])\n",
    "\n",
    "                          try:\n",
    "                              # Use curve_fit to find optimal parameters\n",
    "                              params, cov = curve_fit(reversed_sigmoid_func, p_vals_sens, metric_vals_sens, p0=initial_params, bounds=fit_bounds, maxfev=8000)\n",
    "                              # The estimated critical point (x0) is the second parameter\n",
    "                              pc_est = params[1]\n",
    "                              # Check if the estimated pc is within the range of the data (sanity check)\n",
    "                              if pc_est < min(p_vals_sens) or pc_est > max(p_vals_sens):\n",
    "                                   warnings.warn(f\"Sigmoid fit pc={pc_est:.4f} is outside the range of data [{min(p_vals_sens):.4f}, {max(p_vals_sens):.4f}] for sens_value={sens_value:.4f}.\", RuntimeWarning)\n",
    "                              print(f\"    Estimated p_c ‚âà {pc_est:.6f}\")\n",
    "                              sensitivity_analysis_results.append({'sens_value': sens_value, 'pc': pc_est})\n",
    "                          except RuntimeError as fit_err:\n",
    "                              print(f\"    Sigmoid fit failed to converge: {fit_err}\")\n",
    "                              sensitivity_analysis_results.append({'sens_value': sens_value, 'pc': np.nan})\n",
    "                          except Exception as fit_e:\n",
    "                               print(f\"    Sigmoid fit failed with unexpected error: {fit_e}\")\n",
    "                               sensitivity_analysis_results.append({'sens_value': sens_value, 'pc': np.nan})\n",
    "\n",
    "                  except KeyError as e_agg_key:\n",
    "                       print(f\"    ‚ùå KeyError during aggregation: {e_agg_key}. Check columns.\")\n",
    "                       sensitivity_analysis_results.append({'sens_value': sens_value, 'pc': np.nan})\n",
    "                  except Exception as agg_err:\n",
    "                      print(f\"    Error during aggregation/fitting: {agg_err}\")\n",
    "                      sensitivity_analysis_results.append({'sens_value': sens_value, 'pc': np.nan})\n",
    "\n",
    "                  sens_idx_analyze += 1 # Increment loop counter\n",
    "\n",
    "              # --- Plot Sensitivity Results ---\n",
    "              if len(sensitivity_analysis_results) > 0:\n",
    "                  # Create DataFrame from the fit results\n",
    "                  sens_results_df = pd.DataFrame(sensitivity_analysis_results)\n",
    "                  # Drop rows where pc estimation failed (NaN)\n",
    "                  sens_results_df = sens_results_df.dropna(subset=['pc'])\n",
    "\n",
    "                  if not sens_results_df.empty:\n",
    "                      # Create the plot\n",
    "                      fig_sens, ax_sens = plt.subplots(figsize=(8, 5))\n",
    "                      # Plot estimated pc vs sensitivity parameter value\n",
    "                      ax_sens.plot(sens_results_df['sens_value'], sens_results_df['pc'], marker='o', linestyle='-')\n",
    "                      # Label axes and title\n",
    "                      ax_sens.set_xlabel(f\"Rule Parameter: {sensitivity_param_name}\")\n",
    "                      ax_sens.set_ylabel(f\"Estimated Critical Point (p_c for {TARGET_MODEL_SENS})\")\n",
    "                      ax_sens.set_title(f\"Sensitivity of Critical Point to {sensitivity_param_name}\")\n",
    "                      ax_sens.grid(True, linestyle=':')\n",
    "                      plt.tight_layout()\n",
    "                      # Save the plot\n",
    "                      sens_plot_path = os.path.join(output_dir_sens, f\"{exp_name_sens}_sensitivity_pc_vs_{sensitivity_param_name}.png\")\n",
    "                      try:\n",
    "                           plt.savefig(sens_plot_path, dpi=150)\n",
    "                           print(f\"  ‚úÖ Sensitivity plot saved to: {sens_plot_path}\")\n",
    "                      except Exception as e_save_sens_plot:\n",
    "                           print(f\"  ‚ùå Error saving sensitivity plot: {e_save_sens_plot}\")\n",
    "                      plt.show() # Display the plot\n",
    "                      plt.close(fig_sens) # Close the figure\n",
    "                  else:\n",
    "                      print(\"  No successful sigmoid fits obtained to plot sensitivity.\")\n",
    "              else:\n",
    "                   print(\"  No sensitivity analysis results generated.\") # Should not happen if loop ran unless all fits failed\n",
    "else:\n",
    "    print(\"‚ùå Skipping Sensitivity Analysis section due to configuration, missing data, or errors.\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 11.5: Rule Parameter Sensitivity Analysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.6: State Dimensionality Comparison (Fix Graph Params)\n",
    "# Description: Runs basic WS sweeps for 1D and 2D state representations.\n",
    "#              Fixes KeyError by correctly passing parameters to generate_graph.\n",
    "#              Qualitatively compares behavior to the 5D baseline.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch  # Ensure torch is available if used by simplified runners\n",
    "import multiprocessing as mp  # Ensure imported if using ProcessPool\n",
    "import copy # For deepcopy\n",
    "\n",
    "print(\"\\n--- Cell 11.6: State Dimensionality Comparison (Fix Graph Params) ---\")\n",
    "\n",
    "# --- Configuration & Prerequisite Checks ---\n",
    "analysis_error_dim = False\n",
    "if 'config' not in globals() or not isinstance(config, dict):\n",
    "    print(\"‚ùå FATAL: Config dictionary missing. Run Cell 1.\"); analysis_error_dim = True\n",
    "# Need the Phase 2 worker as it needs to handle variable state_dim\n",
    "if 'run_single_instance_phase2' not in globals():\n",
    "    print(\"‚ùå FATAL: Phase 2 worker function 'run_single_instance_phase2' not defined. Run Cell 2.\")\n",
    "    analysis_error_dim = True\n",
    "else:\n",
    "     # Assign the correct worker function\n",
    "     worker_func_dim = run_single_instance_phase2\n",
    "     print(\"  Using Phase 2 worker 'run_single_instance_phase2' for dimensionality tests.\")\n",
    "\n",
    "if 'get_sweep_parameters' not in globals() or 'generate_graph' not in globals():\n",
    "    print(\"‚ùå FATAL: Helper functions missing. Run Cell 2.\"); analysis_error_dim = True\n",
    "\n",
    "# Load config vars needed\n",
    "dims_to_test_config = []\n",
    "fixed_N_dim = 100\n",
    "target_model_dim = 'WS'\n",
    "graph_params_all_dim = {}\n",
    "graph_params_dim = {}\n",
    "param_name_dim = None\n",
    "param_values_dim = None\n",
    "param_col_name_dim = None\n",
    "num_instances_dim = 5\n",
    "num_trials_dim = 2\n",
    "rule_params_base_dim = {}\n",
    "max_steps_dim = 200\n",
    "conv_thresh_dim = 1e-4\n",
    "workers_dim = 32\n",
    "output_dir_dim = None\n",
    "exp_name_dim = None\n",
    "primary_metric_dim = 'variance_norm'\n",
    "\n",
    "if not analysis_error_dim:\n",
    "     try:\n",
    "         config = globals()['config'] # Use existing config\n",
    "         # Get dimensions to test (e.g., [1, 2]), excluding the baseline 5D\n",
    "         dims_to_test_config = config.get('DIMENSIONALITY_TEST_SIZES', [1, 2, 5])\n",
    "         dims_to_test = [d for d in dims_to_test_config if d != 5 and isinstance(d, int) and d > 0]\n",
    "         if not dims_to_test:\n",
    "             print(\"‚ÑπÔ∏è No dimensions selected for comparison (excluding baseline D=5 or invalid values). Skipping.\")\n",
    "             analysis_error_dim = True\n",
    "\n",
    "         # Get parameters for the comparison run\n",
    "         fixed_N_dim = config.get('DIMENSIONALITY_TEST_N', 100) # Use specific N for this test\n",
    "         target_model_dim = 'WS'  # Compare using WS model as baseline\n",
    "         graph_params_all_dim = config.get('GRAPH_MODEL_PARAMS', {})\n",
    "         # Use original Phase 1 WS params for this sweep\n",
    "         graph_params_dim = graph_params_all_dim.get(target_model_dim, {})\n",
    "         if not graph_params_dim:\n",
    "             print(f\"‚ùå FATAL: Graph parameters for '{target_model_dim}' not found in config.\")\n",
    "             analysis_error_dim = True\n",
    "\n",
    "         # Find primary sweep param name and values for WS model\n",
    "         ws_sweep_key = next((k for k in graph_params_dim if k.endswith('_values')), None)\n",
    "         if ws_sweep_key:\n",
    "             param_name_dim = ws_sweep_key.replace('_values', '')  # e.g., 'p'\n",
    "             param_values_dim = graph_params_dim[ws_sweep_key]\n",
    "             param_col_name_dim = param_name_dim + '_value'  # e.g., 'p_value'\n",
    "         else:\n",
    "             print(f\"‚ùå FATAL: Could not find sweep parameter (ending in '_values') for {target_model_dim}.\")\n",
    "             analysis_error_dim = True\n",
    "\n",
    "         # Adjust run parameters for potentially quicker dimensionality test runs\n",
    "         num_instances_dim = max(1, config.get('NUM_INSTANCES_PER_PARAM', 10) // 2)\n",
    "         num_trials_dim = max(1, config.get('NUM_TRIALS_PER_INSTANCE', 3) // 2)\n",
    "         rule_params_base_dim = config.get('RULE_PARAMS', {})\n",
    "         max_steps_dim = config.get('MAX_SIMULATION_STEPS', 200)\n",
    "         conv_thresh_dim = config.get('CONVERGENCE_THRESHOLD', 1e-4)\n",
    "         workers_dim = config.get('PARALLEL_WORKERS', os.cpu_count())\n",
    "         output_dir_dim = config['OUTPUT_DIR']\n",
    "         exp_name_dim = config['EXPERIMENT_NAME']\n",
    "         primary_metric_dim = config.get('PRIMARY_ORDER_PARAMETER', 'variance_norm')\n",
    "\n",
    "     except KeyError as e_key_dim:\n",
    "          print(f\"‚ùå FATAL: Missing key '{e_key_dim}' in config for dimensionality test.\")\n",
    "          analysis_error_dim = True\n",
    "     except Exception as e_conf_dim:\n",
    "          print(f\"‚ùå FATAL: Error loading config for dimensionality test: {e_conf_dim}.\")\n",
    "          analysis_error_dim = True\n",
    "\n",
    "\n",
    "# --- Device Check ---\n",
    "device_dim = torch.device('cpu') # Default device\n",
    "if not analysis_error_dim:\n",
    "    if 'global_device' in globals():\n",
    "        device_dim = global_device # Use globally set device\n",
    "    else:\n",
    "         if torch.cuda.is_available(): device_dim = torch.device('cuda:0')\n",
    "    print(f\"  Using device for dimensionality runs: {device_dim}\")\n",
    "\n",
    "\n",
    "# --- Run Sweeps for 1D and 2D ---\n",
    "dim_results_list = [] # Store results from D=1, D=2 runs\n",
    "if not analysis_error_dim:\n",
    "    print(f\"\\n--- Running Dimensionality Sweeps for D={dims_to_test} (N={fixed_N_dim}) ---\")\n",
    "    # Set spawn method if needed (should be done in Cell 0)\n",
    "    try:\n",
    "        if mp.get_start_method(allow_none=True) != 'spawn':\n",
    "            print(\"üö® WARNING: Forcing multiprocessing start method to 'spawn' for dimensionality runs.\")\n",
    "            mp.set_start_method('spawn', force=True)\n",
    "    except Exception as e_set_spawn_dim:\n",
    "         print(f\"‚ö†Ô∏è Warning: Could not force 'spawn' start method: {e_set_spawn_dim}. GPU workers might fail.\")\n",
    "\n",
    "    # Iterate through the dimensions to test (e.g., [1, 2])\n",
    "    dim_idx = 0\n",
    "    while dim_idx < len(dims_to_test):\n",
    "        current_dim = dims_to_test[dim_idx]\n",
    "        print(f\"\\n--- Running Dimensionality Sweep for D = {current_dim} ---\")\n",
    "        # Note: Using the same 5D rule parameters. How they affect fewer dimensions depends on the hdc_5d_step_vectorized_torch implementation.\n",
    "        # We assume the step function handles lower dimensions gracefully (as implemented in Cell 2).\n",
    "        current_rule_params_dim = rule_params_base_dim.copy()\n",
    "\n",
    "        # Generate tasks for this dimension using Phase 1 WS sweep range\n",
    "        dim_tasks = get_sweep_parameters(\n",
    "            graph_model_name=target_model_dim,\n",
    "            model_params=graph_params_dim, # Use original Phase 1 WS params\n",
    "            system_sizes=[fixed_N_dim], # Only the specific N for this test\n",
    "            instances=num_instances_dim,\n",
    "            trials=num_trials_dim\n",
    "        )\n",
    "        print(f\"  Generated {len(dim_tasks)} tasks for D={current_dim}, N={fixed_N_dim}.\")\n",
    "\n",
    "        # Check if tasks were generated\n",
    "        if not dim_tasks:\n",
    "            print(\"  No tasks generated, skipping this dimension.\")\n",
    "            dim_idx += 1; continue\n",
    "\n",
    "        # Execute sweep for this dimension\n",
    "        dim_start_time = time.time()\n",
    "        dim_futures = {} # Map future object to original task parameters\n",
    "        pool_broken_flag_dim = False\n",
    "        executor_instance_dim = ProcessPoolExecutor(max_workers=workers_dim)\n",
    "        try:\n",
    "            # Submit tasks\n",
    "            submit_idx_dim = 0\n",
    "            while submit_idx_dim < len(dim_tasks):\n",
    "                task_params = dim_tasks[submit_idx_dim]\n",
    "                # *** CORRECTED PARAMETER PASSING TO generate_graph ***\n",
    "                graph_gen_params_dim = task_params.get('fixed_params', {}).copy()\n",
    "                sweep_param_col = param_col_name_dim # e.g., 'p_value'\n",
    "                if sweep_param_col in task_params:\n",
    "                    # Add sweep value using the base name expected by generate_graph (e.g., 'p')\n",
    "                    graph_gen_params_dim[param_name_dim] = task_params[sweep_param_col]\n",
    "                else:\n",
    "                    # This case indicates an error in get_sweep_parameters\n",
    "                    warnings.warn(f\"Sweep column {sweep_param_col} not found in task {task_params}. Graph generation may fail.\", RuntimeWarning)\n",
    "                    # Add a default value if needed, though failure is likely\n",
    "                    graph_gen_params_dim[param_name_dim] = 0.1 # Example default\n",
    "\n",
    "                # Generate graph\n",
    "                G = generate_graph(task_params['model'], graph_gen_params_dim, task_params['N'], task_params['graph_seed'])\n",
    "                # *********************************************************\n",
    "\n",
    "                if G is None or G.number_of_nodes() == 0:\n",
    "                    submit_idx_dim += 1; continue  # Skip failed graph gen\n",
    "\n",
    "                # Submit task using the Phase 2 worker, passing the current dimension\n",
    "                future = executor_instance_dim.submit(\n",
    "                    worker_func_dim,  # Use the assigned Phase 2 worker\n",
    "                    graph=G, N=task_params['N'], instance_params=task_params, trial_seed=task_params['sim_seed'],\n",
    "                    rule_params_in=current_rule_params_dim,\n",
    "                    max_steps=max_steps_dim, conv_thresh=conv_thresh_dim,\n",
    "                    state_dim=current_dim,  # *** Pass the dimension to simulate ***\n",
    "                    calculate_energy=False,  # Disable energy for simplicity\n",
    "                    store_energy_history=False,\n",
    "                    energy_type=None,\n",
    "                    metrics_to_calc=['variance_norm', 'entropy_dim_0'],  # Request only relevant metrics\n",
    "                    device=str(device_dim),\n",
    "                    # Phase 2 specific args (can be default/False for this run)\n",
    "                    store_state_history=False,\n",
    "                    perturbation_params=None,\n",
    "                    phase2_metrics_to_calc=[],\n",
    "                    keep_full_state_history=False # Don't need history back\n",
    "                )\n",
    "                dim_futures[future] = task_params  # Map future to task\n",
    "                submit_idx_dim += 1 # Increment submit loop\n",
    "\n",
    "            # Collect results\n",
    "            pbar_dim = tqdm(total=len(dim_futures), desc=f\"Sweep D={current_dim}\", mininterval=2.0, unit=\"task\")\n",
    "            results_this_dim = []\n",
    "            try:\n",
    "                # Use as_completed to process futures as they finish\n",
    "                future_get_iter = as_completed(dim_futures)\n",
    "                for future in future_get_iter:\n",
    "                    original_task_params_dim = dim_futures[future] # Get back original params\n",
    "                    if pool_broken_flag_dim: pbar_dim.update(1); continue # Skip if pool broke\n",
    "\n",
    "                    try:\n",
    "                        result_dict = future.result(timeout=300) # Shorter timeout for potentially simpler runs\n",
    "                        if result_dict is not None and isinstance(result_dict, dict):\n",
    "                            # Combine original task params with worker result\n",
    "                            full_result = copy.deepcopy(original_task_params_dim)\n",
    "                            full_result.update(result_dict)\n",
    "                            full_result['state_dim_run'] = current_dim  # Explicitly add dimension run\n",
    "                            # Remove vector/history if present to save memory\n",
    "                            if 'final_state_vector' in full_result: del full_result['final_state_vector']\n",
    "                            if 'state_history' in full_result: del full_result['state_history']\n",
    "                            if 'avg_change_history' in full_result: del full_result['avg_change_history']\n",
    "                            results_this_dim.append(full_result)\n",
    "                    except Exception as e_get_dim:\n",
    "                        error_str_dim = str(e_get_dim)\n",
    "                        is_broken_dim = False\n",
    "                        if \"Broken\" in error_str_dim or \"abruptly\" in error_str_dim or \"shutdown\" in error_str_dim: is_broken_dim = True\n",
    "                        elif isinstance(e_get_dim, TypeError) or isinstance(e_get_dim, AttributeError): is_broken_dim = True\n",
    "                        if is_broken_dim:\n",
    "                            pool_broken_flag_dim = True\n",
    "                            print(f\"\\n‚ùå Pool broke during D={current_dim} run.\"); break # Exit collection loop\n",
    "                        else: # Handle other errors like timeout\n",
    "                            warnings.warn(f\"Error getting result for D={current_dim} task {original_task_params_dim}: {type(e_get_dim).__name__}\", RuntimeWarning)\n",
    "                    finally:\n",
    "                        pbar_dim.update(1) # Update progress bar\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(f\"\\nInterrupted D={current_dim} run.\")\n",
    "            finally:\n",
    "                pbar_dim.close() # Ensure progress bar is closed\n",
    "\n",
    "        except Exception as main_e_dim:\n",
    "            print(f\"\\n‚ùå ERROR during Dimensionality setup/execution for D={current_dim}: {main_e_dim}\")\n",
    "            traceback.print_exc(limit=1)\n",
    "        finally:\n",
    "            print(f\"Shutting down executor D={current_dim}...\")\n",
    "            executor_instance_dim.shutdown(wait=True, cancel_futures=True)\n",
    "            print(\"Executor shut down.\")\n",
    "\n",
    "        # Process results for this dimension\n",
    "        dim_end_time = time.time()\n",
    "        print(f\"  ‚úÖ Sweep for D={current_dim} completed ({dim_end_time - dim_start_time:.1f}s).\")\n",
    "        valid_results_this_dim = [r for r in results_this_dim if r is not None and isinstance(r, dict)]\n",
    "        if len(valid_results_this_dim) > 0:\n",
    "            dim_results_list.extend(valid_results_this_dim)\n",
    "            print(f\"  Added {len(valid_results_this_dim)} results for D={current_dim}.\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è No valid results obtained for D={current_dim}.\")\n",
    "\n",
    "        # Check if pool broke during this dimension's run\n",
    "        if pool_broken_flag_dim:\n",
    "            print(f\"‚ùå Aborting dimensionality sweep due to broken pool at D={current_dim}.\")\n",
    "            analysis_error_dim = True # Mark analysis as failed\n",
    "            break # Exit the outer while loop over dimensions\n",
    "\n",
    "        dim_idx += 1 # Increment dimension loop counter\n",
    "\n",
    "# --- Qualitative Comparison Plot ---\n",
    "if not analysis_error_dim and len(dim_results_list) > 0:\n",
    "    print(\"\\n--- Plotting Dimensionality Comparison ---\")\n",
    "    # Create DataFrame from the collected D=1, D=2 results\n",
    "    dim_results_df = pd.DataFrame(dim_results_list)\n",
    "\n",
    "    # Validate DataFrame structure before plotting\n",
    "    plot_error_dim = False\n",
    "    if 'state_dim_run' not in dim_results_df.columns:\n",
    "         print(\"‚ùå Cannot plot: 'state_dim_run' column missing from results.\"); plot_error_dim = True\n",
    "    if param_col_name_dim not in dim_results_df.columns:\n",
    "         print(f\"‚ùå Cannot plot: Primary sweep column '{param_col_name_dim}' missing from results.\"); plot_error_dim = True\n",
    "    if primary_metric_dim not in dim_results_df.columns:\n",
    "          print(f\"‚ùå Cannot plot: Primary metric column '{primary_metric_dim}' missing from results.\"); plot_error_dim = True\n",
    "\n",
    "    if not plot_error_dim:\n",
    "        fig_dim, ax_dim = plt.subplots(figsize=(10, 6))\n",
    "        dims_found = sorted(dim_results_df['state_dim_run'].unique())\n",
    "        # Use distinct colors/markers for D=1, D=2\n",
    "        plot_styles = {1: ('royalblue', 'o', '-'), 2: ('firebrick', 'x', '-')}\n",
    "\n",
    "        # Plot D=1 and D=2 results using a loop\n",
    "        d_plot_idx = 0\n",
    "        while d_plot_idx < len(dims_found):\n",
    "            d = dims_found[d_plot_idx]\n",
    "            d_data = dim_results_df[dim_results_df['state_dim_run'] == d]\n",
    "            if not d_data.empty:\n",
    "                # Aggregate data: group by sweep param, calc mean/std of primary metric\n",
    "                agg_d_data = d_data.groupby(param_col_name_dim)[primary_metric_dim].agg(['mean', 'std']).reset_index().dropna()\n",
    "                if not agg_d_data.empty:\n",
    "                    style = plot_styles.get(d, ('black', '.', '--')) # Fallback style\n",
    "                    ax_dim.errorbar(agg_d_data[param_col_name_dim], agg_d_data['mean'], yerr=agg_d_data['std'],\n",
    "                                    marker=style[1], linestyle=style[2], color=style[0],\n",
    "                                    label=f'D = {d}', capsize=3, alpha=0.8, markersize=5)\n",
    "            d_plot_idx += 1\n",
    "\n",
    "        # --- Load and plot 5D baseline (using primary WS sweep results) ---\n",
    "        baseline_5d_data = pd.DataFrame() # Initialize empty\n",
    "        if 'global_sweep_results' in globals() and isinstance(global_sweep_results, pd.DataFrame) and not global_sweep_results.empty:\n",
    "             # Filter for WS model and largest N from that sweep\n",
    "             baseline_N = global_sweep_results['N'].max()\n",
    "             baseline_5d_data = global_sweep_results[(global_sweep_results['model'] == target_model_dim) &\n",
    "                                                      (global_sweep_results['N'] == baseline_N)].copy()\n",
    "             if not baseline_5d_data.empty:\n",
    "                  # Check required columns exist in baseline data\n",
    "                  baseline_cols_ok = True\n",
    "                  if primary_metric_dim not in baseline_5d_data.columns: baseline_cols_ok = False\n",
    "                  if param_col_name_dim not in baseline_5d_data.columns: baseline_cols_ok = False\n",
    "\n",
    "                  if baseline_cols_ok:\n",
    "                      agg_5d_data = baseline_5d_data.groupby(param_col_name_dim)[primary_metric_dim].agg(['mean', 'std']).reset_index().dropna()\n",
    "                      if not agg_5d_data.empty:\n",
    "                          ax_dim.errorbar(agg_5d_data[param_col_name_dim], agg_5d_data['mean'], yerr=agg_5d_data['std'],\n",
    "                                          marker='s', linestyle='--', label=f'D = 5 (Baseline, N={baseline_N})',\n",
    "                                          capsize=3, alpha=0.7, markersize=4, color='black', zorder=5) # Plot baseline on top\n",
    "                      else: print(\"  ‚ö†Ô∏è Baseline D=5 data empty after aggregation.\")\n",
    "                  else: print(\"  ‚ö†Ô∏è Baseline D=5 data missing required columns for plotting.\")\n",
    "             else: print(\"  ‚ö†Ô∏è Baseline D=5 data empty after filtering.\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è Could not load D=5 baseline data ('global_sweep_results' missing or empty).\")\n",
    "\n",
    "\n",
    "        # --- Finalize Plot ---\n",
    "        ax_dim.set_xlabel(f\"Topological Parameter ({param_name_dim} for {target_model_dim})\")\n",
    "        ax_dim.set_ylabel(f\"Order Parameter ({primary_metric_dim})\")\n",
    "        ax_dim.set_title(f\"Impact of State Dimensionality (N={fixed_N_dim} vs D=5 N={baseline_N if not baseline_5d_data.empty else 'N/A'})\")\n",
    "        ax_dim.set_xscale('log') # Use log scale for WS 'p' parameter\n",
    "        ax_dim.grid(True, linestyle=':')\n",
    "        ax_dim.legend()\n",
    "        plt.tight_layout()\n",
    "        # Save the plot\n",
    "        if output_dir_dim is not None and exp_name_dim is not None:\n",
    "             dim_plot_path = os.path.join(output_dir_dim, f\"{exp_name_dim}_dimensionality_comparison.png\")\n",
    "             try:\n",
    "                 plt.savefig(dim_plot_path, dpi=150)\n",
    "                 print(f\"  ‚úÖ Dimensionality comparison plot saved to: {dim_plot_path}\")\n",
    "             except Exception as e_save_dim:\n",
    "                 print(f\"  ‚ùå Error saving dimensionality plot: {e_save_dim}\")\n",
    "        else:\n",
    "             print(\"  ‚ö†Ô∏è Could not save dimensionality plot (output path or experiment name missing).\")\n",
    "        plt.show() # Display the plot\n",
    "        plt.close(fig_dim) # Close the figure\n",
    "\n",
    "        print(\"\\n  Qualitative Conclusion:\")\n",
    "        print(\"    Compare curves visually. Differences indicate state dimension impacts the emergent dynamics and transition behavior.\")\n",
    "        print(\"    Lower dimensions might show simpler transitions or different critical points.\")\n",
    "\n",
    "elif not analysis_error_dim:\n",
    "    print(\"‚ùå Skipping dimensionality comparison plotting: No valid results collected.\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping dimensionality comparison due to errors or config flags.\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 11.6: State Dimensionality Comparison completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3551351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: PCA Analysis of Attractor Landscapes (Phase 2 - Load Pickle)\n",
    "# Description: Loads landscape data from Pickle. Performs PCA if flag is set.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "# import ast # No longer needed for parsing with pickle\n",
    "import warnings\n",
    "import traceback\n",
    "import json\n",
    "import pickle # To load landscape data from pickle file\n",
    "\n",
    "print(\"\\n--- Cell 12: PCA Analysis of Attractor Landscapes (Phase 2 - Load Pickle) ---\")\n",
    "\n",
    "# --- Configuration & Prerequisites ---\n",
    "pca_error = False\n",
    "# Check for config dictionary\n",
    "if 'config' not in globals() or not isinstance(config, dict):\n",
    "    print(\"‚ùå FATAL: Config dictionary missing. Run Cell 1.\"); pca_error = True\n",
    "elif not config.get('RUN_PCA_ANALYSIS', False): # Check flag in config\n",
    "    print(\"‚ÑπÔ∏è Skipping PCA Analysis: RUN_PCA_ANALYSIS is False in config.\")\n",
    "    pca_error = True\n",
    "\n",
    "# Load necessary parameters from config if proceeding\n",
    "output_dir = None; exp_name = None; state_dim = 5; N_for_landscape = 700\n",
    "if not pca_error:\n",
    "    try:\n",
    "        config = globals()['config'] # Use existing config\n",
    "        output_dir = config['OUTPUT_DIR']; exp_name = config['EXPERIMENT_NAME']\n",
    "        state_dim = config.get('STATE_DIM', 5)\n",
    "        # Determine N used for landscape runs (usually largest N)\n",
    "        landscape_N_list = config.get('SYSTEM_SIZES', [700])\n",
    "        if landscape_N_list:\n",
    "             # Ensure list is not empty before accessing last element\n",
    "             if len(landscape_N_list) > 0:\n",
    "                  N_for_landscape = landscape_N_list[-1] # Use largest N from config\n",
    "             else:\n",
    "                   N_for_landscape = 700 # Fallback if list is empty\n",
    "        else:\n",
    "             N_for_landscape = 700 # Fallback\n",
    "        print(f\"  Config loaded. Target N for PCA: {N_for_landscape}\")\n",
    "    except KeyError as e_key_pca_conf:\n",
    "         print(f\"‚ùå FATAL: Missing key '{e_key_pca_conf}' in config for PCA.\")\n",
    "         pca_error = True\n",
    "    except Exception as e_pca_conf:\n",
    "         print(f\"‚ùå FATAL: Error loading config for PCA: {e_pca_conf}.\")\n",
    "         pca_error = True\n",
    "\n",
    "\n",
    "# --- Load Landscape Data (from Pickle generated in Cell 11.8) ---\n",
    "landscape_results_list = [] # Initialize empty list\n",
    "if not pca_error:\n",
    "    # Construct the expected pickle file path using Phase 2 variables\n",
    "    landscape_pickle_file = os.path.join(output_dir, f\"{exp_name}_landscape_COMBINED_partial.pkl\")\n",
    "    if os.path.exists(landscape_pickle_file):\n",
    "        print(f\"  Loading landscape data (with vectors) from: {landscape_pickle_file}\")\n",
    "        try:\n",
    "            with open(landscape_pickle_file, 'rb') as f_load_pca:\n",
    "                landscape_results_list = pickle.load(f_load_pca)\n",
    "            # Check if loaded data is a non-empty list\n",
    "            if not isinstance(landscape_results_list, list) or len(landscape_results_list) == 0:\n",
    "                print(\"  ‚ö†Ô∏è Warning: Loaded landscape data list is empty or not a list.\");\n",
    "                landscape_results_list = [] # Reset if invalid\n",
    "                pca_error = True # Treat as error if no data loaded\n",
    "            else:\n",
    "                print(f\"  Loaded {len(landscape_results_list)} entries from landscape pickle.\")\n",
    "        except Exception as e_load_pca_pkl:\n",
    "            print(f\"‚ùå Error loading landscape pickle: {e_load_pca_pkl}\");\n",
    "            landscape_results_list = [] # Reset list\n",
    "            pca_error = True # Treat as error if load fails\n",
    "    else:\n",
    "        # If the pickle file doesn't exist, PCA cannot be performed\n",
    "        print(f\"‚ùå Landscape data pickle not found: {landscape_pickle_file}\")\n",
    "        print(f\"   Run Cell 11.8 (Attractor Landscape UMAP/Data Collection) first.\")\n",
    "        pca_error = True\n",
    "\n",
    "# --- Prepare Data for PCA from Loaded List ---\n",
    "final_state_matrix = None # Initialize matrix\n",
    "pca_metadata = [] # Store corresponding parameters (model, p_value, etc.) for plotting\n",
    "pca_data_prepared = False # Flag\n",
    "\n",
    "if not pca_error:\n",
    "    print(f\"  Processing final state vectors for N={N_for_landscape}...\")\n",
    "    valid_flat_states = [] # List to hold valid numpy vectors\n",
    "    target_vector_size = N_for_landscape * state_dim # Expected size of flattened vector\n",
    "\n",
    "    # Iterate through the loaded list of result dictionaries\n",
    "    item_idx = 0\n",
    "    num_items_total = len(landscape_results_list)\n",
    "    while item_idx < num_items_total:\n",
    "        item = landscape_results_list[item_idx]\n",
    "        # Check if the item is a dictionary, matches the target N, and has the vector\n",
    "        is_valid_dict = isinstance(item, dict)\n",
    "        is_target_N = False\n",
    "        has_vector = False\n",
    "        is_error_free = False\n",
    "        if is_valid_dict:\n",
    "             # Use .get with default for safety if N is missing\n",
    "             if item.get('N') == N_for_landscape: is_target_N = True\n",
    "             if 'final_state_vector' in item: has_vector = True\n",
    "             if item.get('error_message') is None: is_error_free = True # Only use non-error results\n",
    "\n",
    "        # Only process if it's for the correct N, has a vector, and didn't report an error\n",
    "        if is_target_N and has_vector and is_error_free:\n",
    "            vec = item['final_state_vector']\n",
    "            # Check type, attempt conversion to numpy float array, check length and validity\n",
    "            current_vector = None\n",
    "            vector_valid_for_pca = False\n",
    "            if vec is not None: # Ensure vector is not None\n",
    "                try:\n",
    "                    # Ensure it's a numpy array of floats\n",
    "                    current_vector = np.array(vec, dtype=float)\n",
    "                    # Check expected flattened length\n",
    "                    is_correct_length = (current_vector.size == target_vector_size)\n",
    "                    # Check for NaNs or Infs\n",
    "                    contains_no_nan_inf = False # Assume invalid until checked\n",
    "                    if is_correct_length: # Only check content if length is right\n",
    "                         if not (np.isnan(current_vector).any() or np.isinf(current_vector).any()):\n",
    "                              contains_no_nan_inf = True\n",
    "\n",
    "                    if is_correct_length and contains_no_nan_inf:\n",
    "                        vector_valid_for_pca = True # Mark as valid if all checks pass\n",
    "                    # else: # Optional debug prints for invalid vectors\n",
    "                    #     if not is_correct_length: print(f\"Debug PCA Prep: Incorrect length {current_vector.size} vs {target_vector_size}\")\n",
    "                    #     if not contains_no_nan_inf: print(f\"Debug PCA Prep: Vector contains NaN/Inf\")\n",
    "\n",
    "                except (ValueError, TypeError) as parse_err:\n",
    "                    # Catch errors during np.array conversion (e.g., if vec is not numerical)\n",
    "                    warnings.warn(f\"Could not convert vector to valid numpy array during PCA prep: {parse_err}\", RuntimeWarning)\n",
    "                    vector_valid_for_pca = False # Mark as invalid on error\n",
    "                except Exception as unexpected_err:\n",
    "                     warnings.warn(f\"Unexpected error processing vector: {unexpected_err}\", RuntimeWarning)\n",
    "                     vector_valid_for_pca = False # Mark as invalid on unexpected errors\n",
    "\n",
    "            # If vector is valid, add it and its metadata\n",
    "            if vector_valid_for_pca:\n",
    "                valid_flat_states.append(current_vector)\n",
    "                # Store relevant metadata for coloring plot later\n",
    "                meta = {'model': item.get('model', 'Unknown')}\n",
    "                # Find the parameter value key dynamically (e.g., p_value, radius_value)\n",
    "                param_key = 'unknown_param_value' # Default key\n",
    "                param_name_found = 'unknown' # Default name\n",
    "                # Iterate through keys to find the one ending in '_value'\n",
    "                item_key_iter = iter(item.keys())\n",
    "                stop_key_iter = False\n",
    "                while not stop_key_iter:\n",
    "                     try:\n",
    "                          key = next(item_key_iter)\n",
    "                          if isinstance(key, str) and key.endswith('_value'): # Ensure key is string\n",
    "                               param_key = key\n",
    "                               param_name_found = key.replace('_value', '')\n",
    "                               stop_key_iter = True\n",
    "                     except StopIteration:\n",
    "                          stop_key_iter = True\n",
    "                # Store parameter name and value\n",
    "                meta['parameter_name'] = param_name_found\n",
    "                meta['parameter_value'] = item.get(param_key, np.nan)\n",
    "                pca_metadata.append(meta) # Append metadata dictionary\n",
    "\n",
    "        item_idx += 1 # Increment loop counter\n",
    "\n",
    "\n",
    "    # --- Create Final Matrix if Valid States Found ---\n",
    "    if len(valid_flat_states) > 0:\n",
    "         # Double check if all collected states have the same length (should be target_vector_size)\n",
    "         first_len = valid_flat_states[0].size\n",
    "         all_same_len = True\n",
    "         state_idx_check = 1\n",
    "         while state_idx_check < len(valid_flat_states):\n",
    "              if valid_flat_states[state_idx_check].size != first_len:\n",
    "                   all_same_len = False\n",
    "                   break # Exit loop if inconsistency found\n",
    "              state_idx_check += 1\n",
    "\n",
    "         if all_same_len:\n",
    "              # Stack valid vectors vertically to create the matrix [samples, features]\n",
    "              try:\n",
    "                  final_state_matrix = np.vstack(valid_flat_states)\n",
    "                  print(f\"  ‚úÖ Prepared matrix for PCA with shape: {final_state_matrix.shape}\")\n",
    "                  pca_data_prepared = True # Mark data as ready\n",
    "              except MemoryError as e_mem_stack:\n",
    "                   print(f\"‚ùå MemoryError stacking state vectors ({len(valid_flat_states)} vectors of size {first_len}). Cannot perform PCA.\")\n",
    "                   pca_data_prepared = False\n",
    "                   pca_error = True # Treat as error if stacking fails\n",
    "              except Exception as e_vstack:\n",
    "                   print(f\"‚ùå Error stacking state vectors for PCA: {e_vstack}\")\n",
    "                   pca_data_prepared = False\n",
    "                   pca_error = True # Treat as error\n",
    "         else:\n",
    "              # This case should ideally not happen if length check during collection works\n",
    "              print(\"  ‚ùå Error: Inconsistent vector lengths found after filtering for PCA.\")\n",
    "              lengths = [arr.size for arr in valid_flat_states]\n",
    "              print(\"   Lengths found:\", set(lengths))\n",
    "              pca_data_prepared = False\n",
    "              pca_error = True # Treat as error\n",
    "    else:\n",
    "         # No valid states found for the target N\n",
    "         print(f\"  ‚ö†Ô∏è No valid final state vectors found for PCA (N={N_for_landscape}) after filtering.\")\n",
    "         pca_data_prepared = False # Cannot proceed\n",
    "\n",
    "# --- Perform PCA ---\n",
    "if not pca_error and pca_data_prepared:\n",
    "    # Check if enough samples for requested components\n",
    "    num_pca_components_req = config.get(\"PCA_COMPONENTS\", 3) # Get requested components from config\n",
    "    min_samples_needed = max(2, num_pca_components_req) # Need at least 2 samples, and at least as many as components\n",
    "    num_samples_avail = final_state_matrix.shape[0]\n",
    "\n",
    "    if num_samples_avail < min_samples_needed:\n",
    "        print(f\"‚ùå Error: Not enough valid states ({num_samples_avail}) for PCA (need ‚â• {min_samples_needed}).\")\n",
    "        pca_error = True\n",
    "    else:\n",
    "         # --- Standardization ---\n",
    "         # Standardize data (mean=0, variance=1) before applying PCA\n",
    "         print(\"  Standardizing data (mean=0, variance=1)...\")\n",
    "         scaler = StandardScaler()\n",
    "         scaled_final_state_matrix = scaler.fit_transform(final_state_matrix)\n",
    "         print(\"  Standardization complete.\")\n",
    "\n",
    "         # --- PCA Fitting ---\n",
    "         # Determine number of components, ensuring it doesn't exceed data dimensions\n",
    "         # Max components = min(n_samples, n_features)\n",
    "         max_possible_components = min(scaled_final_state_matrix.shape[0], scaled_final_state_matrix.shape[1])\n",
    "         num_pca_components = min(num_pca_components_req, max_possible_components)\n",
    "\n",
    "         # Check if at least 2 components are possible for plotting\n",
    "         can_plot_pca = num_pca_components >= 2\n",
    "         if not can_plot_pca:\n",
    "             print(f\"‚ö†Ô∏è Warning: Cannot perform 2D PCA plot (only {num_pca_components} component possible/requested).\")\n",
    "             # Optionally run PCA anyway if 1 component is useful\n",
    "             # pca_error = True # Set error if plot is essential\n",
    "\n",
    "         if not pca_error: # Proceed only if plotting is possible or PCA itself is desired\n",
    "             print(f\"  Fitting PCA model (n_components={num_pca_components})...\")\n",
    "             pca_model = PCA(n_components=num_pca_components)\n",
    "             try:\n",
    "                 # Fit PCA model and transform the data\n",
    "                 pca_transformed_data = pca_model.fit_transform(scaled_final_state_matrix)\n",
    "                 # Get explained variance ratios\n",
    "                 explained_variance_ratios = pca_model.explained_variance_ratio_\n",
    "                 print(f\"  PCA fitting complete.\")\n",
    "                 # Format explained variance for printing\n",
    "                 explained_var_str = [f'{v:.4f}' for v in explained_variance_ratios]\n",
    "                 print(f\"  Explained variance per component: {explained_var_str}\")\n",
    "                 total_explained_variance = explained_variance_ratios.sum()\n",
    "                 print(f\"  Total variance explained by {num_pca_components} components: {total_explained_variance:.4f}\")\n",
    "\n",
    "                 # --- Plot PCA Results (Color by parameter value for each model) ---\n",
    "                 if can_plot_pca:\n",
    "                     # Combine PCA results (first 2 components) with metadata\n",
    "                     pca_plot_df = pd.DataFrame(pca_transformed_data[:, :2], columns=['PC1', 'PC2'])\n",
    "                     meta_df = pd.DataFrame(pca_metadata) # Convert metadata list to DataFrame\n",
    "                     # Ensure indices align before concatenation (should align if data wasn't filtered after stacking)\n",
    "                     if len(pca_plot_df) == len(meta_df):\n",
    "                         pca_plot_df.index = meta_df.index\n",
    "                         pca_plot_df = pd.concat([pca_plot_df, meta_df], axis=1)\n",
    "                     else:\n",
    "                          warnings.warn(\"PCA results and metadata have different lengths. Plotting may be incorrect.\", RuntimeWarning)\n",
    "                          # Attempt merge based on index if possible, otherwise skip plotting\n",
    "                          pca_plot_df = pd.DataFrame() # Make empty to skip plot\n",
    "\n",
    "\n",
    "                     # Find unique models present in the PCA data\n",
    "                     models_in_pca = []\n",
    "                     if not pca_plot_df.empty and 'model' in pca_plot_df.columns:\n",
    "                          models_in_pca = pca_plot_df['model'].unique()\n",
    "                     num_models_pca = len(models_in_pca)\n",
    "\n",
    "                     if num_models_pca > 0:\n",
    "                         # Create subplots: one row, one column per model\n",
    "                         fig_pca, axes_pca = plt.subplots(1, num_models_pca, figsize=(7 * num_models_pca, 6), squeeze=False)\n",
    "                         axes_pca = axes_pca.flatten() # Ensure axes_pca is iterable\n",
    "\n",
    "                         plot_idx_pca = 0\n",
    "                         while plot_idx_pca < num_models_pca:\n",
    "                             model = models_in_pca[plot_idx_pca]\n",
    "                             ax = axes_pca[plot_idx_pca] # Select subplot axis\n",
    "                             # Filter PCA data for the current model\n",
    "                             model_pca_data = pca_plot_df[pca_plot_df['model'] == model]\n",
    "\n",
    "                             if not model_pca_data.empty:\n",
    "                                 # Get parameter name and values for coloring\n",
    "                                 # Check if metadata columns exist\n",
    "                                 if 'parameter_name' in model_pca_data.columns and 'parameter_value' in model_pca_data.columns:\n",
    "                                     param_name_plot = model_pca_data['parameter_name'].iloc[0] # Assume consistent name within model\n",
    "                                     param_values_plot = model_pca_data['parameter_value']\n",
    "\n",
    "                                     # Decide whether to use log scale for color map (e.g., for WS 'p')\n",
    "                                     use_log_color = (model == 'WS') # Example condition\n",
    "                                     if use_log_color:\n",
    "                                         # Calculate log10, handle potential zeros or negative values safely\n",
    "                                         color_values = np.log10(np.maximum(param_values_plot.astype(float), 1e-6)) # Use maximum to avoid log(0)\n",
    "                                         color_label = f\"log10({param_name_plot})\" # Label for color bar\n",
    "                                     else:\n",
    "                                         # Use linear scale for color\n",
    "                                         color_values = param_values_plot.astype(float)\n",
    "                                         color_label = param_name_plot # Label for color bar\n",
    "\n",
    "                                     # Create scatter plot for this model\n",
    "                                     scatter = ax.scatter(model_pca_data['PC1'], model_pca_data['PC2'],\n",
    "                                                          c=color_values, cmap='viridis', # Color points by parameter value\n",
    "                                                          s=10, alpha=0.7) # Adjust size/transparency\n",
    "\n",
    "                                     # Add labels and title using explained variance\n",
    "                                     pc1_var_label = f\"{explained_variance_ratios[0]*100:.1f}%\"\n",
    "                                     pc2_var_label = f\"{explained_variance_ratios[1]*100:.1f}%\"\n",
    "                                     ax.set_xlabel(f\"PC 1 ({pc1_var_label})\")\n",
    "                                     ax.set_ylabel(f\"PC 2 ({pc2_var_label})\")\n",
    "                                     ax.set_title(f\"PCA of Final States ({model}, N={N_for_landscape})\")\n",
    "                                     ax.grid(True, linestyle=':')\n",
    "\n",
    "                                     # Add colorbar to the plot\n",
    "                                     cbar = fig_pca.colorbar(scatter, ax=ax, orientation='vertical')\n",
    "                                     cbar.set_label(color_label, rotation=270, labelpad=15)\n",
    "                                 else:\n",
    "                                     # Fallback if metadata columns are missing\n",
    "                                     ax.scatter(model_pca_data['PC1'], model_pca_data['PC2'], s=10, alpha=0.7)\n",
    "                                     ax.set_title(f\"{model}\\n(Metadata missing)\")\n",
    "                                     warnings.warn(f\"Metadata columns missing for model {model} in PCA plot.\", RuntimeWarning)\n",
    "                             else:\n",
    "                                  # Handle case where model has no data after filtering\n",
    "                                  ax.set_title(f\"{model}\\n(No PCA data)\")\n",
    "\n",
    "                             plot_idx_pca += 1 # Increment subplot index\n",
    "\n",
    "                         # Adjust layout and save the combined figure\n",
    "                         plt.tight_layout()\n",
    "                         pca_plot_filename = f\"{exp_name}_pca_landscape_comparison.png\"\n",
    "                         pca_plot_filepath = os.path.join(output_dir, pca_plot_filename)\n",
    "                         try:\n",
    "                             fig_pca.savefig(pca_plot_filepath, dpi=150, bbox_inches='tight')\n",
    "                             print(f\"  ‚úÖ PCA comparison plot saved to: {pca_plot_filepath}\")\n",
    "                         except Exception as e_save_pca:\n",
    "                              print(f\"‚ùå Error saving PCA plot: {e_save_pca}\")\n",
    "                         plt.show() # Display the plot\n",
    "                         plt.close(fig_pca) # Close figure\n",
    "\n",
    "                     else: # num_models_pca == 0\n",
    "                          print(\"  ‚ö†Ô∏è No models found in PCA data to plot.\")\n",
    "\n",
    "             except Exception as e_pca_fit:\n",
    "                 print(f\"‚ùå Error during PCA fitting/transform: {e_pca_fit}\")\n",
    "                 traceback.print_exc(limit=1)\n",
    "                 pca_error = True # Mark as error\n",
    "\n",
    "# --- Final Error Handling Messages ---\n",
    "elif not pca_error and not pca_data_prepared:\n",
    "    print(\"‚ùå Skipping PCA calculation: Data preparation failed (no valid vectors found or processed).\")\n",
    "elif pca_error:\n",
    "    # Message should have been printed already if pca_error was set\n",
    "    if not ('RUN_PCA_ANALYSIS' in config and not config['RUN_PCA_ANALYSIS']): # Avoid repeating skip message if disabled by flag\n",
    "         print(\"‚ùå Skipping PCA calculation due to errors or flags.\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 12: PCA analysis completed (or attempted).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Synthesis and Theoretical Summary (Emergenics - Conceptual)\n",
    "# Description: Creates markdown text summarizing experimental findings and\n",
    "#              articulating the Emergenics theoretical framework using thermodynamic analogies.\n",
    "# NOTE: This cell reflects an older summary style based on beta/variance FSS,\n",
    "#       kept for historical context but superseded by Cell 14's Chi FSS summary.\n",
    "\n",
    "import pandas as pd # Need pandas for pd.notna check\n",
    "import numpy as np # Need numpy for np.* types\n",
    "\n",
    "print(\"\\n--- Cell 13: Synthesis & Theoretical Framework (Conceptual - Pre Chi FSS) ---\")\n",
    "print(\"‚ö†Ô∏è Note: This summary reflects an older analysis state (pre-Chi FSS). Cell 14 provides the final Phase 1 summary.\")\n",
    "\n",
    "# --- Attempt to load values for placeholders ---\n",
    "# These values might not exist or might be from older, less reliable analyses\n",
    "beta_val_str = \"N/A\" # Default string\n",
    "# Check if the variable exists in the global scope\n",
    "if 'global_beta_exponent' in globals():\n",
    "     beta_val = globals()['global_beta_exponent']\n",
    "     # Check if the value is not None and is a finite number\n",
    "     if beta_val is not None and pd.notna(beta_val):\n",
    "          try:\n",
    "               beta_val_str = f\"{beta_val:.3f}\" # Format to 3 decimal places\n",
    "          except (TypeError, ValueError):\n",
    "               pass # Keep N/A if formatting fails\n",
    "\n",
    "pc_val_str = \"N/A\" # Default string\n",
    "if 'global_p_c_estimate' in globals():\n",
    "     pc_val = globals()['global_p_c_estimate']\n",
    "     if pc_val is not None and pd.notna(pc_val):\n",
    "          try:\n",
    "               pc_val_str = f\"{pc_val:.4f}\" # Format to 4 decimal places\n",
    "          except (TypeError, ValueError):\n",
    "               pass # Keep N/A\n",
    "\n",
    "pca_var_str = \"N/A\" # Default string\n",
    "# Check if total_explained_variance exists (set during PCA in Cell 12)\n",
    "if 'total_explained_variance' in globals():\n",
    "     pca_var = globals()['total_explained_variance']\n",
    "     if pca_var is not None and pd.notna(pca_var):\n",
    "         try:\n",
    "             pca_var_str = f\"{pca_var * 100.0:.1f}%\" # Format as percentage\n",
    "         except (TypeError, ValueError):\n",
    "             pass # Keep N/A\n",
    "\n",
    "pca_comps_str = \"N/A\" # Default string\n",
    "# Check if config exists and contains the PCA_COMPONENTS key\n",
    "if 'config' in globals() and isinstance(config, dict):\n",
    "     pca_comps = config.get(\"PCA_COMPONENTS\") # Use .get for safe access\n",
    "     if pca_comps is not None:\n",
    "          pca_comps_str = str(pca_comps)\n",
    "\n",
    "\n",
    "# Define summary text using f-string for dynamic values\n",
    "summary_markdown_text_conceptual = f\"\"\"\n",
    "# Emergenics: Synthesis & Theoretical Framework (Conceptual Summary)\n",
    "\n",
    "## Experimental Findings (Based on Initial Analysis - Pre-Chi FSS)\n",
    "\n",
    "The computational experiments provide empirical support for the Emergenics hypothesis, although initial analysis methods required refinement.\n",
    "\n",
    "- **Parametric Sweep (Watts-Strogatz):**\n",
    "  Varying the rewiring probability *p* induced a clear phase transition in the 5D Network Automaton's behavior, observed via the `variance_norm` order parameter. The system transitioned from a high-variance state (diverse dynamics) at low *p* to a low-variance state (homogenized dynamics) at high *p*.\n",
    "  - **Critical Point:** Initial estimation near *p_c* ‚âà {pc_val_str} (though subsequent Chi FSS provided a more reliable value).\n",
    "  - **Critical Scaling:** Direct FSS on the order parameter (`variance_norm`) suggested power-law scaling but yielded potentially unreliable exponents (e.g., **Œ≤ ‚âà {beta_val_str}**). This indicated the need for more sensitive observables like susceptibility.\n",
    "\n",
    "- **Universality Testing (WS, SBM, RGG):**\n",
    "  Analysis across different graph models revealed similar topology-driven transitions, supporting the universality of the Emergenics principle that structure controls dynamics. However, quantitative comparison of critical exponents (see Cell 11.3, Cell 14) revealed significant differences, pointing towards **distinct universality classes**.\n",
    "\n",
    "- **Attractor Landscape (PCA):**\n",
    "  PCA performed on the high-dimensional flattened final state vectors (if successful):\n",
    "  - **High Dimensionality:** Indicated by the fact that the top {pca_comps_str} principal components explained only ~{pca_var_str} of the total variance, confirming the system operates in a high-dimensional state space.\n",
    "  - **Topological Influence:** The distribution of final states in the PCA projection showed dependence on the topological control parameter (e.g., *p*), indicating that topology continuously shapes the accessible attractor landscape.\n",
    "\n",
    "## Theoretical Framework: Computational Thermodynamics\n",
    "\n",
    "Emergenics interprets these findings through a thermodynamic lens:\n",
    "\n",
    "- **Order Parameter:** Measures the degree of computational order/uniformity (e.g., `variance_norm`). Low variance = uniform/ordered, high variance = diverse/disordered.\n",
    "- **Control Parameter:** Topology (*p*, *p_intra*, *r*) acts like temperature or another external field, tuning the system between computational phases.\n",
    "- **Phase Transition:** The sharp change near the critical point (*p_c*, *r_c*) marks a shift between computational regimes (e.g., from locally processed information to globally integrated states).\n",
    "- **Critical Exponents (Œ≥, ŒΩ, Œ≤):** Quantify universal scaling behavior near the transition, linking computational dynamics to principles of statistical mechanics and universality. Differences in exponents classify different computational regimes.\n",
    "- **State Space:** The high-dimensional space (partially revealed by PCA/UMAP) represents the system's computational capacity or 'phase space'. Structure constrains the dynamics within this space.\n",
    "\n",
    "## Conclusion (Conceptual): Structure IS Computation\n",
    "\n",
    "This work demonstrates computationally that network topology acts as a fundamental control parameter, inducing quantifiable phase transitions in the emergent dynamics of a novel 5D Network Automaton. The identification of critical points and scaling exponents (particularly Œ≥ and ŒΩ from Chi FSS) provides strong support for the Emergenics framework. The system exhibits rich, high-dimensional behavior influenced by network structure, offering a powerful paradigm for understanding and potentially designing computation in complex networks. The finding of distinct universality classes adds significant depth to this picture.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps (Post Phase 1):**\n",
    "1. Further analyze Phase 2 results (information, landscape, perturbation).\n",
    "2. Develop more sophisticated computational metrics (Phase 2).\n",
    "3. Explore finite-size scaling corrections.\n",
    "4. Formulate quantitative design principles linking structure to function (Phase 3).\n",
    "5. Apply framework to specific computational tasks (Phase 4).\n",
    "\"\"\"\n",
    "\n",
    "# Print the summary to the console\n",
    "print(summary_markdown_text_conceptual)\n",
    "# Store for saving or later use if needed, though Cell 14 is the primary summary\n",
    "global_summary_markdown_text_conceptual = summary_markdown_text_conceptual\n",
    "\n",
    "print(\"‚úÖ Cell 13: Conceptual Synthesis and Theoretical Summary generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fc769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Synthesis & Summary (Phase 1 Completion - Final v3)\n",
    "# Description: Summarizes Phase 1 findings: criticality via Chi FSS, LACK of universality,\n",
    "#              energy checks, and sensitivity. Removes mention of skipped PCA.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "print(\"\\n--- Cell 14: Synthesis & Summary (Phase 1 Completion - Final v3) ---\")\n",
    "\n",
    "# --- Gather Data Safely ---\n",
    "# Load config dictionary if available\n",
    "config = {}\n",
    "if 'config' in globals() and isinstance(globals()['config'], dict):\n",
    "    config = globals()['config']\n",
    "else:\n",
    "    warnings.warn(\"Config dictionary not found in Cell 14. Summary may use defaults or be incomplete.\", RuntimeWarning)\n",
    "\n",
    "# Extract necessary info from config safely using .get()\n",
    "exp_name_summary = config.get('EXPERIMENT_NAME', \"N/A_Phase1_Experiment\")\n",
    "output_dir_summary = config.get('OUTPUT_DIR', \".\") # Default to current dir if not set\n",
    "primary_metric_summary = config.get('PRIMARY_ORDER_PARAMETER', 'N/A')\n",
    "sensitivity_param = config.get('SENSITIVITY_RULE_PARAM', 'N/A')\n",
    "energy_checked = config.get('CALCULATE_ENERGY', False);\n",
    "history_stored = config.get('STORE_ENERGY_HISTORY', False)\n",
    "\n",
    "# --- Helper Function (copied from Cell 11.3) ---\n",
    "def format_metric(value, fmt):\n",
    "    \"\"\"Safely formats a numerical value using a format string.\"\"\"\n",
    "    is_valid_number = False\n",
    "    if value is not None:\n",
    "        if isinstance(value, (int, float)) and np.isfinite(value): is_valid_number = True\n",
    "    if is_valid_number:\n",
    "        try: return fmt % value\n",
    "        except (TypeError, ValueError): return \"Format Error\"\n",
    "    else: return \"N/A\"\n",
    "\n",
    "# --- Get Results from Global Variables (set by previous analysis cells) ---\n",
    "# Use .get() on globals() dictionary to access variables safely\n",
    "ws_chi_results = globals().get('global_optuna_fss_chi_results', {})\n",
    "sbm_chi_results = globals().get('global_optuna_fss_chi_sbm_results', {})\n",
    "rgg_chi_results = globals().get('global_optuna_fss_chi_rgg_results', {})\n",
    "\n",
    "# Check if sensitivity analysis was performed by looking for the output plot file\n",
    "sensitivity_plot_path = os.path.join(output_dir_summary, f\"{exp_name_summary}_sensitivity_pc_vs_{sensitivity_param}.png\")\n",
    "sensitivity_analyzed = os.path.exists(sensitivity_plot_path)\n",
    "\n",
    "\n",
    "# --- Extract specific values safely using .get() from result dictionaries ---\n",
    "pc_ws=ws_chi_results.get('pc',np.nan); gamma_ws=ws_chi_results.get('gamma',np.nan); nu_ws=ws_chi_results.get('nu',np.nan)\n",
    "ws_success=ws_chi_results.get('success',False)\n",
    "\n",
    "pc_sbm=sbm_chi_results.get('pc',np.nan); gamma_sbm=sbm_chi_results.get('gamma',np.nan); nu_sbm=sbm_chi_results.get('nu',np.nan)\n",
    "sbm_success=sbm_chi_results.get('success',False)\n",
    "\n",
    "pc_rgg=rgg_chi_results.get('pc',np.nan); gamma_rgg=rgg_chi_results.get('gamma',np.nan); nu_rgg=rgg_chi_results.get('nu',np.nan)\n",
    "rgg_success=rgg_chi_results.get('success',False)\n",
    "\n",
    "# --- Calculate Universality Statistics ---\n",
    "# Collect valid exponent values only from successful runs\n",
    "gamma_values = []\n",
    "if ws_success and pd.notna(gamma_ws): gamma_values.append(gamma_ws)\n",
    "if sbm_success and pd.notna(gamma_sbm): gamma_values.append(gamma_sbm)\n",
    "if rgg_success and pd.notna(gamma_rgg): gamma_values.append(gamma_rgg)\n",
    "\n",
    "nu_values = []\n",
    "if ws_success and pd.notna(nu_ws): nu_values.append(nu_ws)\n",
    "if sbm_success and pd.notna(nu_sbm): nu_values.append(nu_sbm)\n",
    "if rgg_success and pd.notna(nu_rgg): nu_values.append(nu_rgg)\n",
    "\n",
    "# Count how many models had successful exponent calculations\n",
    "models_compared_count = len(gamma_values) # Count based on successful gamma values\n",
    "\n",
    "# Calculate stats only if enough data points exist (>= 2)\n",
    "gamma_mean = np.nan; gamma_std = np.nan; gamma_rsd = np.inf # Initialize\n",
    "if models_compared_count >= 2:\n",
    "    gamma_mean=np.mean(gamma_values); gamma_std=np.std(gamma_values)\n",
    "    # Calculate RSD = (StdDev / |Mean|) * 100%, check for zero mean\n",
    "    if gamma_mean != 0 and pd.notna(gamma_mean) and pd.notna(gamma_std):\n",
    "        gamma_rsd=(gamma_std / abs(gamma_mean)) * 100.0\n",
    "\n",
    "nu_mean = np.nan; nu_std = np.nan; nu_rsd = np.inf # Initialize\n",
    "# Use nu_values list length for nu comparison, as gamma/nu might have different success rates\n",
    "if len(nu_values) >= 2:\n",
    "    nu_mean=np.mean(nu_values); nu_std=np.std(nu_values)\n",
    "    # Calculate RSD for Nu\n",
    "    if nu_mean != 0 and pd.notna(nu_mean) and pd.notna(nu_std):\n",
    "        nu_rsd=(nu_std / abs(nu_mean)) * 100.0\n",
    "\n",
    "\n",
    "# --- Generate Summary Text using an f-string and list joining ---\n",
    "summary_lines = [] # Initialize empty list for lines of summary\n",
    "\n",
    "summary_lines.append(f\"# Emergenics Phase 1 Summary: {exp_name_summary}\\n\")\n",
    "summary_lines.append(\"## Objective:\")\n",
    "summary_lines.append(\"Rigorously analyze topology-driven phase transitions in a 5D Network Automaton across WS, SBM, and RGG models using FSS on Susceptibility (œá) via Optuna. Assess universality and sensitivity.\")\n",
    "\n",
    "summary_lines.append(\"\\n## Key Findings:\")\n",
    "summary_lines.append(\"- **Phase Transitions Confirmed:** All models exhibit clear computational phase transitions controlled by topology (p, p_intra, r), observable via order parameters like variance_norm.\")\n",
    "summary_lines.append(\"- **Susceptibility (œá) FSS Success:** Optuna-driven FSS on œá yielded robust critical point and exponent estimates for each model:\")\n",
    "# Format results using helper function\n",
    "summary_lines.append(f\"  - **WS:**  p_c ‚âà {format_metric(pc_ws, '%.5f')}, Œ≥ ‚âà {format_metric(gamma_ws, '%.3f')}, ŒΩ ‚âà {format_metric(nu_ws, '%.3f')} ({'Success' if ws_success else 'Failed'})\")\n",
    "summary_lines.append(f\"  - **SBM:** p_c ‚âà {format_metric(pc_sbm, '%.5f')}, Œ≥ ‚âà {format_metric(gamma_sbm, '%.3f')}, ŒΩ ‚âà {format_metric(nu_sbm, '%.3f')} ({'Success' if sbm_success else 'Failed'})\")\n",
    "summary_lines.append(f\"  - **RGG:** r_c ‚âà {format_metric(pc_rgg, '%.5f')}, Œ≥ ‚âà {format_metric(gamma_rgg, '%.3f')}, ŒΩ ‚âà {format_metric(nu_rgg, '%.3f')} ({'Success' if rgg_success else 'Failed'})\")\n",
    "\n",
    "summary_lines.append(\"- **Universality Analysis (Based on œá FSS):**\")\n",
    "if models_compared_count >= 2:\n",
    "    # Report stats if calculated\n",
    "    summary_lines.append(f\"  - Models Compared (Successful Œ≥ fit): {models_compared_count}\")\n",
    "    summary_lines.append(f\"  - Gamma (Œ≥): Mean={format_metric(gamma_mean, '%.3f')} ¬± {format_metric(gamma_std, '%.3f')} (RSD: {format_metric(gamma_rsd, '%.1f')}%)\")\n",
    "    summary_lines.append(f\"  - Nu (ŒΩ):    Mean={format_metric(nu_mean, '%.3f')} ¬± {format_metric(nu_std, '%.3f')} (RSD: {format_metric(nu_rsd, '%.1f')}%)\") # Use nu_values count\n",
    "    # Conclusion based on RSD - using the threshold from Phase 1 results (high RSD -> distinct)\n",
    "    if gamma_rsd > 25 or nu_rsd > 25: # Check if *either* shows high variation\n",
    "        summary_lines.append(\"  - **Conclusion: Significant variation in exponents (RSD > 25%) strongly indicates WS, SBM, RGG belong to DIFFERENT universality classes.**\")\n",
    "    elif gamma_rsd < 15 and nu_rsd < 15: # Requires low RSD for *both* to suggest same class\n",
    "        summary_lines.append(\"  - **Conclusion: Low variation suggests a single universality class (Œ≥‚âà{:.3f}, ŒΩ‚âà{:.3f}).**\".format(gamma_mean, nu_mean)) # Format means if consistent\n",
    "    else: # Intermediate case\n",
    "        summary_lines.append(\"  - **Conclusion: Moderate variation in exponents (RSD between 15-25%) makes universality questionable; distinct classes remain likely.**\")\n",
    "else:\n",
    "    # Message if not enough successful models for comparison\n",
    "    summary_lines.append(f\"  - Comparison not performed (requires successful exponent results from >= 2 models, found {models_compared_count}).\")\n",
    "\n",
    "summary_lines.append(\"- **Sensitivity:**\")\n",
    "if sensitivity_analyzed:\n",
    "    # Report sensitivity findings if plot exists\n",
    "    summary_lines.append(f\"  - Assessed impact of '{sensitivity_param}' on p_c ({config.get('TARGET_MODEL_SENS','WS')} model).\") # Add model tested\n",
    "    summary_lines.append(f\"  - Conclusion: Critical point shifts predictably with '{sensitivity_param}', but transition phenomenon persists (see plot).\")\n",
    "else:\n",
    "    # Message if sensitivity analysis was skipped or failed\n",
    "    summary_lines.append(f\"  - Sensitivity analysis for '{sensitivity_param}' not completed or plot not found.\")\n",
    "\n",
    "summary_lines.append(\"- **Energy & Dynamics:**\")\n",
    "# Report status based on config flags\n",
    "if energy_checked:\n",
    "    summary_lines.append(f\"  - Final energy calculated (type: {config.get('ENERGY_FUNCTIONAL_TYPE', 'N/A')}).\")\n",
    "else: summary_lines.append(\"  - Final energy calculation disabled.\")\n",
    "if history_stored:\n",
    "    summary_lines.append(\"  - Energy monotonicity check performed (details in Cell 11.4 output).\")\n",
    "else: summary_lines.append(\"  - Energy monotonicity check skipped (STORE_ENERGY_HISTORY=False).\")\n",
    "\n",
    "summary_lines.append(\"- **Other Analysis Notes:**\")\n",
    "summary_lines.append(f\"  - FSS on primary order parameter ('{primary_metric_summary}') yielded poor collapse; œá FSS proved more suitable.\")\n",
    "# Remove mention of PCA failure as it was fixed/attempted in Phase 2 context\n",
    "# summary_lines.append(\"  - PCA analysis of final states was skipped or failed.\")\n",
    "\n",
    "summary_lines.append(\"\\n## Overall Phase 1 Conclusion:\")\n",
    "summary_lines.append(\"Phase 1 successfully used GPU acceleration and robust analysis (Optuna FSS on œá) to quantify topology-driven phase transitions in WS, SBM, and RGG models for the 5D HDC/RSV Network Automaton.\")\n",
    "# Final conclusion adjusted based on RSD - check for high RSD indicating distinct classes\n",
    "if gamma_rsd > 25 or nu_rsd > 25:\n",
    "     summary_lines.append(f\"**Crucially, evidence strongly suggests these models belong to DISTINCT universality classes (High RSDs: Œ≥‚âà{format_metric(gamma_rsd, '%.1f')}%, ŒΩ‚âà{format_metric(nu_rsd, '%.1f')}%).**\")\n",
    "     summary_lines.append(\"This indicates the *type* of network structure fundamentally alters the critical computational dynamics.\")\n",
    "elif gamma_rsd < 15 and nu_rsd < 15:\n",
    "      summary_lines.append(f\"**Evidence supports UNIVERSALITY, with consistent critical exponents (Œ≥‚âà{format_metric(gamma_mean, '%.3f')}, ŒΩ‚âà{format_metric(nu_mean, '%.3f')}) across these distinct topological classes.**\")\n",
    "      summary_lines.append(\"This suggests fundamental, shared principles governing computational emergence in these networks.\")\n",
    "else: # Intermediate case\n",
    "     summary_lines.append(f\"**Universality is questionable (Moderate RSDs: Œ≥‚âà{format_metric(gamma_rsd, '%.1f')}%, ŒΩ‚âà{format_metric(nu_rsd, '%.1f')}%). Distinct classes remain likely.**\")\n",
    "\n",
    "summary_lines.append(\"Sensitivity analysis confirmed the robustness of the transition phenomenon. The Emergenics framework is validated, providing a solid quantitative foundation for Phase 2 (exploring computational capabilities) and Phase 3 (design principles).\")\n",
    "\n",
    "# --- Save Summary ---\n",
    "summary_text = \"\\n\".join(summary_lines) # Join lines into single string\n",
    "# Ensure output directory exists before saving\n",
    "summary_filename_phase1 = \"Phase1_Summary_Error.md\" # Default filename on error\n",
    "if output_dir_summary is not None and exp_name_summary is not None and output_dir_summary != \".\":\n",
    "    try:\n",
    "        os.makedirs(output_dir_summary, exist_ok=True) # Create dir if needed\n",
    "        summary_filename_phase1 = os.path.join(output_dir_summary, f\"{exp_name_summary}_summary_phase1.md\")\n",
    "        with open(summary_filename_phase1, 'w') as f_write_summary:\n",
    "             f_write_summary.write(summary_text)\n",
    "        print(f\"\\n‚úÖ Saved Phase 1 summary document to: {summary_filename_phase1}\")\n",
    "    except Exception as e_save_summary:\n",
    "        print(f\"‚ùå Error saving Phase 1 summary document: {e_save_summary}\")\n",
    "else:\n",
    "     print(\"\\n‚ö†Ô∏è Could not save Phase 1 summary document (Output directory or experiment name invalid).\")\n",
    "\n",
    "\n",
    "# --- Print Summary to Console ---\n",
    "print(\"\\n\" + \"=\"*80); print(summary_text); print(\"=\"*80)\n",
    "print(\"\\n--- Phase 1 Analysis & Summary Generation Complete ---\")\n",
    "print(\"Cell 14 execution complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86776827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Phase 2 Synthesis & Summary\n",
    "# Description: Generates a markdown summary of Phase 2 findings, comparing\n",
    "#              computational characteristics (information processing, attractors\n",
    "#              via UMAP/PCA, perturbation response) across the different\n",
    "#              universality classes identified in Phase 1.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "print(\"\\n--- Cell 15: Phase 2 Synthesis & Summary ---\")\n",
    "\n",
    "# --- Gather Data Safely ---\n",
    "# Load config dictionary if available\n",
    "config = {}\n",
    "if 'config' in globals() and isinstance(globals()['config'], dict):\n",
    "    config = globals()['config']\n",
    "else:\n",
    "    warnings.warn(\"Config dictionary not found in Cell 15. Phase 2 Summary may be incomplete.\", RuntimeWarning)\n",
    "\n",
    "# Extract necessary info from config safely using .get()\n",
    "# Use Phase 2 experiment name and output dir from config\n",
    "exp_name_summary = config.get('EXPERIMENT_NAME', \"Phase2_N/A\")\n",
    "output_dir_summary = config.get('OUTPUT_DIR', \".\") # Default to current dir if not set\n",
    "\n",
    "# --- Check if Phase 2 analysis results exist (by checking for expected output plots/files) ---\n",
    "# Construct expected file paths based on Phase 2 naming conventions\n",
    "info_plot_path = os.path.join(output_dir_summary, f\"{exp_name_summary}_info_metrics_comparison.png\")\n",
    "umap_plot_path = os.path.join(output_dir_summary, f\"{exp_name_summary}_umap_landscape_comparison.png\")\n",
    "pca_plot_path = os.path.join(output_dir_summary, f\"{exp_name_summary}_pca_landscape_comparison.png\")\n",
    "pert_plot_path = os.path.join(output_dir_summary, f\"{exp_name_summary}_perturbation_response_comparison.png\")\n",
    "\n",
    "# Check existence of plot files as indicators of successful analysis steps\n",
    "info_plot_exists = os.path.exists(info_plot_path)\n",
    "umap_plot_exists = os.path.exists(umap_plot_path)\n",
    "pca_plot_exists = os.path.exists(pca_plot_path)\n",
    "pert_plot_exists = os.path.exists(pert_plot_path)\n",
    "\n",
    "# --- Load key metrics from Phase 1 (if path was stored in Phase 2 config) ---\n",
    "phase1_metrics = {}\n",
    "# Retrieve path to Phase 1 metrics file stored during Phase 2 config setup\n",
    "phase1_key_metrics_path = config.get('phase1_key_metrics_path')\n",
    "if phase1_key_metrics_path and os.path.exists(phase1_key_metrics_path):\n",
    "    try:\n",
    "        with open(phase1_key_metrics_path, 'r') as f_p1_metrics:\n",
    "             phase1_metrics = json.load(f_p1_metrics)\n",
    "        print(f\"  Loaded Phase 1 key metrics from: {phase1_key_metrics_path}\")\n",
    "    except Exception as e_load_p1m:\n",
    "         warnings.warn(f\"Could not load Phase 1 key metrics from {phase1_key_metrics_path}: {e_load_p1m}\", RuntimeWarning)\n",
    "         phase1_metrics = {} # Reset if load fails\n",
    "else:\n",
    "     print(\"  Phase 1 key metrics file path not found or file doesn't exist. Cannot display Phase 1 exponents.\")\n",
    "\n",
    "# --- Helper for formatting ---\n",
    "def format_metric(value, fmt):\n",
    "    \"\"\"Safely formats a numerical value using a format string.\"\"\"\n",
    "    is_valid_number = False\n",
    "    if value is not None:\n",
    "        if isinstance(value, (int, float)) and np.isfinite(value): is_valid_number = True\n",
    "    if is_valid_number:\n",
    "        try: return fmt % value\n",
    "        except (TypeError, ValueError): return \"Format Error\"\n",
    "    else: return \"N/A\"\n",
    "\n",
    "# Extract Phase 1 exponents safely from loaded metrics\n",
    "pc_ws=phase1_metrics.get('final_pc_ws_chi', np.nan)\n",
    "gamma_ws=phase1_metrics.get('final_gamma_ws_chi', np.nan); nu_ws=phase1_metrics.get('final_nu_ws_chi', np.nan)\n",
    "pc_sbm=phase1_metrics.get('final_pc_sbm_chi', np.nan)\n",
    "gamma_sbm=phase1_metrics.get('final_gamma_sbm_chi', np.nan); nu_sbm=phase1_metrics.get('final_nu_sbm_chi', np.nan)\n",
    "pc_rgg=phase1_metrics.get('final_pc_rgg_chi', np.nan)\n",
    "gamma_rgg=phase1_metrics.get('final_gamma_rgg_chi', np.nan); nu_rgg=phase1_metrics.get('final_nu_rgg_chi', np.nan)\n",
    "\n",
    "\n",
    "# --- Generate Summary Text ---\n",
    "summary_lines = [f\"# Emergenics Phase 2 Summary: {exp_name_summary}\\n\"]\n",
    "summary_lines.append(\"## Objective:\")\n",
    "summary_lines.append(\"Characterize the computational properties associated with the different phases (ordered, critical, disordered) and distinct universality classes (WS, SBM, RGG) identified in Phase 1.\")\n",
    "\n",
    "summary_lines.append(\"\\n## Phase 1 Recap - Distinct Universality Classes:\")\n",
    "summary_lines.append(\"Phase 1 concluded that WS, SBM, and RGG models likely belong to **distinct universality classes** based on differing critical exponents derived from Susceptibility (œá) FSS:\")\n",
    "summary_lines.append(f\"  - **WS:**  (p_c‚âà{format_metric(pc_ws, '.4f')}, Œ≥‚âà{format_metric(gamma_ws, '.3f')}, ŒΩ‚âà{format_metric(nu_ws, '.3f')})\")\n",
    "summary_lines.append(f\"  - **SBM:** (p_c‚âà{format_metric(pc_sbm, '.4f')}, Œ≥‚âà{format_metric(gamma_sbm, '.3f')}, ŒΩ‚âà{format_metric(nu_sbm, '.3f')})\")\n",
    "summary_lines.append(f\"  - **RGG:** (r_c‚âà{format_metric(pc_rgg, '.4f')}, Œ≥‚âà{format_metric(gamma_rgg, '.3f')}, ŒΩ‚âà{format_metric(nu_rgg, '.3f')})\")\n",
    "\n",
    "summary_lines.append(\"\\n## Phase 2 Findings - Computational Characteristics:\")\n",
    "\n",
    "summary_lines.append(\"\\n### 1. Information Processing:\")\n",
    "if info_plot_exists:\n",
    "    summary_lines.append(\"  - Analysis performed using `mean_final_state_entropy` (average Shannon entropy across state dimensions).\")\n",
    "    summary_lines.append(\"  - **Observation:** *[Manually interpret plot '{}_info_metrics_comparison.png']:* \".format(exp_name_summary))\n",
    "    summary_lines.append(\"    - Describe how entropy changes vs. the control parameter (p, p_intra, r) for each model.\")\n",
    "    summary_lines.append(\"    - Does entropy peak near the critical point (p_c, r_c)?\")\n",
    "    summary_lines.append(\"    - Are the peak heights or shapes significantly different between WS, SBM, RGG?\")\n",
    "    summary_lines.append(\"  - **Interpretation:** Differences in entropy profiles likely reflect varying levels of state diversity and predictability across phases and universality classes. A peak near criticality might indicate maximal dynamic complexity. Distinct profiles reinforce the idea of different computational regimes.\")\n",
    "else:\n",
    "    summary_lines.append(\"  - Analysis skipped, failed, or plot not found.\")\n",
    "\n",
    "summary_lines.append(\"\\n### 2. Attractor Landscape:\")\n",
    "umap_analysis_done = config.get('RUN_UMAP_ANALYSIS', False) and umap_plot_exists\n",
    "pca_analysis_done = config.get('RUN_PCA_ANALYSIS', False) and pca_plot_exists\n",
    "if umap_analysis_done or pca_analysis_done:\n",
    "    summary_lines.append(\"  - Explored using dimensionality reduction on final state vectors near sub-critical, critical, and super-critical regimes.\")\n",
    "    if umap_analysis_done: summary_lines.append(f\"  - UMAP visualizations generated (see '{exp_name_summary}_umap_landscape_comparison.png').\")\n",
    "    if pca_analysis_done: summary_lines.append(f\"  - PCA visualizations generated (see '{exp_name_summary}_pca_landscape_comparison.png').\")\n",
    "    summary_lines.append(\"  - **Observation:** *[Manually interpret UMAP/PCA plots]:*\")\n",
    "    summary_lines.append(\"    - Describe the structure in the low-dimensional embedding (e.g., number/separation of clusters, shape of manifold).\")\n",
    "    summary_lines.append(\"    - How does this structure change across the phase transition (sub-critical vs. critical vs. super-critical)?\")\n",
    "    summary_lines.append(\"    - Are there qualitative differences in landscape structure between WS, SBM, and RGG at similar relative distances from their respective critical points?\")\n",
    "    summary_lines.append(\"  - **Interpretation:** Visualizations provide insights into the geometry of the system's accessible state space. Changes across the transition reflect shifts in computational dynamics. Differences between models highlight how underlying topology shapes the attractor landscape, consistent with distinct universality classes.\")\n",
    "else:\n",
    "    summary_lines.append(\"  - Analysis skipped, failed, or plots not found.\")\n",
    "\n",
    "summary_lines.append(\"\\n### 3. Perturbation Response:\")\n",
    "if pert_plot_exists:\n",
    "    pert_metric = config.get('PERTURBATION_METRICS_TO_CALC', ['relaxation_time'])[0] # Get first metric plotted\n",
    "    summary_lines.append(f\"  - Assessed system response to transient node clamping using '{pert_metric}'.\")\n",
    "    summary_lines.append(\"  - **Observation:** *[Manually interpret plot '{}_perturbation_response_comparison.png']:* \".format(exp_name_summary))\n",
    "    summary_lines.append(f\"    - How does {pert_metric} change vs. the control parameter?\")\n",
    "    summary_lines.append(f\"    - Does {pert_metric} show a peak or significant change near the critical point (indicative of critical slowing down or heightened sensitivity)?\")\n",
    "    summary_lines.append(f\"    - Are there differences in the response profiles (peak height, width) between WS, SBM, RGG?\")\n",
    "    summary_lines.append(\"  - **Interpretation:** Perturbation response quantifies system stability and information propagation dynamics. Sensitivity often peaks near criticality. Differences across models reinforce that distinct structural classes handle perturbations differently, impacting computational robustness and information flow.\")\n",
    "else:\n",
    "    summary_lines.append(\"  - Analysis skipped, failed, or plot not found.\")\n",
    "\n",
    "summary_lines.append(\"\\n## Overall Phase 2 Conclusion:\")\n",
    "summary_lines.append(\"Phase 2 provided initial characterizations of the computational properties associated with the distinct universality classes identified in Phase 1. Key takeaways:\")\n",
    "summary_lines.append(\"- **Distinct Computational Regimes:** Analyses of information entropy, attractor landscapes (via UMAP/PCA), and perturbation response suggest qualitative and potentially quantitative differences in computational behavior between the WS, SBM, and RGG structural classes, supporting the Phase 1 finding of distinct universality.\")\n",
    "summary_lines.append(\"- **Criticality & Computation:** The critical region appears associated with unique computational characteristics (e.g., potentially peak entropy, heightened sensitivity to perturbations), aligning with theoretical expectations ('edge of chaos'). The exact nature varies across universality classes.\")\n",
    "summary_lines.append(\"- **Structure-Function Link Validated:** These findings further strengthen the Emergenics principle: the *type* of network structure (not just a single parameter) fundamentally shapes the *nature* and *class* of the emergent computation.\")\n",
    "summary_lines.append(\"\\nThis phase provides valuable comparative insights. Phase 3 can now focus on formulating more precise structure-function relationships and developing tools for designing networks with desired emergent computational properties based on these observed class distinctions.\")\n",
    "\n",
    "# --- Save Summary ---\n",
    "summary_text = \"\\n\".join(summary_lines) # Join lines into single string\n",
    "summary_filename_phase2 = \"Phase2_Summary_Error.md\" # Default filename on error\n",
    "if output_dir_summary is not None and exp_name_summary is not None and output_dir_summary != \".\":\n",
    "    try:\n",
    "        os.makedirs(output_dir_summary, exist_ok=True) # Create dir if needed\n",
    "        summary_filename_phase2 = os.path.join(output_dir_summary, f\"{exp_name_summary}_summary_phase2.md\")\n",
    "        with open(summary_filename_phase2, 'w') as f_write_summary_p2:\n",
    "             f_write_summary_p2.write(summary_text)\n",
    "        print(f\"\\n‚úÖ Saved Phase 2 summary document to: {summary_filename_phase2}\")\n",
    "    except Exception as e_save_summary_p2:\n",
    "        print(f\"‚ùå Error saving Phase 2 summary document: {e_save_summary_p2}\")\n",
    "else:\n",
    "     print(\"\\n‚ö†Ô∏è Could not save Phase 2 summary document (Output directory or experiment name invalid).\")\n",
    "\n",
    "# --- Print Summary to Console ---\n",
    "print(\"\\n\" + \"=\"*80); print(summary_text); print(\"=\"*80)\n",
    "print(\"\\n--- Phase 2 Analysis & Summary Generation Complete ---\")\n",
    "print(\"Cell 15 execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-automaton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
